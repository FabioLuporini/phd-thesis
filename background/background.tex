\chapter{Background}

\section{The Finite Element Method}
\label{sec:bkg:fem}
Computational methods based upon finite elements are used to approximate the solution of partial differential equations (henceforth, PDEs) in a wide variety of domains. The mathematical abstraction used in finite element methods (or FEMs) is extremely powerful: not only does it help reasoning about the problem, but also provides systematic ways of deriving effective computer implementations. In~\cite{brenner-and-scott}, it is usefully suggested to consider an FEM as a black box that, given a differential equation, returns a discrete algorithm capable of approximating the equation solution. Unveiling the whole magic of such a black box is clearly out of the scope of this chapter. We rather limit ourselves to review the mathematical properties and the computational aspects that are essential for understanding the contributions in Chapters~\ref{ch:optimality} and~\ref{ch:lowlevelopt}. The content and the notation used in this section are inspired from~\cite{florian-thesis,kirby-and-logg,olgaard-and-wells}. For a complete treatment of the subject, the reader is invited to refer to~\cite{brenner-and-scott}.



\subsection{Variational Formulation}
\label{sec:bkg:var-problems}
We consider the weak formulation of a {\em linear variational problem}

\begin{equation}
\begin{split}
\text{Find}\ u \in U\ \text{such that} \\
a(u, v) = L(v), \forall v \in V
\end{split}
\end{equation}

where $a$ and $L$ are, respectively, a bilinear form and a linear form. The term ``variational'' stems from the fact that the function $v$ can vary arbitrarily. The unfamiliar reader may find this expression unusual. Understanding the actual meaning of this formulation is far beyond the goals of this review, since an entire course in functional analysis would be needed. Informally, we can think of $V$ as a ``very nice'' space, in which functions have desirable properties. The underlying idea of the variational formulation is to ``transfer'' certain requirements (e.g., differentiability) from the unknown $u \in U$ to $v \in V$.

The sets $U$ and $V$ and called, respectively, trial and test functions. The variational problem is discretized by using discrete test and trial spaces

\begin{equation}
\label{sec:bkg:eq:disc-var}
\begin{split}
\text{Find}\ u_h \in U_h \subset U\ \text{such that} \\
a(u_h, v_h) = L(v_h), \forall v_h \in V_h \subset V
\end{split}
\end{equation}

Let $\lbrace \psi_i \rbrace_{i=1}^N$ be the set of basis functions spanning $U_h$ and let $\lbrace \phi_j \rbrace_{j=1}^{N}$ be the set of basis functions spanning $V_h$. Then the unknown solution $u$ can be approximated as a linear combination of the basis functions $\lbrace \psi_i \rbrace_{i=1}^N$,

\begin{equation}
u_h = \sum_{j=1}^N U_j \psi_j.
\end{equation}

This allows us to rewrite~\ref{sec:bkg:eq:disc-var} as:

\begin{equation}
\sum_{j=1}^N U_j a(\psi_j, \phi_i) = L(\phi_i),\ i=1,2,...,N
\end{equation}

From the solution of the following linear system we determine the set of {\em degrees of freedom} $U$ to express $u_h$:

\begin{equation}
\label{sec:bkg:eq:lin-sys}
Au = b
\end{equation}

where clearly

\begin{equation}
\label{sec:bkg:eq:disc-op}
\centering
\begin{split}
A_{ij} = a(\phi_i(x), \phi_j(x)) \\
b_i = L(\phi_i(x))
\end{split}
\end{equation}

The matrix $A$ and the vector $b$ can be seen as the discrete operators arising from the bilinear form $a$ and from the linear form $L$ for the given choice of basis functions.

The variational formulation of a {\em non-linear variational problem} requires refinements that are out of the scope of this review. The interested reader should refer to~\cite{brenner-and-scott}.

\subsection{Finite Elements}
In an FEM the domain $\Omega$ of the PDE is partitioned into a finite set of disjoint cells $\lbrace K \rbrace$; that is, $\bigcup K = \Omega$ and $\bigcap K = \emptyset$. This forms a {\em mesh}. A finite element is usually defined (see for example~\cite{brenner-and-scott}) as a triple ${<}K,\mathcal{P}_K,\mathcal{L}_K{>}$, where:
\begin{itemize}
\item $K$ is a cell in the mesh with non-empty interior and piecewise smooth boundary;
\item $\mathcal{P}_K$ is a finite dimensional ``local'' function space of dimension $n_K$;
\item the set of degrees of freedom $\mathcal{L}_K$ is a basis $\lbrace l_1^K, l_2^K, ..., l_{n_K}^K\rbrace$ for $\mathcal{P}'_K$, the dual space of $\mathcal{P}_K$. 
\end{itemize}
This definition allows to impose constraints on the set of basis functions $\lbrace \phi_1^K, \phi_2^K, ..., \phi_{n_K}^K\rbrace$ spanning $\mathcal{P}_K$. For instance, to enforce a {\em nodal basis} for $\mathcal{P}_K$ -- a particularly useful property for expressing solutions in $U_h$ -- we can impose that the relationship

\begin{equation}
l_i^K(\phi_j^K) = \delta_{ij},\ i,j = 1,2,...,n_K
\end{equation}

must be satisfied ($\delta_{ij}$ is the Kronecker delta). This allows to express any $v \in \mathcal{P}_K$ as

\begin{equation}
v = \sum_{i=1}^{n_K} l_i^K(v) \phi_i^K.
\end{equation}

Each linear functional in $\mathcal{L}_K$ is used to evaluate one degree of freedom of $v$ in terms of the chosen nodal basis. In other words, we can refer to both the coefficients $U$ introduced in the previous section and $\mathcal{L}_K$ as the degrees of freedom.

\paragraph{Example: the triangular Lagrange element}
As an example\footnote{The example is extracted from~\cite{fenics-book}}, consider a triangular cell $K$ and let $\mathcal{P}_K$ be the space of polynomials of order $1$ on $K$. $\mathcal{L}_K$ may be the set of bounded linear functionals representing point evaluation at the vertices $\boldsymbol{x}^i$ ($i=1,2,3$) of $K$ such that

\begin{equation}
\centering
\begin{split}
l_i^K : \mathcal{P^K} \rightarrow \mathbb{R} \\
l_i^K(v) = v(\boldsymbol{x}^i)
\end{split}
\end{equation}

Since if $v$ is zero at each vertex then $v$ must be zero everywhere, $\mathcal{L}_K$ really is a basis for $\mathcal{P}'_K$, so what we have defined is indeed a finite element. In particular, if we take $\boldsymbol{x}^1 = (0, 0),\ \boldsymbol{x}^2 = (1,0),\ \boldsymbol{x}^3 = (0,1)$, we have that the nodal basis is given by:

\begin{equation}
\phi_1(\boldsymbol{x}) = 1 - x_1 - x_2,\ \ \phi_2(\boldsymbol{x}) = x_1,\ \ \phi_3(\boldsymbol{x}) = x_2.
\end{equation}


\subsection{Global Discretization}
\label{sec:bkg:refel}
A {\em local-to-global mapping} allows to patch together the finite elements to form a global function space, for instance the set of trial functions $U_h = \operatorname{span}\lbrace \psi_i \rbrace_{i=1}^N$ introduced in Section~\ref{sec:bkg:var-problems}. A local-to-global mapping is a function

\begin{equation}
\iota_K : [1,n_K] \rightarrow [1,N]
\end{equation}

that maps the local degrees of freedom $\mathcal{L}_K$ to global degrees of freedom $\mathcal{L}$. The mappings $\iota_K$, together with the choice of $\mathcal{L}_K$, determine the continuity of a function space or, in simpler words, the continuity of a function throughout the domain $\Omega$. The reader is invited to refer to~\cite{fenics-book} for a comprehensive yet simple description of this step.

One of the crucial aspects of an FEM is that global function spaces are often defined in terms of a {\em reference finite element} ${<}\hat{K}, \hat{\mathcal{P}}, \hat{\mathcal{L}}{>}$ and a set of invertible mappings $\lbrace \mathcal{G}_K\rbrace_{K}$ from $\hat{K}$ to each cell in the mesh such that $K = \mathcal{G}_K(\hat{K})$.  This situation is illustrated in Figure~\ref{fig:bkg:reference-el}.

For each $K$, $\mathcal{G}_K$ also allows to generate $\mathcal{P}_K$ and $\mathcal{L}_K$.  The complexity of this process depends on the mapping itself. In the simplest case, the mapping is affine; that is, expressible as $\mathcal{G}_K(\hat{\boldsymbol{x}}) = A_K \hat{\boldsymbol{x}} + b_K$, where $A_K$ and $b_K$ are, respectively, some matrix and vector.


\subsection{Assembly}
\label{sec:bkg:assembly}
The {\em assembly} of an FEM is the phase in which the matrix $A$ and the vector $b$ from~\ref{sec:bkg:eq:disc-op} are constructed. This is accomplished by adding the contributions from each $K$ to $A$ and $b$. Let us consider the bilinear form $a$, and assume this is an integral over $\Omega$. Thanks to the linearity of the operator, we can express $a$ as

\begin{equation}
a = \sum_{K} a_K
\end{equation}

where $a_K$ is an element bilinear form. We can then define the local element matrix

\begin{equation}
A_i^K = a_K (\psi_{i_1}^K, \phi_{i_2}^K)
\end{equation}

where $i \in \mathcal{I}_K$ is the index set on $A_i^K$. That is, $\mathcal{I}_K = \lbrace (1,1), (1,2), ..., (n_U, n_V) \rbrace$, with $n_U$ and $n_V$ representing the number of degrees of freedom for the local trial functions $\psi^K \in U_h^K$ and the local test functions $\phi^K \in V_h^K$. The element matrix $A^K$ is therefore a (typically dense) matrix of dimension $n_U \times n_V$.

Now let $\iota_K^U$ and $\iota_K^V$ be the local-to-global mappings for the local discrete function spaces $U_h^K$ and $V_h^K$. We can define, for each $K$, the collective local-to-global mapping $\iota_K : \mathcal{I}_K \rightarrow \mathcal{I}$ such that

\begin{equation}
\iota_K (i) = (\iota_K^U(i_1), \iota_K^V(i_2))\ \forall i \in \mathcal{I}_K.
\end{equation}

This simply maps a pair of local degrees of freedom to a pair of global degrees of freedom. Let also $\mathcal{T}$ be the subset of the cells in the mesh in which $\psi_{i_1}$ and $\phi_{i_2}$ are both non-zero. Note that here we are talking about the global functions whose restrictions to $K$ gives $\psi_{i_1}^K$ and $\phi_{i_2}^K$. By construction, $\iota_K$ is invertible if $K \in \mathcal{T}$. At this point, we have all the ingredients to compute $A$ as the sum of local contributions from the cells in the mesh:

\begin{equation}
\begin{split}
A_i & = \sum_{K \in \mathcal{T}} a_K (\psi_{i_1},\phi_{i_2}) \\
& = \sum_{K \in \mathcal{T}} a_K(\psi_{(\iota_K^U)^{-1}(i_1)}^K, \phi_{(\iota_K^V)^{-1}(i_2)}^K) = \sum_{K \in \mathcal{T} A_{\iota_K^{-1}(i)}^K}
\end{split}
\end{equation} 

Similar conclusions may be drawn for the linear form $L$. We observe that this computation can be implemented as a single iteration over all cells in the mesh. On each cell, $A^K$ is computed and added to $A$ using the inverse mappings. This approach is particularly efficient because it only evaluates the non-zero entries in the sparse matrix $A$. Other more trivial implementations of the assembly phase are possible, although rarely used in practice because ineffective. 

We conclude with a clarification concerning the terminology. The assembly process is often naturally described as a two-step procedure: {\em local assembly}  and {\em global assembly}. The former consists of computing the contributions of each single element (i.e., the element matrices $A^K$); the latter represents the ``coupling'' of all $A^K$ into $A$. As we shall see, one of the main subjects of this thesis is the computational optimization of the local assembly phase.

\subsection{Local Assembly Example: from Math to Code}
\label{sec:bkg:math-to-code}
Because of its relevance in this thesis, we illustrate local assembly in a concrete example, the evaluation of the element matrix for a Laplacian operator. 

\subsubsection{Specification of the Laplacian operator}
Consider the weighted Laplace equation

\begin{equation}
- \nabla \cdot (w \nabla u) = 0
\end{equation}

in which $u$ is unknown, while $w$ is prescribed. The bilinear form associated with the weak variational form of the equation is:

\begin{equation}
\label{sec:bkg:eq:spec-laplacian}
a(v, u) = \int_\Omega w \nabla v \cdot \nabla u\ \mathrm{d}x
\end{equation}

The domain $\Omega$ of the equation is partitioned into a set of cells (elements) $T$ such that $\bigcup T = \Omega$ and $\bigcap T = \emptyset$. Assuming for simplicity that the sets of trial and test functions are the same and by defining $\lbrace \phi_i^K \rbrace$ as the set of local basis functions spanning $U$ and $V$ on the element $K$, we can express the local element matrix as

\begin{equation}
\label{sec:bkg:stiffness}
A_{ij}^K = \int_K w \nabla \phi_i^K \cdot \nabla \phi_j^K\ \mathrm{d}x
\end{equation}

The element vector $L$ can be determined in an analogous way. 


In this example, the element tensors are expressed as a single integral over the cell domain. In general, they are expressed as a sum of integrals over $K$, each integral being the product of derivatives of functions from sets of discrete spaces and, possibly, functions of some spatially varying coefficients. Such an integral is often called \textit{monomial}. 

\subsubsection{Quadrature Mode}
A quadrature scheme is typically used to numerically evaluate $A_{ij}^K$. It consists of evaluating an integral at a set of given {\em quadrature points}, each point multiplied with some suitable {\em quadrature weight}. By mapping the computation to a reference element as explained in Section~\ref{sec:bkg:refel} and using the same approach as in Section~\ref{sec:bkg:assembly}, a quadrature scheme for the Laplacian operator on $K$ is as follows

\begin{equation}
\begin{split}
A^K & = \int_K w \nabla \phi_i^K \cdot \nabla \phi_j^K \\
& \approx w \sum_{k=1}^{N_q} W_k \nabla \phi_i^K(\boldsymbol{x}^k) \cdot \nabla \phi_i^K (\boldsymbol{x}^k) | \operatorname{det} \mathcal{G}_K'(\boldsymbol{x}^k) |,
\end{split}
\end{equation} 

where $\lbrace \boldsymbol{x}^1, \boldsymbol{x}^2, ..., \boldsymbol{x}^{N_q}$ is the set of $N_q$ quadrature points, and $\lbrace W_1, W_2, ..., W_{N_q} \rbrace$ the corresponding set of quadrature weights scaled such that $\sum_{k=1}^{N_q} W_k = |\hat{K}|$. 

To compute a local basis function $\phi_i^K$ from a reference element basis function $\Phi_i$ we exploit the invertible map $\mathcal{G}_K^{-1}$, which allows us to write $\phi_i^K = \Phi_i \circ \mathcal{G}_K^{-1}$. To evaluate the gradient of a basis function $\phi_i^K$ at a quadrature point $\boldsymbol{x}^k$, with $\boldsymbol{x}^k = \mathcal{G}_K(\boldsymbol{X}^k)$ and $\boldsymbol{X}^k \in \hat{K}$, we therefore have to compute a matrix-vector product

\begin{equation}
\nabla_x \phi_i^K(\boldsymbol{x}^k) = ((\mathcal{G}_K')^{-1})^{T}(\boldsymbol{x}^k) \nabla_X \Phi_i(\boldsymbol{X}^k).
\end{equation}

The term $\mathcal{G}_K')^{-1}$ represents the inverse of the Jacobian matrix originating from the change of coordinates. The resulting scalar-valued expression for each entry $A_{ij}^K$, assuming $\Omega$ to be a domain of dimension $d$, reads as:

\begin{equation}
\label{eq:quadrature}
A_{ij}^K = \sum_{k=1}^{N_q} \sum_{\alpha_3=1}^n \phi_{\alpha_3}(\boldsymbol{X}^k) w_{\alpha_3} \sum_{\alpha_1=1}^d \sum_{\alpha_2=1}^d \sum_{\beta=1}^d \frac{\partial X_{\alpha_1}}{\partial x_{\beta}} \frac{\partial \phi_i^K(\boldsymbol{X}^k)}{\partial X_{\alpha_1}} \frac{\partial X_{\alpha_2}}{\partial x_{\beta}} \frac{\partial \phi_j^K(\boldsymbol{X}^k)}{\partial X_{\alpha_2}} \operatorname{det} \mathcal{G}_K' W^k.
\end{equation}


\subsubsection{Tensor Contraction Mode}
%\label{sec:tc}
Consider the case in which $\mathcal{G}_K : \hat{K} \rightarrow K$ is an affine mapping. Starting from Equation~\ref{eq:quadrature} and exploiting linearity, associativity and distributivity of the involved mathematical operators, we can rewrite \ref{eq:quadrature} as
\begin{equation}
\label{eq:tensor}
A_{ij}^K = \sum_{\alpha_1=1}^d \sum_{\alpha_2=1}^d \sum_{\alpha_3=1}^n \operatorname{det} \mathcal{G}_K' w_{\alpha_3} \sum_{\beta=1}^d \frac{X_{\alpha_1}}{\partial x_{\beta}} \frac{\partial X_{\alpha_2}}{\partial x_{\beta}} \int_{K_0} \phi_{\alpha_3} \frac{\partial \phi_{i_1}}{\partial X_{\alpha_1}} \frac{\partial \phi_{i_2}}{\partial X_{\alpha_2}} dX.
\end{equation}
A generalization of this transformation has been proposed in~\cite{Kirby:TC}. Because of only involving reference element terms, the integral in the equation can be pre-evaluated and stored in temporary variables. The evaluation of the local tensor can then be abstracted as
\begin{equation}
A_{ij}^K = \sum_{\alpha} A_{i_1 i_2 \alpha}^0 G_{K}^\alpha
\end{equation}
in which the pre-evaluated ``reference tensor'' $A_{i_1 i_2 \alpha}$ and the cell-dependent ``geometry tensor'' $G_{K}^\alpha$ are exposed. 


\subsubsection{Qualitative comparison}
%\label{sec:qualitative}
Depending on form and discretization, the relative performance of the two modes, in terms of the operation count, can vary quite dramatically. The presence of derivatives or coefficient functions in the input form increases the rank of the geometry tensor, making the traditional quadrature mode preferable for ``complex'' forms. On the other hand, speed-ups from adopting tensor mode can be significant in a wide class of forms in which the geometry tensor remains ``sufficiently small''. The discretization, particularly the relative polynomial order of trial, test, and coefficient functions, also plays a key role in the resulting operation count. 

These two modes have been implemented in the FEniCS Form Compiler~\cite{FFC-TC}, which we review in later sections. In this compiler, a heuristic is used to choose the most suitable mode for a given form. It consists of analysing each monomial in the form, counting the number of derivatives and coefficient functions, and checking if this number is greater than a constant found empirically (\cite{Fenics}). In Chapter~\ref{ch:optimality}, we will describe a code generation system that goes beyond the dichotomy between quadrature and tensor modes, relying on cost models comparing the impact of different rewrite operators (e.g., expansion of products, factorization).


\subsubsection{Code Examples}
\label{sec:bkg:mathcode}
A possible C implementation of Equation~\ref{eq:quadrature} is illustrated in Listing~\ref{code:weighted-laplace}. We assume a domain of dimension $d=2$ and polynomial order $1$ Lagrange elements. The values at the quadrature points of the derivatives of the basis functions are pre-tabulated in the \texttt{B} and \texttt{D} arrays (representing, respectively, the derivatives with respect to the coordinates $x$ and $y$). The values at the quadrature points of the basis functions spanning the field $w$ are pre-tabulated in the array \texttt{C}. Pre-tabulation, which is made possible by mapping the computation to a reference element, is of fundamental importance to speed-up the local assembly phase. The summation over the $N_q = 6$ quadrature points is implemented by the \texttt{i} loop. The \texttt{r} loop implements the summation over $\alpha_3$ for discretization of the weight $w$. Given their small range, the summations over the spatial dimensions $\alpha_1$, $\alpha_2$ and $\beta$ have all been expanded in the ``assembly expression'' that evaluates the local element matrix $A$. The $K$ array includes the four components of the inverse of the Jacobian matrix for the change of coordinates. 

\begin{algorithm}
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}

\KwSty{void} weighted$\_$laplace(\KwSty{double} A[3][3], \KwSty{double} **coords, \KwSty{double} w[3]) \\
$\lbrace$ \\
~~// Compute Jacobian \\
~~\KwSty{double} J[4]; \\
~~compute$\_$jacobian$\_$triangle$\_$2d(J, coords); \\
~~\\
~~// Compute Jacobian inverse and determinant \\
~~\KwSty{double} K[4]; \\
~~\KwSty{double} detJ; \\
~~compute$\_$jacobian$\_$inverse$\_$triangle$\_$2d(K, detJ, J); \\
~~\KwSty{const double} det = fabs(detJ); \\
~~\\
~~// Quadrature weights \\
~~\KwSty{static const double} W[6] = {0.5}; \\
~~\\
~~// Basis functions \\
~~\KwSty{static const double} B[6][3] = $\lbrace\lbrace$...$\rbrace\rbrace$ ;\\
~~\KwSty{static const double} C[6][3] = $\lbrace\lbrace$...$\rbrace\rbrace$ ;\\
~~\KwSty{static const double} D[6][3] = $\lbrace\lbrace$...$\rbrace\rbrace$ ;\\
~~\\
~~\KwSty{for} (\KwSty{int} i = 0; i < 6; ++i) $\lbrace$ \\
~~~~\KwSty{double} f0  = 0.0;\\
~~~~\KwSty{for} (\KwSty{int} r  = 0; r < 3; ++r) $\lbrace$ \\
~~~~~~f0 += (w[r] * C[i][r]);\\
~~~~$\rbrace$ \\
~~~~\KwSty{for} (\KwSty{int} j = 0; j < 3; ++j) $\lbrace$\\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k < 3; ++k) $\lbrace$\\
~~~~~~~~A[j][k] += (((((K[1]*B[i][k]) + (K[3]*D[i][k])) * \\
~~~~~~~~~~~~~~~~~~~~~((K[1]*B[i][j]) + (K[3]*D[i][j]))) + \\
~~~~~~~~~~~~~~~~~~~~(((K[0]*B[i][k]) + (K[2]*D[i][k])) * \\
~~~~~~~~~~~~~~~~~~~~~((K[0]*B[i][j]) + (K[2]*D[i][j]))))*det*W[i]*f0);\\
~~~~~~$\rbrace$\\
~~~~$\rbrace$\\
~~$\rbrace$\\
$\rbrace$
\caption{A possible implementation of Equation~\ref{eq:quadrature} assuming a 2D triangular mesh and polynomial order $1$ Lagrange basis functions.}
\label{code:weighted-laplace}
\end{algorithm}

\begin{algorithm}
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}

\KwSty{void} burgers(\KwSty{double} A[12][12], \KwSty{double} **coords, \KwSty{double} **w) \\
$\lbrace$ \\
~~// Compute Jacobian \\
~~\KwSty{double} J[9]; \\
~~compute$\_$jacobian$\_$triangle$\_$3d(J, coords); \\
~~\\
~~// Compute Jacobian inverse and determinant \\
~~\KwSty{double} K[9]; \\
~~\KwSty{double} detJ; \\
~~compute$\_$jacobian$\_$inverse$\_$triangle$\_$3d(K, detJ, J); \\
~~\KwSty{const double} det = fabs(detJ); \\
~~\\
~~// Quadrature weights \\
~~\KwSty{static const double} W[5] = $\lbrace$...$\rbrace$\\
~~\\
~~// Basis functions \\
~~\KwSty{static const double} B[5][12] = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~\KwSty{static const double} C[5][12] = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~//11 other basis functions definitions \\
~~...\\
~~\KwSty{for} (\KwSty{int} i = 0; i$<$5; i++) $\lbrace$\\
~~~~\KwSty{double} f0 = 0.0;\\
~~~~//10 other declarations (f1, f2,...)\\
~~~~...\\
~~~~\KwSty{for} (\KwSty{int} r = 0; r$<$12; r++) $\lbrace$\\
~~~~~~f0 += (w[r][0]*C[i][r]);\\
~~~~~~//10 analogous statements (f1, f2, ...)\\
~~~~~~\\
~~~~$\rbrace$\\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$12; j++) $\lbrace$\\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$12; k++) $\lbrace$\\
~~~~~~~~A[j][k] += ...\\
~~~+ ((K[5]*F9) + (K[8]*F10))*Y1[i][j]) + ... + \\
~~~+ (((K[0]*C[i][k]) + (K[3]*D[i][k]) + (K[6]*E[i][k]))*Y2[i][j]))*f11) + \\
~~~+ (((K[2]*C[i][k]) + (K[5]*D[i][k]) + (K[8]*E[i][k]))*((K[2]*E[i][j]) + ...))) + \\
~~~+ $<$roughly a hundred sum/muls go here$>$)..)*det*W[i];\\
~~~~~~$\rbrace$\\
~~~~$\rbrace$\\
~~$\rbrace$ \\
$\rbrace$
\caption{Local assembly implementation for a Burgers problem on a 3D mesh using polynomial order $q=1$ Lagrange basis functions.}
\label{code:burgers}
\end{algorithm}

The evaluation of integrals becomes more computationally expensive if the complexity of a variational form grows, in terms of number of coefficients and tensor algebra or differential operators employed. It is not pathological a scenario in which the computation of a local element tensor requires more than hundreds or even thousands of floating point operations. An excerpt from one such example is shown in Listing~\ref{code:burgers}: here, in the main expression, $14$ unique arrays are accessed (with the same array referenced multiple times within the expression) along with several other constants. The loop trip counts are also larger due to the different domain discretization employed. As we shall see, automated code generation for finite element operators has had two main objectives:

\begin{itemize}
\item relieving the implementation burden when it comes to translate into code non-trivial operators, a tedious, error-prone task;
\item facilitating the introduction of techniques to reduce the operation count and powerful low-level optimizations.
\end{itemize}

The second point, as already anticipated, is one of the topics treated in this thesis.


\subsection{Linear Solvers}
\label{sec:bkg:linearsolvers}
The last step of an FEM is the resolution of the linear system (\ref{sec:bkg:eq:lin-sys}) arising from the variational form of the input problem. This and the assembly of $A$ and $b$ are the most expensive phases of an FEM. There is a whole science concerning the efficient resolution of linear systems. Among the most effective solvers are the family of {\em Krylov-type iteration methods}, such {\em conjugate gradient} for symmetric positive-definite matrices and {\em generalized minimal residual} (GMRES), which does not require $A$ in explicit form. {\em Multi-grid} methods are also widely used, whereas {\em direct methods} computing an LU factorization using {\em Gaussian elimination} have limited applicability. 

The resolution of linear systems is not one of the topics of this thesis, so the interested reader is invited to refer to~\cite{good-linear-system-source}. It is however important to keep in mind that this phase usually has a significant impact on the execution time of an FEM: the performance optimization of the assembly phase has marginal impact if the method is solver-dominated. 




\section{Abstractions in Computational Science}
\label{sec:bkg:abstractions}

This thesis centres on performance optimizations for scientific codes targeting different layers of abstraction. In this section, we dive into such abstractions and review the state-of-the-art on established software. The objective is to provide a foundation upon which motivating and explaining the contributions in Chapters~\ref{ch:sparsetiling},~\ref{ch:optimality} and~\ref{ch:lowlevelopt}.

%We adopt a top-down approach: we start discussing high-level languages for specifying numerical methods in mathematical notation and we finish with explaining the automatic parallelization on clusters of multi-core nodes.


\subsection{Automating the Finite Element Method}
\label{sec:bkg:fenics-and-firedrake}

The need for rapid implementation of high performance, robust, and portable scientific simulations has led to approaches based on automated code generation. This has been proven extremely successful in the context of the FEniCS (\cite{Fenics}) and Firedrake (\cite{firedrake-paper}) projects, which target the finite element method. In these frameworks, the weak variational form of a problem is expressed at high level by means of a domain-specific language. The mathematical specification is then manipulated by a form compiler that generates a representation of the assembly operators. The representation is transformed for performance optimization and subsequently translated into C code. The generated code is compiled and executed over the mesh. This entire process occurs at run-time because both FEniCS and Firedrake are implemented in Python and rely on just-in-time code generation and compilation. When the operators are assembled, a linear system needs be solved. For this, the PETSc library~\cite{petsc-cite} is employed. In the following, we describe the aspects of this tool-chain that are relevant for the following chapters. We focus on Firedrake, rather than FEniCS, because all algorithms and techniques developed in this thesis have been integrated with this framework.

\subsubsection{Problem Specification}
Firedrake uses a mathematical language called UFL, the {\em Unified Form Language}~\cite{ufl-cite}. UFL is unrelated to meshes, function spaces, and solvers. It only concerns with the variational formulation of a problem, by providing different kinds of operators -- like \texttt{grad} (gradient) and \texttt{inner} (inner product), but also algebraic operators (e.g., \texttt{transpose}, \texttt{inverse}) and elementary functions (e.g., \texttt{abs}, \texttt{sqrt}). Before emitting a representation suitable for the underlying compiler, UFL analyzes the form to collect useful information and applies some preliminary transformations, including automatic differentiation.

\begin{algorithm}[h]
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}

element = \textcolor{RedOrange}{FiniteElement} (\textcolor{ForestGreen}{'Lagrange'}, \textcolor{RedOrange}{triangle}, 1)\\
~\\
u = \textcolor{RedOrange}{TrialFunction} (element)\\
v = \textcolor{RedOrange}{TestFunction} (element)\\
w = \textcolor{RedOrange}{Coefficient} (element)\\
~\\
a = w*\textcolor{RedOrange}{dot} (\textcolor{RedOrange}{grad}(v), \textcolor{RedOrange}{grad}(u))*\textcolor{RedOrange}{dx}\\

\caption{UFL specification of the weighted Laplace operator defined in (\ref{sec:bkg:eq:spec-laplacian}). In orange the keywords of the language. }
\label{code:ufl-laplace}
\end{algorithm}

A UFL representation of the weighted Laplace operator shown in (\ref{sec:bkg:eq:spec-laplacian}) is given in Listing~\ref{code:ufl-laplace}. When constructing a finite element, three pieces of information are specified: {\em family}, {\em cell}, and {\em polynomial degree}. The family allows varying the type of the element. UFL supports many types, ranging from the most standard {\em Lagrange} element to {\em Discontinuous Lagrange} and even mixed elements such as {\em H(div)} and {\em H(curl)}. This allows solving problems with the most disparate requirements on continuity of the functions. A thorough description is provided in~\cite{fenics-book}. The cell represents the shape of the reference element: possible values include {\em triangle}, {\em quadrilateral} and {\em tetrahedron}. The polynomial order drives the number of degrees of freedom in an element. Functions can also be vector-valued, in which case one must use the special construct {\em VectorElement} in place of {\em FiniteElement}. 

We are not interested in a complete review of UFL, which would also require a knowledge of the finite element method that goes far beyond the tool-kit we need in this thesis. It is however worth appreciating the power, versatility and depth of this language, since it opened a new era in the development of computational methods.


\subsubsection{Form Compilers}
The UFL specification of a variational form is eventually passed to a form compiler, whose objective is to construct a representation of the assembly operators. The {\em FEniCS Form Compiler}, or FFC, was originally adapted and used in Firedrake for this task. FFC, which supports both quadrature and tensor modes (see Section~\ref{sec:bkg:math-to-code}), used to directly emit C code. It was later modified (one of the initial steps of this thesis) to rather output abstract syntax trees, a structure allowing further manipulation in the lower layers of the stack. More recently, FFC has been supplanted by a new system, the {\em Two-Stage Form Compiler}, or TSFC. Just like FFC, TSFC emits abstract syntax trees (ASTs). The optimizations described in Chapters~\ref{ch:optimality} and~\ref{ch:lowlevelopt} are implemented by manipulating the AST representation in a lower-level compiler, COFFEE (outlined in Chapter~\ref{ch:coffee}). 

TSFC has two main features:
\begin{itemize}
\item It is a \textit{structure-preserving compiler} in that it keeps intact the structure of linear algebra operations. Rather than committing to a specific implementation strategy, it preserves the algebraic operations (e.g., index sums, inner products), thus letting the lower-level compiler to easily explore the space of all possible transformations.
\item As opposed to FFC, it supports the compilation of complicated forms making extensive use of tensor algebra. TSFC can efficiently identify repeated sub-expressions and assign them to temporary variables, thus drastically reducing the code generation time.
\end{itemize}

From this overview, it is clear that in Firedrake there is a neat separation of concerns:
\begin{itemize}
\item UFL is the mathematical language that allows expressing finite element problems.
\item TSFC progressively abstracts away finite element from ...
\item COFFEE handles performance optimization
\end{itemize}

Prior to this thesis, a form compiler ...


\subsubsection{Iteration over the Mesh}
\label{sec:bkg:meshiteration}
\begin{description}
\item[Structured Meshes] ...
\item[Unstructured Meshes] OP2, PyOP2, Lizst
\end{description}

\subsection{The OP2 Active Library}
\label{sec:bkg:op2}

\begin{algorithm}
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}

void kernel1 (double * x, double * tmp1, double * tmp2) {
  *tmp1 += *x;
  *tmp2 += *x;
}
// loop over edges
op$\_$par$\_$loop (edges, kernel1,
  op$\_$arg$\_$dat (x, -1, OP$\_$ID, OP$\_$READ),
  op$\_$arg$\_$dat (temp, 0, edges2vertices, OP$\_$INC),
  op$\_$arg$\_$dat (temp, 1, edges2vertices, OP$\_$INC))

// loop over cells
op$\_$par$\_$loop (cells, kernel2,
  op$\_$arg$\_$dat (temp, 0, cells2vertices, OP$\_$INC),
  op$\_$arg$\_$dat (temp, 1, cells2vertices, OP$\_$INC),
  op$\_$arg$\_$dat (temp, 2, cells2vertices, OP$\_$INC),
  op$\_$arg$\_$dat (res, -1, OP$\_$ID, OP$\_$READ))

// loop over edges
op$\_$par$\_$loop (edges, kernel3,
  op$\_$arg$\_$dat (temp, 0, edges2vertices, OP$\_$INC),
  op$\_$arg$\_$dat (temp, 1, edges2vertices, OP$\_$INC))

\caption{...}
\label{code:op2program}
\end{algorithm}

%TODO : paste description from ESA report


\subsection{Stencil Languages}
...


\section{Compilers and Libraries for Code Optimization}
\label{sec:bkg:codeopt}


\subsection{Loop Optimization - Static Analysis}
\label{sec:bkg:poly}
Polyhedral compilers ...
scheduling functions
Mention their unsuitability for tiling unstructured meshes... (use email I sent listing all issues...)




\subsection{Loop Optimization - Dynamic Analysis}
\label{sec:bkg:ie}
\begin{itemize}
\item Inspector/Executor schemes
\item Space filling curves for unstructured meshes
\item sparse tiling = loop fusion + loop tiling
\end{itemize}


\subsection{On the Dichotomy between Loop Tiling and Loop Fusion}
\label{sec:bkg:tiling}

\begin{itemize}
\item Tiling is for a nested loop ! 
\item tiles are atomic blabla
\item split vs overlapped
\end{itemize}


\subsubsection{Split tiling}


\subsubsection{Overlapped tiling}
 %These techniques work by dividing a loop nest's iteration space into a number of overlapping regions called tiles. These tiles are shaped such that each of them can execute in parallel without requiring communication.  
 
 \subsubsection{Fusion vs Tiling}
 \label{sec:bkg:sparsetiling}
 so time tiling is just fusion across the time stepping loop, then tiling
% IDEA: time skewing is just fusion. If it's a nested loop nest, you do tiling; but across loops at the same level of a block, you always do fusion. This way, SFCs and tiling are instances of scheduling functions for a nested loop; what is called "time tiling" or "time skewing" is just a shorthand for "fuse the loops within the time stepping loop".

%Sparse tiling techniques were developed to group iterations of irregular applications
%into atomic tiles at runtime with an 
%inspector~\citep{dimeEtna00,StroutIJHPCA,StroutPLDI03,commAvoidingSparse2009}.
%In general, the inspector iterates over index arrays that do not change during
%the main computation to determine data reorderings or new schedules, like
%sparse tiling schedules.
%The resulting tiles have either an implicit or explicit partial ordering (i.e., a task graph)
%that exposes asynchronous parallelism~\citep{Adams99c,dimeEtna00,StroutLCPC2002}.
%These benchmark-specific, sparse tiling executors
%exhibited performance improvements for sparse stencil computations, 
%%FIXME: dime?Jacobi, % FIXME:~\cite{dime?}, 
%Gauss-Seidel~\citep{Adams99c,StroutIJHPCA}, moldyn~\cite{StroutPLDI03},
%and sparse matrix powers kernel~\cite{commAvoidingSparse2009}.

%\subsection{Space Filling Curves}
%...

\subsection{Domain Specific Optimization}
...

\subsubsection{Tensor Contraction Engine}
...

\subsubsection{LGen}
Why potentially useful ...

\subsubsection{Halide}
...

\subsubsection{Spiral}
...

\section{State-of-the-art Hardware Architectures}
\label{sec:bkg:arch}
...

\subsection{SIMD Vectorization}
...

\subsection{Terminology}
\label{sec:bkg:terminology}

\begin{description}
\item[Memory pressure, Register pressure]
\item[Arithmetic intensity]
\item[Operational intensity] Maybe add a subsection of the roofline model ?
\item[Flops]
\item[Access function (for array)]
\item[General-purpose compiler]
\item[Communication vs computation]
\item[CPU- vs Memory-bound]
\item[Local vs Global reduction]
\end{description}
