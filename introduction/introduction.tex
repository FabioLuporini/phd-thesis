\chapter{Introduction}

\section{Thesis Statement}
%We explore automated performance optimization of numerical methods based upon unstructured meshes. 

%The performance optimization of numerical methods based upon unstructured meshes is a challenging task that requires knowledge of the computational domain as well as expertise in computer architecture and software engineering. This burden can be relieved by raising the level of abstraction: applications are expressed by means of domain-specific languages, while optimization is automated through high-level compilers and run-time supports. The research productivity, with high-performance, structured, and extendible software being developed, is thus maximized.

%We exploit domain-specific languages to automate the performance optimization of numerical methods based upon unstructured meshes. 

%Maximizing the performance of numerical methods based upon unstructured meshes is a challenging task that can be relieved by raising the level of abstraction: if applications are expressed with domain-specific languages, powerful optimizations can be automated through compilers without the need for complex source code analysis. 

Maximizing the performance of numerical methods for partial differential equations is a challenging task that can be facilitated by raising the level of abstraction, through which automated optimization becomes possible.

\section{Overview}
In many fields such as computational fluid dynamics, computational electromagnetics and structural mechanics, phenomena are modelled by partial differential equations. Numerical techniques, such as the finite volume method and the finite element method, are widely employed to approximate their solutions. Unstructured meshes are often used to discretize the computational domain, since they allow an accurate representation of complex geometries. The solution is sought by applying suitable numerical operations, or kernels, to the entities of a mesh, such as edges, vertices, or cells. On typical clusters of multi-core processors, usually, a kernel is executed sequentially by a process, while parallelism is achieved by assigning mesh partitions to different nodes or processes. This execution model is adopted by many real-world applications and frameworks.

The time required to apply the numerical kernels is a major issue, since the equation domain needs be discretized into an extremely large number of cells to obtain a satisfactory approximation of the solution, possibly of the order of trillions, as in \cite{Rossinelli2013}. For example, it has been well established that mesh resolution is critical in the accuracy of numerical weather forecasts. However, operational forecast centers have a strict time limit in which to produce a forecast - 60 minutes in the case of the UK Met Office. Producing efficient kernels or improving the mesh iteration has a direct scientific payoff in higher resolution, and therefore more accurate, forecasts. 
%Optimizing the computational cost is a critical problem for most scientific simulations.

This thesis shows that the use of domain-specific languages for expressing a class of numerical methods based on unstructured meshes plays a crucial role in performance optimization. In the translation of the problem specification into low level code (e.g., C), a stack of compilers applies sophisticated transformations. Ideally, these transformations would manually be written, specialized, and tuned for each application and for each architecture. This is however unrealistic, as all fundamental rules of software engineering, including maintainability, extendibility, and modularity, would be violated.

As we shall demonstrate, optimizing applications via domain-specific compilers has significant advantages. First, simplicity: both the program structure and domain properties are captured by the high level syntax, which invaluably simplifies the compiler's job. Second, portability: multiple applications use the same transformation pipeline, and execution on heterogeneous architectures may be supported. Third, separation of concerns. Application specialists focus on expressing simulations with a notation close to the mathematics of textbooks, whereas performance optimization is hidden in the lower abstraction layers. 

\section{Thesis Outline and Contributions}
\label{sec:contributions}
This thesis investigates two main topics: (i) the generalization of a compiler transformation known as {\em sparse tiling}, whose aim is to improve data locality in sequences of irregular loops; (ii) the optimization of so called {\em local assembly kernels} in finite element methods. In fact, the latter spans two research directions: (ii-a) the minimization of the operation count; (ii-b) the low level optimization of the code resulting from (ii-a), through novel and state-of-the-art loop transformations. For both (i) and (ii), {\it automation via domain-specific compilers} is a distinguishing feature. The order in which these topics are covered is detailed below.

The thesis comprises seven chapters, including the present introduction and the conclusions. Chapter~\ref{ch:background} establishes the foundation for the contributions in the subsequent chapters. The basics of the finite element method, the state-of-the-art on automated code generation for unstructured mesh computations, and compiler optimization are reviewed. Chapters~\ref{ch:sparsetiling},~\ref{ch:optimality}, and~\ref{ch:lowlevelopt}, which respectively treat the aforementioned topics (i), (ii-a), and (ii-b), represent the core contributions of this thesis, as detailed next:

\begin{description}
\item[Chapter~\ref{ch:sparsetiling}] Sparse tiling is a transformation that fuses and tiles loops performing indirect memory accesses (e.g., {\tt A[B[i]]}). These loops are common when iterating over unstructured meshes. The research on sparse tiling was characterized by two phases:
\begin{enumerate}
\item The initial work was a joint international effort with people from several institutions: Michelle M. Strout, Christopher Krieger, and Catherine Olschanowsky (Colorado State University; fundamental contributions to the first version of the {\em generalized sparse tiling algorithm} for shared-memory architectures, performance evaluation of the Jacobi sparse matrix solver benchmark); Carlo Bertolli (IBM T. J. Watson; tiling algorithm, debugging); J. ``Ram'' Ramanujam (Louisiana State University; formalization); Gheorghe-Teodor Bercea and Paul H. J. Kelly (Imperial College; tiling algorithm, performance evaluation of the Airfoil benchmark). The design and the implementation of the first sparse tiling algorithm in the OP2 library were entirely performed by the author of this thesis. 
\item The work has later been extended in four directions: a more efficient sparse tiling algorithm for shared-memory architectures; support for distributed-memory architectures, with contributions from Michael Lange (Imperial College); automation in the Firedrake framework through the SLOPE library; extensive performance evaluation with a real-world application (Seigen, an elastic wave equation solver for seismological problems). SLOPE implements sparse tiling and is a by-product of this thesis.
\end{enumerate}

\item[Chapter~\ref{ch:optimality}] An algorithm for minimizing the operation count in a class of local assembly kernels is devised and evaluated. This algorithm, which exploits fundamental mathematical properties of finite element operators, is proven to achieve a locally optimal operation count. Rewrite operators, such as factorization and generalized loop-invariant code motion, as well as cost models are used to this purpose. The algorithm is implemented in COFFEE, a domain-specific compiler conceived and written by the author of this thesis. COFFEE is integrated with Firedrake, a system capable of solving partial differential equations expressed in mathematical syntax through the finite element method~\citep{firedrake-paper}. As such, the technology developed in this chapter is in use in a number of projects built on top of Firedrake.

\item[Chapter~\ref{ch:lowlevelopt}] The low level optimization of the code resulting from the minimization algorithm in Chapter~\ref{ch:optimality} is studied. A number of transformations, both novel (inspired by the kernel structure and/or domain properties) and well-known (although specialized to take advantage of kernel properties), are introduced. SIMD vectorization and register locality on state-of-the-art CPUs are the primary target. The implementation is carried out in COFFEE, hence the technology developed is available to Firedrake users. Some transformations -- the ones which are proven to provide systematic performance improvements across a range of problems -- are automatically introduced during the default optimization process performed by COFFEE. Extensive performance evaluation provides compelling evidence about the effectiveness of the transformations. In particular, the Intel Xeon Phi experimentation was performed in collaboration with Ana Lucia Varbanescu (Delft University of Technology).
\end{description}

Finally, Chapter~\ref{ch:coffee} describes the conception, architecture and interface of COFFEE. This is a technical chapter, in which implementation choices and optimization strategies are discussed.

Summarizing, this thesis advances the state-of-the-art by introducing new performance optimizations for a class of unstructured mesh computations (with emphasis on the finite element method) and by automating them through two new software components: SLOPE and COFFEE.


\section{Dissemination}
The tools developed in this thesis are released under open source licenses. The underlying theory has been exposed to the scientific community through various publications and presentations.

\begin{description}
\item[Publications] From this thesis derive three main publications; a further publication is planned for the most recent achievements in sparse tiling. In chronological order:
\begin{itemize}
\item M.M. Strout, F. Luporini, C.D. Krieger, C. Bertolli, G.-T. Bercea, C. Olschanowsky, J. Ramanujam, and P.H.J. Kelly. {\em Generalizing run-time tiling with the loop chain abstraction}, in Parallel and Distributed Processing Symposium (IPDPS), 2014 (\cite{st-paper}). This conference paper generalizes sparse tiling to arbitrary sequences of irregular loops (i.e., Chapter~\ref{ch:sparsetiling}, first research phase on sparse tiling, as explained in Section~\ref{sec:contributions}).
\item F. Luporini, A.L. Varbanescu, F. Rathgeber, G.-T. Bercea, J. Ramanujam, D.A. Ham, and P.H.J. Kelly. {\em Cross-loop optimization of arithmetic intensity for finite element local assembly}, in ACM Transactions on Architectures and Code Optimizations (TACO), 2015 (\cite{Luporini-coffee}). This journal paper describes the work on the low level optimization of local assembly kernels (i.e., Chapter~\ref{ch:lowlevelopt}).
\item F. Luporini, D.A. Ham, P.H.J. Kelly. {\em An algorithm for the optimization of finite element integration loops}, submitted to ACM Transactions on Mathematical Software (TOMS), 2016 (\cite{Luporini-minimalflops}). This journal article introduces the operation count minimization algorithm for local assembly kernels (i.e., Chapter~\ref{ch:optimality}).
\end{itemize}
Other publications -- the result of collaborations with people at Imperial College -- are not reported here since they do not contribute directly to the thesis.
\item[Presentations] A number of formal and informal presentations were given at various conferences, workshops, and meetings. The most relevant, in chronological order, are:
\begin{itemize}
\item {\em Generalised Sparse Tiling for Unstructured Mesh Computations in the OP2 Framework.} Compilers for Parallel Computing (CPC) workshop, 2013.
\item {\em COFFEE: an Optimizing Compiler for Finite Element Local Assembly.} FEniCS workshop, 2014.
\item {\em Optimization of Arithmetic Intensity for Finite Element Assembly.} GungHo! workshop, 2014.
\item {\em Cross-loop Optimization of Arithmetic Intensity and Data Locality for Unstructured Mesh Applications.} Oxford Many-core seminars, 2014.
\item {\em Cross-loop Optimization of Arithmetic Intensity for Finite Element Local Assembly.} High Performance and Embedded Architecture and Compilation (HiPEAC) conference, 2015.
\item {\em Generating high performance finite element kernels using optimality criteria.} SIAM Parallel Processing for Scientific Computing (PP) conference, 2016.
\item {\em An algorithm for the optimization of finite element integration loops}. Platform for Research In Simulation Methods (PRISM) workshop, 2016.
\end{itemize}

\item[Software] The research described in this thesis has led to the development of two software components.
\begin{itemize}
\item COFFEE, a compiler for optimising the arithmetic intensity of expressions embedded in loop nests~\citep{coffee-code}.
\item SLOPE, a library for fusing sequences of loops characterized by indirect memory accesses~\citep{slope-code}.
\end{itemize}
\end{description}


