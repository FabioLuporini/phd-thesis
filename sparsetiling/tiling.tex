\chapter{Automated Tiling for Irregular Computations}
\label{ch:sparsetiling}

\section{Motivation}
\label{sec:tiling:motivation}
%mention dichotomy tiling/fusion...

%- irregular codes from pde-land are often memory bound (this however depends on the discretization)
%- investigation of tiling in real-world irregular codes

%- fully parallel loops ! with local reductions ... (diff with global reductions)



Many numerical methods for partial differential equations (PDEs) are structured as sequences of parallel loops, often interleaved by sparse linear algebra operations. This exposes parallelism well, but often does not convert data reuse between loops into data locality, since very large datasets are usually accessed. In Section~\ref{sec:bkg:poly} we have explained that loop fusion and loop tiling may be used to retain a significant fraction of this potential data locality. However, as elaborated in Section~\ref{sec:tiling:limits}, the complexity inherent in real-world applications make it challenging to adopt such optimizations in practice. 

Our focus in on unstructured mesh PDE solvers, such as those based on the finite volume or the finite element methods. Here, the  loop-to-loop dependence structure is data-dependent due to
indirect references such as \texttt{A[map[i]]}; the \texttt{map} array stores connectivity information, for example from elements in the mesh to degrees of freedom. A similar pattern occurs in molecular dynamics simulations and graph processing, so both the theory and the tools that we will develop in this chapter are generalizable to these domains. 

% Compressed version of the History of sparse tiling subsection: we see potential
Because of the irregular memory access pattern, our approach to loop optimization will be based on dynamic analysis, particularly on \textit{inspector/executor schemes automatically generated at execution time}. Our hypothesis, backed by the studies reviewed in Section~\ref{sec:bkg:ie}, is that dynamic loop optimization has strong potentials for the applications in which we are interested. The three main issues that we attack in this chapter are as follows:

\begin{itemize}
\item Previous approaches to irregular loop tiling/fusion are all based upon ``ad-hoc'' inspector/executor strategies; that is, developed ``by hand'', per application. We seek for a generalized technique, applicable to arbitrary computations on unstructured meshes.
\item For this research to be successful, we believe we need a fully automated system capable of optimizing real-world applications. Automation is essential because computational scientists must abstract from low level optimization. We aim for a mixed compiler/library based approach, to be integrated with a framework for solving PDEs.
\item There are only very few studies tackling loop fusion and inter-node parallelization. We are aware of none in presence of irregular memory accesses. Our technique must support MPI parallelization, because scientific simulations almost always run on at least hundreds of nodes.
\end{itemize}




\section{Limitations in Real-World Applications}
\label{sec:tiling:limits}

\subsection{Mesh-based Numerical Methods for PDEs}
Loop tiling is widely studied in the literature. In spite of a notable research effort, however, it is not clear how widespread this optimization is in real-world applications. Most studies centre their experimentation on relatively simple benchmarks and single-node performance; this unfortunately does not expose the complexity and the limitations of most scientific codes. On the other hand, it has repeatedly been shown that applying tiling to ``simple'' memory-bound loop nests can result in considerable speed-ups. The most striking examples are stencil codes arising in the finite difference method~\cite{stencil-tiling}, BLAS routines such as matrix multiplication~\cite{MKL}, and image processing kernels~\cite{Halide}. Since numerical methods for partial differential equations (PDEs) are often structured as sequences of parallelizable ``sweeps'' over the discretized equation domain, often implemented as memory-intensive loop nests, the following questions arise naturally: 

\begin{description}
\item[Applicability] Can we adopt loop tiling, whose application has traditionally been limited to simple loop nests, in real-life numerical methods for solving PDEs?
\item[Lack of evidence] Why, despite decades of research, is it so difficult to find successful examples of integration of loop tiling with production code? 
\item[Challenges] What are the theoretical and technical challenges that we have to overcome to automate this optimization, such that an entire community of computational scientists can benefit from it?
\end{description}

In this chapter, we will try answering these questions. Our study focuses on a particular class of applications:
\begin{description}
\item[Irregular codes] Unstructured meshes are often used to discretize the computational domain, since they allow an accurate representation of complex geometries. For this type of meshes, the connectivity is usually stored by means of adjacency lists (or any equivalent structure), which results in indirect memory accesses (e.g., \texttt{A[B[i]]}) within loop nests. Indirections break static analysis, thus making any compiler-based approach (e.g., polyhedral optimization) unsuitable for our context. Indirections also make data dependence analysis more difficult, since this must now take place at runtime, introducing additional overhead.
\item[Realistic dataset] Most complex simulations operate on at least gigabytes of data, requiring multi-node execution. Any tiling-based optimization we will consider must therefore cope well with MPI execution.
\item[Automation, but no legacy code] We view tiling as an ``extreme optimization''; that is, as an optimization that should be tried at the end of the development process, once the experimental results have been validated, to improve the execution time. In our experience, however, it is extremely rare that computational scientists have the expertise to add any low level optimizations, such as loop tiling, to their codes. In addition, applying these optimizations at the source level makes the code impenetrable and, therefore, almost impossible to maintain and to extend. Our aim is a fully automated technique abstracted to the users through a simple ``switch'' (i.e., loop tiling on/off) and a tiny set of parameters to drive performance tuning (e.g., the tile size). Automation clearly comes at the price of implementation complexity; we will integrate our tiling system with a generic framework for mesh iteration. We are therefore not interested in supporting legacy code, in which key computational aspects (e.g., mesh iteration, MPI communication) are usually hidden, for modularity, underneath several layers of software.
\end{description}

To support this class of applications and in order to maximize the chances that our work can be used in a variety of simulations in the years to come, our approach builds on two pillars:
\begin{itemize}
\item a library for writing inspector/executor schemes (\cite{IEscheme}) for tiling arbitrary computations on unstructured meshes (or graphs);
\item a multilayer framework based on DSLs and runtime code generation that uses such a library to automate the applciation of loop tiling.
\end{itemize}

\subsection{Hypotheses}
\label{sec:tiling:struct}

\begin{algorithm}
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}

// Time-stepping loop (T = total number of iterations)\\
\KwSty{for} t = 0 \KwSty{to} T $\lbrace$\\
~~// 1st sweep over the $C$ cells of the mesh\\
~~\KwSty{for} i = 0 \KwSty{to} $C$ $\lbrace$\\
~~~~buffer$\_$0 = gather$\_$data ( A[f(map[i])], ... )\\
~~~~...\\
~~~~kernel$\_$1( buffer$\_$0, buffer$\_$1, ... );\\
~~~~scatter$\_$data ( buffer$\_$0, f(map[i]) )\\
~~$\rbrace$\\
~~Calc ( ... );\\
~~MPI$\_$Comm ( ...); \\
~~// 2nd sweep over the $N$ nodes of the mesh\\
~~\KwSty{for} i = 0 \KwSty{to} $N$ $\lbrace$\\
~~~~... // Similar to sweep 1 \\
~~$\rbrace$\\
~~// Boundary conditions: sweep over the $BE$ boundary edges\\
~~\KwSty{for} i = 0 \KwSty{to} $BE$ $\lbrace$\\
~~~~... // Similar to sweep 1 \\
~~$\rbrace$\\
$\rbrace$
\caption{....}
\label{code:tiling-struct}
\end{algorithm}

The ``bare'' structure of a scientific code solving a PDE by applying a numerical method on a discretized domain is shown in Listing~\ref{code:tiling-struct}. In the next section, we will use this example to motivate two fundamental hypotheses:

\begin{description}
\item[Hypothesis 1] A wide range of loop nests in mesh-based codes is, from a computational viewpoint, memory-bound, and increasing the data reuse across consecutive loop nests may relieve this issue.
\item[Hypothesis 2] Automating loop tiling in this kind of codes presents challenges that none of the state-of-the-art technologies can overcome. Our approach aims to solve this problem.
\end{description}



% Is skewing used for optimizing cross-time dependency, i.e., is the wave due to time?

\subsection{Automating Loop Tiling is More Difficult than Commonly Thought}
We identify three classes of problems that are either neglected or treated disjointly in the relevant literature. 

\begin{description}
\item[Theoretical questions] We first of all wonder whether loop tiling really is the optimization we are looking for.
\begin{description}
\item[Computational Boundedness] Since most computational methods are structured as sequences of loop nests and each loop nest is characterized by its own operational intensity, the computational boundedness is a concept that does not refer to the whole application. Rather, some loop nests may be memory-bound, whereas others CPU-bound, due to the complexity of the equations being solved or the employment of high order function spaces. Clearly, if all loop nests in an application are CPU-bound, the benefits of tiling on data locality will tend to be marginal. Before applying an aggressive optimization such as loop tilling, it is therefore fundamental to study the computational behaviour of an application for: (i) determining what fraction of the execution time is due to memory-bound loop nests; (ii) understanding if CPU-boundedness can be eliminated by applying other optimizations (e.g., vectorization), or if it is instead a natural consequence of the kernels and the architecture.
\item[Tiling vs Space Filling Curves] It is our impression that different communities have disjointedly researched solutions to the problem of relieving the memory pressure of mesh-based computations for years. We view both tiling and space filling curves (SFCs) -- and likewise sequential execution -- as instances of possible scheduling functions for a given loop nest (see Section~\ref{sec:bkg:poly}). Nevertheless, we could not find studies comparing the performance of the two approaches, denoting a lack of communication between people in two distinct yet potentially very close domains: compiler optimization and computational science.

\end{description}

\item[Practical issues] Loop tiling for structured meshes is widely covered in the literature. Several studies have tackled automation (e.g., polyhedral compilers), time-loop tiling (i.e., time skewing), exotic tile shapes for minimizing communication (e.g., diamond tiling), and so on. However, the following challenges tend to be neglected, or at least only marginally tackled.
\begin{description}
\item[Unstructured meshes] A technique to tile arbitrary computations on unstructured meshes was missing until this thesis\footnote{We reinforce once more that the generalized tiling algorithm is actually the result of a joint collaboration between .... See~\cite{st-paper}.}. Some inspector-executor strategies for tiling specific benchmarks, as discussed in Section~\ref{sec:tiling:relatedwork}, were available though. 
\item[Time tiling and MPI] We reiterate the fact that real computations almost always run on distributed memory machines. The multi-node parallelization is often carried out resorting to MPI. This is evident in Listing~\ref{code:tiling-struct} where MPI calls are placed between two consecutive mesh sweeps, since the first cells loop produces data needed by the subsequent nodes loop. Distributed memory parallelism poses a big challenge for time tiling, because now tiles at the partition boundary need special handling.
\item[Time tiling and extra code] The \texttt{Comp(...)} function in Listing~\ref{code:tiling-struct} denotes the possibility that additional computation is performed between consecutive sweeps. This could be, for instance, check-pointing or I/O. Also, conditional execution of loops (e.g., through \texttt{if-then-else}) breaks time tiling. 
\item[Legacy code is usually impenetrable] Our experience is that real-life code for scientific simulation tends to hide the tiling opportunities that a polyhedral compiler, for example, would search for. As explained in~\cite{strout-common-problems}, common problems are: 1) finding tilable loop nests, which are usually hidden for code modularity; 2) handling of boundary conditions; 3) dist
\end{description}

\item[Limitations inherent in the numerical method] Two loops cannot be fused if they are separated by a global synchronization point. This is often a global reduction, either explicit (e.g., the first loop updates a global variable that will be needed by the second loop) or implicit (i.e., within an external function invoked in between the two loops, like in many iterative solvers for linear systems). By limiting the applicability of many loop optimizations, global synchronization points pose great challenges and research questions. If strong scaling is the primary goal and memory-boundedness is the key limiting factor, then fundamental questions are: (i) can the numerical method be reformulated to relieve the constraints on low level optimization (which requires a joint effort between people in different domains); (ii) can the tools be made more sophisticated to work around these problems; (iii) most importantly, will the effort be rewarded by significant performance improvements?
\end{description}

In this chapter, we will formulate answers to some of these problems.



%\subsection{Time-Tilable Codes and Possible Solutions}
%I've been trying to solve these issues. Especially points 4, 5 and 2 (thanks Michael for simplifying my life!)
%
%
%What's the solution we propose?
%DSL and automated code generation schemes. Goal is to generate code which is a *suitable input* for other tools capable of saving us "automagically" from the burden of optimising everything by ourself
%
%
%So, back to the original question: "Is non-trivial loop tiling really widely applied in real-world software ?" And I said "no" because of the aforementioned points. 
%
%Where are with REAL tiling? Who is actually using it in real codes? What sort of improvements did they get? How did they work around the MPI "problem"?
%
%And, finally:
%
%What do we want to say about that body of literature presenting increasingly sophisticated tiling schemes that *seem* to be rarely used in practice? What do we want to say and ask to people talking about tiling?
%
%I'd like to hear what you think about this.

\section{Related Work}
\label{sec:tiling:relatedwork}

\subsection*{Loop Chain}
% Loop chain
The data dependence analysis that we will develop is based on an abstraction called \textit{loop chain}, which was originally presented in~\cite{KriegerHIPS2013}. This abstraction is sufficiently general to capture data dependency in programs structured as arbitrary sequences of loops. We will detail our loop chain abstraction in Section~\ref{sec:tiling:lc}.

\subsection*{Inspector/Executor and Sparse Tiling}
% Inspector/Executor
The loop chain is the mechanism by means of which we will generalize inspector/executor schemes for unstructured codes. Inspector/executor strategies were first formalized by~\cite{Saltz91} and used to fuse and tile loops for improving data locality and providing parallelism in~\cite{dimeEtna00,StroutLCPC2002,Demmel08,KriegerIAAA2012}. 

Sparse tiling, which we introduced in Section~\ref{sec:bkg:ie}, is the most notable technique based upon inspection/execution. The term was coined by Strout et al.~\cite{StroutLCPC2002,StroutIJHPCA} in the context of the Gauss-Seidel algorithm and in~\cite{StroutPLDI03} in the context of the moldyn benchmark. However, the technique was initially proposed by Douglas et al.~\cite{dimeEtna00} to parallelize computations over unstructured meshes, taking the name of \textit{unstructured cache blocking}. The mesh was initially partitioned; the partitioning represented the tiling in the first sweep over the mesh. Tiles would then shrink by one layer of vertices for each iteration of the loop. This shrinking represented what parts of the mesh could be update in later iterations of the outer loop without communicating with the processes executing other tiles. The unstructured cache blocking technique also needed to execute a serial clean-up tile at the end of the computation. Mark Adams~\cite{Adams99c} also developed an algorithm very similar to sparse tiling, to parallelize Gauss-Seidel computations. The main difference between~\cite{StroutLCPC2002,StroutIJHPCA} and~\cite{dimeEtna00} was that in the former work the tiles fully covered the iteration space, so a sequential clean-up phase at the end could be avoided. 

We reiterate the fact that all these approaches were either specific to individual benchmarks or general but not scheduling across loops (i.e., loop fusion). Filling this gap is one of the contributions of this chapter.

\subsection*{Automated Code Generation and Optimization for Mesh-Based Computations}
% Automated code generation and unstructured grids
The automated code generation technique presented in \cite{Ravishankar12} examines the data affinity among loops and partitions the execution with the goal of minimizing communication between processes, while maintaining load balancing. This technique supports unstructured mesh applications (being based on an inspector/executor strategy) and targets distributed memory systems, although it does not exploit the loop chain abstraction and abstracts from loop optimization.

% Automated code generation and structured grids 
Automated code generation techniques, such as those based on polyhedral compilers (reviewed in Section~\ref{sec:bkg:poly}), have been applied to structured mesh benchmarks or proxy applications. Notable examples are~\cite{pluto,polly,loopy}. There has been very little effort in providing evidence that these tools can be effective in real-world applications. Time-loop diamond tiling was applied in~\ref{cohen-timetiling} to a proxy code, but experimentation is limited to a single-node.
%TODO other examples?

\subsection*{Overlapped Tiling}
% Overlapped tiling
In structured codes, Using multiple layers of halo, or ``ghost'', elements is a common optimization in structured codes~\cite{Bassetti98}. Overlapped tilling (see Section~\ref{sec:bkg:tiling}) exploits the same principle to reduce communication at the expense of performing redundant computation along the boundary~\cite{Zhou12}. Several studies centered on overlapped tiling within single regular loop nests (mostly stencil-based computations)~\cite{Meng09,Krishnamoorthy07,Chen02}. Techniques known as ``communication avoiding''~\cite{Demmel08,commAvoidingSparse2009} also fall in this broader category. To the best of our knowledge, overlapped tiling for unstructured grids by automated code generation has been studied only analytically in~\cite{gihan-overlapped}.



\section{Generalized Inspector/Executor Schemes}
\label{sec:tiling:lc}
In Section~\ref{sec:tiling:motivation} we introduced that our approach to loop tiling is based upon an \textit{inspector/executor scheme automatically generated at execution time}. In this section, we explain how to construct such an inspector/executor scheme, whereas automation will be tackled in Section~\ref{sec:tiling:automation}.


\subsection{The Loop Chain Abstraction}
The mechanism we build is based upon the \textit{loop chain}, an abstraction originally introduced in~\cite{KriegerHIPS2013}. Informally, a loop chain is a sequence of loops with no global synchronization points, with attached some extra information to enable run-time data dependence analysis. 

We recall from Section~\ref{sec:tiling:limits} that the complexity -- particularly the presence of indirect memory accesses -- and the modularity of real-world unstructured mesh applications prevent most common static optimizations for data locality. The data dependence information carried by a loop chain is supposed to be used to replace compile-time with run-time optimization. The basic idea is that the user provides the loop chain; then the compiler modifies the source code adding two extra pieces of code, the inspector and the executor; finally, at run-time, the inspector will exploit the data dependence information to build a ``scheduling'', which will eventually be used by the executor. 

Before diving into the description of the loop chain abstraction, we observe two notable facts:
\begin{itemize}
\item The run-time execution of the inspection may introduce a significant overhead. In many scientific computations, however, the data dependence information is static; or, in more informal words, ``the mesh does not change over time''. Usually, the loop chain will be located within the time loop of the application, so the inspection cost is amortized. If the mesh changes over time (e.g., in case of adaptive mesh refinement), the inspection needs be re-executed. We have devoted a significant effort in optimizing the inspection algorithm,  with the hope that its cost tends to be negligible even in the extreme case of frequent changes in the data dependence pattern. Further details will be provided in the later sections. 
\item The loop chain is expected to be provided by the user. We reinforce the idea that, in our approach, we will have two possibilities for constructing loop chains: either explicitly, with users required to change their program by adding calls to an external library to build the inspector/executor scheme, or implicitly, with the loop chain built automatically ``behind the scenes'' through a higher level framework.
\end{itemize}

In~\cite{ST-KriegerHIPS2013,KriegerThesis}, a loop chain $L$ is defined as follows:
\begin{itemize}
\item $L$ defines a scope in which $n$ loops are present, $L_0, L_1, ..., L_{n-1}$. There are no global synchronization points between the various loops.
\item $D$ is a set of disjoint $m$ data spaces, $D_0, D_1, ..., D_{m-1}$. Each loop accesses (reads from, writes to) a certain subset of these data spaces. An access can be either direct (e.g., \texttt{A[i]}) or indirect (e.g., \texttt{A[map(i)]}).
\item $R_{L_l\rightarrow D_d}(\vec{i})$ and $W_{L_l\rightarrow D_d}(\vec{i})$, where the $R$ and $W$ access relations are defined over for each data space $D_d \in D$ and indicate which data locations in data space $D_d$ an iteration $i \in L_l$ reads from and writes to respectively (e.g., if we have \texttt{A[map(i)] = ...} in loop $L_j$, the access relation $map_{L_j\rightarrow A}(\vec{i})$ will be available). 
\end{itemize}

\subsection{Loop Chains for Unstructured Mesh Computations}
Because of our focus on unstructured mesh computations, and inspired by the programming model of OP2 (see Section~\ref{sec:bkg:meshiteration}), we refine the loop chain definition as follows:
\begin{itemize}
\item $L$ defines a scope in which $n$ loops are present, $L_0, L_1, ..., L_{n-1}$. There are no global synchronization points between the various loops.
\item $S$ is a set of disjoint $m$ sets, $S_0, S_1, ..., S_{m-1}$. Possible sets are the elements in the mesh or the degrees of freedom associated to a certain function. A loop iterates over one of these sets, or iteration spaces; a data space is associated with one of these sets. 
\item $M$ is a set of $k$ maps, $M_0, M_1, ..., M_{k-1}$. A map of arity $a$ is a vector-valued function $M : S_i \rightarrow S_j^0 \times S_j^1 \times ... \times S_j^{a-1}$. It essentially connects each element of $S_i$ to one or more elements in $S_j$. For example, if a triangular cell $c$ is connected to three vertices $v_0, v_1, v_2$, we trivially have $M(c) = [v_0, v_1, v_2]$. 
\item A loop $L_i$ over the iteration space $S_j$ is associated with a set $D_{L_i}$ of $d$ descriptors, $D_0, D_1, ..., D_{d-1}$. Each descriptor $D_j$ is a 2-tuple $D_j = {<}mode,\ M{>}$; $mode$ is one of $[READ,\ WRITE,\ INC]$, where $M$ is either a map from $S_j$ to some other sets or the special placeholder $\perp$, which indicates that the access is direct to some data associated with $S_j$.
\end{itemize}

The striking difference with respect to the original definition is the lack of data spaces. Essentially, we abstract data spaces to the level of sets. Each set $S_j$ is usually associated multiple data spaces, and loops often access different data spaces associated with $S_j$. The idea, which will become more clear in the next section, is to limit data dependence tracking at the sets level. This may conservatively return some ``false positives'', but it significantly improves the inspection cost since typically $|S| << |D|$. 

An example of a loop chain for an unstructured mesh application is provided in Figure~\ref{code:tiling-loopchain}.

\begin{algorithm}
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}

void kernel1 (double * x, double * tmp1, double * tmp2) $\lbrace$\\
  *tmp1 += *x;\\
  *tmp2 += *x;\\
$\rbrace$\\
~\\
// Build the loop chain \\
set (E, ...); set (C, ...); \\ 
map (mapE, ...); map (mapC, ...); \\  
loop (...); loop (...); loop (...); \\
~\\
// Original version of the program\\
\KwSty{for} t = 0 \KwSty{to} T $\lbrace$\\
~~// Loop over edges\\
~~\KwSty{for} e = 0 \KwSty{to} E $\lbrace$\\
~~~~x = X[e];\\
~~~~tmp$\_$0 = tmp + mapE[e + 0]; tmp$\_$1 = tmp + mapE[e + 1]; \\
~~~~kernel1 (x, tmp$\_0$, tmp$\_$1);\\
~~$\rbrace$\\

~~// Loop over cells\\
~~\KwSty{for} c = 0 \KwSty{to} C $\lbrace$\\
~~~~res = R[C];\\
~~~~tmp$\_$0 = tmp + mapC[c + 0]; tmp$\_$1 = tmp + mapC[c + 1]; tmp$\_$2 = tmp + mapC[c + 2];\\
~~~~kernel2 (res, tmp$\_0$, tmp$\_$1, tmp$\_$2);\\
~~$\rbrace$\\

~~// Loop over edges\\
~~\KwSty{for} e = 0 \KwSty{to} E $\lbrace$\\
~~~~tmp$\_$0 = tmp + mapE[e + 0]; tmp$\_$1 = tmp + mapE[e + 1]; \\
~~~~kernel3 (tmp$\_0$, tmp$\_$1);\\
~~$\rbrace$\\
$\rbrace$\\

\caption{Section of a toy program that is used as a running example to illustrate the loop chain abstraction and show how the tiling algorithm works.}
\label{code:tiling-loopchain}
\end{algorithm}


\section{Example}
...
% the most general case, i.e., for shared memory

\section{The Tiling Algorithm}
...

\subsection{Formalization}
...

\subsection{Correctness}
...


\section{Automation}
\label{sec:tiling:automation}
%- is automation possible ? -> DSLs !!! + lazy evaluation + inspector/executor + mpi-trick

\subsection{SLOPE: a Library for Tiling Irregular Computations}
...

\subsection{PyOP2: a Runtime Library for Mesh Iteration}
...

\subsection{Firedrake/DMPlex: the S-depth mechanism for MPI}
...


\section{Performance Evaluation}
...

\subsection{Benchmarks}
\begin{itemize}
\item Sparse Jacobi
\item Airfoil
\item Wave Explicit
\end{itemize}

\subsection{Seigen: an Elastic Wave Equation Solver for Seismological Problems}
\label{sec:tiling:seigen}
...
