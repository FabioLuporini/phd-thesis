\chapter{Automated Tiling for Irregular Computations}
\label{ch:sparsetiling}

\section{Motivation}
\label{sec:tiling:motivation}
%mention dichotomy tiling/fusion...

%- irregular codes from pde-land are often memory bound (this however depends on the discretization)
%- investigation of tiling in real-world irregular codes

%- fully parallel loops ! with local reductions ... (diff with global reductions)



Many numerical methods for partial differential equations (PDEs) are structured as sequences of parallel loops, often interleaved by sparse linear algebra operations. This exposes parallelism well, but often does not convert data reuse between loops into data locality, since very large datasets are usually accessed. In Section~\ref{sec:bkg:poly} we have explained that loop fusion and loop tiling may be used to retain a significant fraction of this potential data locality. However, as elaborated in Section~\ref{sec:tiling:limits}, the complexity inherent in real-world applications makes it challenging to adopt such optimizations in practice. 

Our focus is on unstructured mesh PDE solvers, such as those based on the finite volume or the finite element methods. Here, the  loop-to-loop dependence structure is data-dependent due to
indirect references such as \texttt{A[map[i]]}; the \texttt{map} array stores connectivity information, for example from elements in the mesh to degrees of freedom. A similar pattern occurs in molecular dynamics simulations and graph processing, so both the theory and the tools that we will develop in this chapter are generalizable to these domains. 

Because of the irregular memory access pattern, our approach to loop optimization must be based on dynamic analysis, particularly on \textit{inspector/executor schemes}. Our hypothesis, backed by the studies reviewed in Section~\ref{sec:bkg:ie}, is that dynamic loop optimization through inspector/executor schemes can improve the performance of the applications in which we are interested. Among the various dynamic loop optimizations, we target \textit{sparse tiling}. In Section~\ref{sec:bkg:sparsetiling} we explained that sparse tiling aims to exploit data reuse across consecutive loops. We recall that this optimization can be seen as the composition of three transformations: loop fusion, loop tiling, and automatic parallelization. 

To summarize, the three main issues that we attack in this chapter are as follows:

\begin{itemize}
\item Previous approaches to sparse tiling are all based upon ``ad-hoc'' inspector/executor strategies; that is, developed ``by hand'', per application. We seek for a generalized technique, applicable to arbitrary computations on unstructured meshes.
\item For this research to be successful, we believe we need a fully automated system capable of optimizing real-world applications. Automation is essential because computational scientists must abstract from low level optimization. We aim for a mixed compiler/library based approach, to be integrated with a framework for solving PDEs.
\item There are only very few studies tackling loop fusion and inter-node parallelization. We are aware of none in presence of irregular memory accesses. We describe a technique that can co-exist with distributed-memory parallelism. This is essential because most scientific simulations run on at least hundreds of nodes.
\end{itemize}



\section{Tackling Real-World Applications}
\label{sec:tiling:limits}

\subsection{Mesh-based Numerical Methods for PDEs}
Loop fusion and loop tiling are widely studied in the literature. In spite of a notable research effort, however, it is not clear how widespread these optimizations are in real-world applications. Most studies centre their experimentation on relatively simple benchmarks and single-node performance; this unfortunately does not expose the complexity and the limitations of most scientific codes. On the other hand, it has repeatedly been shown that applying tiling to ``simple'' memory-bound loop nests can result in considerable speed-ups. The most striking examples are stencil codes arising in the finite difference method~\cite{stencil-tiling}, BLAS routines such as matrix multiplication~\cite{MKL}, and image processing kernels~\cite{Halide}. Since numerical methods for partial differential equations (PDEs) are often structured as sequences of parallelizable ``sweeps'' over the discretized equation domain, often implemented as memory-intensive loop nests, the following questions arise naturally: 

\begin{description}
\item[Applicability] Can we adopt sparse tiling (i.e., loop fusion composed with loop tiling), whose application has traditionally been limited to simple loop nests, in real-life numerical methods for solving PDEs?
\item[Lack of evidence] Why, despite decades of research, is it so difficult to find successful examples of integration of loop tiling with production code? 
\item[Challenges] What are the theoretical and technical challenges that we have to overcome to automate this optimization, such that an entire community of computational scientists can benefit from it?
\end{description}

In this chapter, we will try answering these questions. Our study focuses on a particular class of applications:
\begin{description}
\item[Irregular codes] Unstructured meshes are often used to discretize the computational domain, since they allow an accurate representation of complex geometries. For this type of meshes, the connectivity is usually stored by means of adjacency lists (or any equivalent structure), which results in indirect memory accesses (e.g., \texttt{A[B[i]]}) within loop nests. Indirections break static analysis, thus making any compiler-based approach (e.g., polyhedral optimization) unsuitable for our context. Indirections also make data dependence analysis more difficult, since this must now take place at runtime, introducing additional overhead.
\item[Realistic dataset] Most complex simulations operate on at least gigabytes of data, requiring multi-node execution. Any tiling-based optimization we will consider must therefore cope well with MPI execution.
\item[Automation, but no legacy code] We view tiling as an ``extreme optimization''; that is, as an optimization that should be tried at the end of the development process, once the experimental results have been validated, to improve the execution time. In our experience, however, it is extremely rare that computational scientists have the expertise to add any low level optimizations, such as loop tiling, to their codes. In addition, applying these optimizations at the source level makes the code impenetrable and, therefore, almost impossible to maintain and to extend. Our aim is a fully automated technique abstracted to the users through a simple ``switch'' (i.e., loop tiling on/off) and a tiny set of parameters to drive performance tuning (e.g., the tile size). Automation clearly comes at the price of implementation complexity; we will integrate our tiling system with a generic framework for mesh iteration. We are therefore not interested in supporting legacy code, in which key computational aspects (e.g., mesh iteration, MPI communication) are usually hidden, for modularity, underneath several layers of software.
\end{description}

To support this class of applications and in order to maximize the chances that our work can be used in a variety of simulations in the years to come, our approach builds on two pillars:
\begin{itemize}
\item a library for writing inspector/executor schemes (\cite{IEscheme}) for tiling arbitrary computations on unstructured meshes (or graphs);
\item a multilayer framework based on DSLs and runtime code generation that uses such a library to automate the applciation of loop tiling.
\end{itemize}

\subsection{Hypotheses}
\label{sec:tiling:struct}

\begin{algorithm}
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}

// Time-stepping loop (T = total number of iterations)\\
\KwSty{for} t = 0 \KwSty{to} T $\lbrace$\\
~~// 1st sweep over the $C$ cells of the mesh\\
~~\KwSty{for} i = 0 \KwSty{to} $C$ $\lbrace$\\
~~~~buffer$\_$0 = gather$\_$data ( A[f(map[i])], ... )\\
~~~~...\\
~~~~kernel$\_$1( buffer$\_$0, buffer$\_$1, ... );\\
~~~~scatter$\_$data ( buffer$\_$0, f(map[i]) )\\
~~$\rbrace$\\
~~Calc ( ... );\\
~~MPI$\_$Comm ( ...); \\
~~// 2nd sweep over the $N$ nodes of the mesh\\
~~\KwSty{for} i = 0 \KwSty{to} $N$ $\lbrace$\\
~~~~... // Similar to sweep 1 \\
~~$\rbrace$\\
~~// Boundary conditions: sweep over the $BE$ boundary edges\\
~~\KwSty{for} i = 0 \KwSty{to} $BE$ $\lbrace$\\
~~~~... // Similar to sweep 1 \\
~~$\rbrace$\\
$\rbrace$
\caption{....}
\label{code:tiling-struct}
\end{algorithm}

The ``bare'' structure of a scientific code solving a PDE by applying a numerical method on a discretized domain is shown in Listing~\ref{code:tiling-struct}. In the next section, we will use this example to motivate two fundamental hypotheses:

\begin{description}
\item[Hypothesis 1] A wide range of loop nests in mesh-based codes is, from a computational viewpoint, memory-bound, and increasing the data reuse across consecutive loop nests may relieve this issue.
\item[Hypothesis 2] Automating loop tiling in this kind of codes presents challenges that none of the state-of-the-art technologies can overcome. Our approach aims to solve this problem.
\end{description}



% Is skewing used for optimizing cross-time dependency, i.e., is the wave due to time?

\subsection{Automating Loop Tiling is More Difficult than Commonly Thought}
We identify three classes of problems that are either neglected or treated disjointly in the relevant literature. 

\begin{description}
\item[Theoretical questions] We first of all wonder whether loop tiling really is the optimization we are looking for.
\begin{description}
\item[Computational Boundedness] Since most computational methods are structured as sequences of loop nests and each loop nest is characterized by its own operational intensity, the computational boundedness is a concept that does not refer to the whole application. Rather, some loop nests may be memory-bound, whereas others CPU-bound, due to the complexity of the equations being solved or the employment of high order function spaces. Clearly, if all loop nests in an application are CPU-bound, the benefits of tiling on data locality will tend to be marginal. Before applying an aggressive optimization such as loop tilling, it is therefore fundamental to study the computational behaviour of an application for: (i) determining what fraction of the execution time is due to memory-bound loop nests; (ii) understanding if CPU-boundedness can be eliminated by applying other optimizations (e.g., vectorization), or if it is instead a natural consequence of the kernels and the architecture.
\item[Tiling vs Space Filling Curves] It is our impression that different communities have disjointedly researched solutions to the problem of relieving the memory pressure of mesh-based computations for years. We view both tiling and space filling curves (SFCs) -- and likewise sequential execution -- as instances of possible scheduling functions for a given loop nest (see Section~\ref{sec:bkg:poly}). Nevertheless, we could not find studies comparing the performance of the two approaches, denoting a lack of communication between people in two distinct yet potentially very close domains: compiler optimization and computational science.

\end{description}

\item[Practical issues] Loop tiling for structured meshes is widely covered in the literature. Several studies have tackled automation (e.g., polyhedral compilers), time-loop tiling (i.e., time skewing), exotic tile shapes for minimizing communication (e.g., diamond tiling), and so on. However, the following challenges tend to be neglected, or at least only marginally tackled.
\begin{description}
\item[Unstructured meshes] A technique to tile arbitrary computations on unstructured meshes was missing until this thesis\footnote{We reinforce once more that the generalized tiling algorithm is actually the result of a joint collaboration between .... See~\cite{st-paper}.}. Some inspector-executor strategies for tiling specific benchmarks, as discussed in Section~\ref{sec:tiling:relatedwork}, were available though. 
\item[Time tiling and MPI] We reiterate the fact that real computations almost always run on distributed memory machines. The multi-node parallelization is often carried out resorting to MPI. This is evident in Listing~\ref{code:tiling-struct} where MPI calls are placed between two consecutive mesh sweeps, since the first cells loop produces data needed by the subsequent nodes loop. Distributed memory parallelism poses a big challenge for time tiling, because now tiles at the partition boundary need special handling.
\item[Time tiling and extra code] The \texttt{Comp(...)} function in Listing~\ref{code:tiling-struct} denotes the possibility that additional computation is performed between consecutive sweeps. This could be, for instance, check-pointing or I/O. Also, conditional execution of loops (e.g., through \texttt{if-then-else}) breaks time tiling. 
\item[Legacy code is usually impenetrable] Our experience is that real-life code for scientific simulation tends to hide the tiling opportunities that a polyhedral compiler, for example, would search for. As explained in~\cite{strout-common-problems}, common problems are: 1) finding tilable loop nests, which are usually hidden for code modularity; 2) handling of boundary conditions; 3) dist
\end{description}

\item[Limitations inherent in the numerical method] Two loops cannot be fused if they are separated by a global synchronization point. This is often a global reduction, either explicit (e.g., the first loop updates a global variable that will be needed by the second loop) or implicit (i.e., within an external function invoked in between the two loops, like in many iterative solvers for linear systems). By limiting the applicability of many loop optimizations, global synchronization points pose great challenges and research questions. If strong scaling is the primary goal and memory-boundedness is the key limiting factor, then fundamental questions are: (i) can the numerical method be reformulated to relieve the constraints on low level optimization (which requires a joint effort between people in different domains); (ii) can the tools be made more sophisticated to work around these problems; (iii) most importantly, will the effort be rewarded by significant performance improvements?
\end{description}

In this chapter, we will formulate answers to some of these problems.



%\subsection{Time-Tilable Codes and Possible Solutions}
%I've been trying to solve these issues. Especially points 4, 5 and 2 (thanks Michael for simplifying my life!)
%
%
%What's the solution we propose?
%DSL and automated code generation schemes. Goal is to generate code which is a *suitable input* for other tools capable of saving us "automagically" from the burden of optimising everything by ourself
%
%
%So, back to the original question: "Is non-trivial loop tiling really widely applied in real-world software ?" And I said "no" because of the aforementioned points. 
%
%Where are with REAL tiling? Who is actually using it in real codes? What sort of improvements did they get? How did they work around the MPI "problem"?
%
%And, finally:
%
%What do we want to say about that body of literature presenting increasingly sophisticated tiling schemes that *seem* to be rarely used in practice? What do we want to say and ask to people talking about tiling?
%
%I'd like to hear what you think about this.

\section{Related Work}
\label{sec:tiling:relatedwork}

\subsection*{Loop Chain}
% Loop chain
The data dependence analysis that we will develop is based on an abstraction called \textit{loop chain}, which was originally presented in~\cite{KriegerHIPS2013}. This abstraction is sufficiently general to capture data dependency in programs structured as arbitrary sequences of loops. We will detail our loop chain abstraction in Section~\ref{sec:tiling:lc}.

\subsection*{Inspector/Executor and Sparse Tiling}
% Inspector/Executor
The loop chain is the mechanism by means of which we will generalize inspector/executor schemes for unstructured codes. Inspector/executor strategies were first formalized by~\cite{Saltz91} and used to fuse and tile loops for improving data locality and providing parallelism in~\cite{dimeEtna00,StroutLCPC2002,Demmel08,KriegerIAAA2012}. 

Sparse tiling, which we introduced in Section~\ref{sec:bkg:ie}, is the most notable technique based upon inspection/execution. The term was coined by Strout et al.~\cite{StroutLCPC2002,StroutIJHPCA} in the context of the Gauss-Seidel algorithm and in~\cite{StroutPLDI03} in the context of the moldyn benchmark. However, the technique was initially proposed by Douglas et al.~\cite{dimeEtna00} to parallelize computations over unstructured meshes, taking the name of \textit{unstructured cache blocking}. The mesh was initially partitioned; the partitioning represented the tiling in the first sweep over the mesh. Tiles would then shrink by one layer of vertices for each iteration of the loop. This shrinking represented what parts of the mesh could be update in later iterations of the outer loop without communicating with the processes executing other tiles. The unstructured cache blocking technique also needed to execute a serial clean-up tile at the end of the computation. Mark Adams~\cite{Adams99c} also developed an algorithm very similar to sparse tiling, to parallelize Gauss-Seidel computations. The main difference between~\cite{StroutLCPC2002,StroutIJHPCA} and~\cite{dimeEtna00} was that in the former work the tiles fully covered the iteration space, so a sequential clean-up phase at the end could be avoided. 

We reiterate the fact that all these approaches were either specific to individual benchmarks or general but not scheduling across loops (i.e., loop fusion). Filling this gap is one of the contributions of this chapter.

\subsection*{Automated Code Generation and Optimization for Mesh-Based Computations}
% Automated code generation and unstructured grids
The automated code generation technique presented in \cite{Ravishankar12} examines the data affinity among loops and partitions the execution with the goal of minimizing communication between processes, while maintaining load balancing. This technique supports unstructured mesh applications (being based on an inspector/executor strategy) and targets distributed memory systems, although it does not exploit the loop chain abstraction and abstracts from loop optimization.

% Automated code generation and structured grids 
Automated code generation techniques, such as those based on polyhedral compilers (reviewed in Section~\ref{sec:bkg:poly}), have been applied to structured mesh benchmarks or proxy applications. Notable examples are~\cite{pluto,polly,loopy}. There has been very little effort in providing evidence that these tools can be effective in real-world applications. Time-loop diamond tiling was applied in~\ref{cohen-timetiling} to a proxy code, but experimentation is limited to a single-node.
%TODO other examples?

\subsection*{Overlapped Tiling}
% Overlapped tiling
In structured codes, Using multiple layers of halo, or ``ghost'', elements is a common optimization in structured codes~\cite{Bassetti98}. Overlapped tilling (see Section~\ref{sec:bkg:tiling}) exploits the same principle to reduce communication at the expense of performing redundant computation along the boundary~\cite{Zhou12}. Several studies centered on overlapped tiling within single regular loop nests (mostly stencil-based computations)~\cite{Meng09,Krishnamoorthy07,Chen02}. Techniques known as ``communication avoiding''~\cite{Demmel08,commAvoidingSparse2009} also fall in this broader category. To the best of our knowledge, overlapped tiling for unstructured grids by automated code generation has been studied only analytically in~\cite{gihan-overlapped}.



\section{Generalized Inspector/Executor Schemes}
\label{sec:tiling:lc}
In Section~\ref{sec:tiling:motivation} we introduced that our approach to sparse tiling is based upon an \textit{inspector/executor scheme automatically generated at execution time}. In this section, we explain what is required to to construct such an inspector/executor scheme. The automation of this process will be tackled in Section~\ref{sec:tiling:automation}.


\subsection{The Loop Chain Abstraction}
The mechanism we build is based upon the \textit{loop chain}, an abstraction originally introduced in~\cite{KriegerHIPS2013}. Informally, a loop chain is a sequence of loops with no global synchronization points, with attached some extra information to enable run-time data dependence analysis. 

We recall from Section~\ref{sec:tiling:limits} that the complexity -- particularly the presence of indirect memory accesses -- and the modularity of real-world unstructured mesh applications prevent most common static optimizations for data locality. The data dependence information carried by a loop chain is supposed to be used to replace compile-time with run-time optimization. The basic idea is that the user provides the loop chain; then the compiler modifies the source code adding two extra pieces of code, the inspector and the executor; finally, at run-time, the inspector will exploit the data dependence information to build a ``scheduling'', which will eventually be used by the executor. 

Before diving into the description of the loop chain abstraction, we observe two notable facts:
\begin{itemize}
\item The run-time execution of the inspection may introduce a significant overhead. In many scientific computations, however, the data dependence information is static; or, in more informal words, ``the mesh does not change over time''. Usually, the loop chain will be located within the time loop of the application, so the inspection cost is amortized. If the mesh changes over time (e.g., in case of adaptive mesh refinement), the inspection needs be re-executed. We have devoted a significant effort in optimizing the inspection algorithm,  with the hope that its cost tends to be negligible even in the extreme case of frequent changes in the data dependence pattern. Further details will be provided in the later sections. 
\item The loop chain is expected to be provided by the user. We reinforce the idea that, in our approach, we will have two possibilities for constructing loop chains: either explicitly, with users required to change their program by adding calls to an external library to build the inspector/executor scheme, or implicitly, with the loop chain built automatically ``behind the scenes'' through a higher level framework.
\end{itemize}

In~\cite{ST-KriegerHIPS2013,KriegerThesis}, a loop chain $L$ is defined as follows:
\begin{itemize}
\item $L$ defines a scope in which $n$ loops are present, $L_0, L_1, ..., L_{n-1}$. There are no global synchronization points between the various loops.
\item $D$ is a set of disjoint $m$ data spaces, $D_0, D_1, ..., D_{m-1}$. Each loop accesses (reads from, writes to) a certain subset of these data spaces. An access can be either direct (e.g., \texttt{A[i]}) or indirect (e.g., \texttt{A[map(i)]}).
\item $R_{L_l\rightarrow D_d}(\vec{i})$ and $W_{L_l\rightarrow D_d}(\vec{i})$, where the $R$ and $W$ access relations are defined over for each data space $D_d \in D$ and indicate which data locations in data space $D_d$ an iteration $i \in L_l$ reads from and writes to respectively (e.g., if we have \texttt{A[map(i)] = ...} in loop $L_j$, the access relation $map_{L_j\rightarrow A}(\vec{i})$ will be available). 
\end{itemize}

\subsection{Loop Chains for Unstructured Mesh Computations}
Because of our focus on unstructured mesh computations, and inspired by the programming and execution models of OP2 (see Section~\ref{sec:bkg:op2}), we refine the loop chain definition as follows:
\begin{itemize}
\item $L$ defines a scope in which $n$ loops are present, $L_0, L_1, ..., L_{n-1}$. There are no global synchronization points between the various loops.
\item The execution order of a loop iterations does not influence the result. This means that given $L_i$, we can choose an arbitrary scheduling of iterations because there will not be loop-carried dependencies. Clearly, in case of parallel execution, some mechanisms to avoid race conditions must be present, because of potential local reductions (see also Section~\ref{sec:bkg:terminology}). 
\item $S$ is a set of disjoint $m$ sets, $S_0, S_1, ..., S_{m-1}$. Possible sets are the elements in the mesh or the degrees of freedom associated to a certain function. A loop iterates over one of these sets, or iteration spaces; a data space is associated with one of these sets. 
\item $M$ is a set of $k$ maps, $M_0, M_1, ..., M_{k-1}$. A map of arity $a$ is a vector-valued function $M : S_i \rightarrow S_j^0 \times S_j^1 \times ... \times S_j^{a-1}$. It connects each element of $S_i$ to one or more elements in $S_j$. For example, if a triangular cell $c$ is connected to three vertices $v_0, v_1, v_2$, we have $M(c) = [v_0, v_1, v_2]$. 
\item A loop $L_i$ over the iteration space $S$ is associated with $d$ descriptors, $D_0, D_1, ..., D_{d-1}$. Each descriptor $D_j$ is a 2-tuple $D_j = {<}M,\ mode{>}$; $M$ is either a map from $S_j$ to some other sets or the special placeholder $\perp$, which indicates that the access is direct to some data associated with $S_j$, whereas $mode$ is one of $[READ,\ WRITE,\ INC]$.
\end{itemize}

The striking difference with respect to the original definition is the lack of data spaces. Essentially, we abstract data spaces to the level of sets. Each set $S_j$ is usually associated multiple data spaces, and loops often access different data spaces associated with $S_j$. The idea, which will become more clear in the next section, is to limit data dependence tracking at the sets level. This may conservatively return some ``false positives'', but it significantly improves the inspection cost since typically $|S| << |D|$. 



\begin{algorithm}
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}

\KwSty{for} t = 0 \KwSty{to} T $\lbrace$\\
~~// Loop over edges\\
~~\KwSty{for} e = 0 \KwSty{to} E $\lbrace$\\
~~~~x = X[e];\\
~~~~tmp$\_$0 = tmp + edges2vertices[e + 0];\\
~~~~tmp$\_$1 = tmp + edges2vertices[e + 1]; \\
~~~~kernel1 (x, tmp$\_0$, tmp$\_$1);\\
~~$\rbrace$\\
~\\
~~// Loop over cells\\
~~\KwSty{for} c = 0 \KwSty{to} C $\lbrace$\\
~~~~res = R[C];\\
~~~~tmp$\_$0 = tmp + cells2vertices[c + 0];\\
~~~~tmp$\_$1 = tmp + cells2vertices[c + 1];\\
~~~~tmp$\_$2 = tmp + cells2vertices[c + 2];\\
~~~~kernel2 (res, tmp$\_0$, tmp$\_$1, tmp$\_$2);\\
~~$\rbrace$\\
~\\
~~// Loop over edges\\
~~\KwSty{for} e = 0 \KwSty{to} E $\lbrace$\\
~~~~tmp$\_$0 = tmp + edges2vertices[e + 0];\\
~~~~tmp$\_$1 = tmp + edges2vertices[e + 1]; \\
~~~~kernel3 (tmp$\_0$, tmp$\_$1);\\
~~$\rbrace$\\
$\rbrace$\\

\caption{Section of a toy program that is used as a running example to illustrate the loop chain abstraction and show how the tiling algorithm works.}
\label{code:tiling-loopchain}
\end{algorithm}


\begin{algorithm}
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}
inspector = init$\_$inspector (...);\\
~\\
// Three sets, edges, cells, and vertices\\
E = set (inspector, nedges, ...);\\
C = set (inspector, ncells, ...);\\
V = set (inspector, nvertices, ...);\\ 
~\\
// Two maps: from edges to vertices and from cells to vertices\\
e2vMap = map (inspector, E, V, edges2vertices, ...);\\
c2vMap = map (inspector, C, V, cells2vertices, ...);\\
~\\
// The loop chain comprises three loops; each loop has a set of descriptors\\
loop (inspector, E, $\lbrace \perp$, READ$\rbrace$, $\lbrace$e2vMap, INC$\rbrace$);\\
loop (inspector, C, $\lbrace \perp$, READ$\rbrace$, $\lbrace$c2vMap, INC$\rbrace$);\\
loop (inspector, E, $\lbrace$e2vMap, INC$\rbrace$); \\
~\\
// Now can run the inspector\\
inspection = run$\_$inspection (inspector, tile$\_$size, ...)\\
~\\
\caption{Building the loop chain for inspection.}
\label{code:tiling-inspector}
\end{algorithm}


Listing~\ref{code:tiling-loopchain} shows a plain C implementation of the unstructured mesh OP2 program illustrated in Listing~\ref{code:op2program}. A loop chain for this program is constructed in Listing~\ref{code:tiling-inspector}. The inspection output is stored in the data structure \texttt{inspection}. This is used (Listing~\ref{code:tiling-executor}) to execute a ``tiled'' version of the original program. In the next section, we show through examples how the inspector exploits the information captured by the loop chain to construct tiles.


\section{Sparse Tiling Examples}
Using the example in Listing~\ref{code:tiling-loopchain}, we describe the actions performed by a sparse tiling inspector. We show two variants of inspection: for shared-memory and distributed-memory parallelism. 

\subsection{Overview of the Algorithm}
The inspector starts with partitioning the iteration space of a \textit{seed loop}, for example $L_0$. Partitions are used to initialize tiles: the edges in partition $P_i$ -- or, equivalently, the iterations of $L_0$ falling in $P_i$ -- are assigned to tile $T_i$. In the later stages of the inspection,  as detailed in the upcoming sections, $T_i$ will be populated with iterations from $L_1$ and $L_2$. The executor will assign a single thread/process to each $T_i$. $T_i$ is executed ``atomically''; that is, without the need for communication with other tiles. When executing $T_i$, first all iterations from $L_0$ are executed, then all iterations from $L_1$ and finally those from $L_2$. The challenge in scheduling iterations from $L_j$ to $T_i$, therefore, is to guarantee that all data dependencies -- read after write, write after read, write after write -- are preserved. 

\subsection{Shared-Memory Parallelism}
\label{sec:tiling:ex-sm}
Figure~\ref{fig:st-initial-part-sm} displays the situation after the initial partitioning of $L_0$. 

There are four partitions, two of which are not connected through any edge or cell. These four partitions correspond to four tiles, $[T_0,\ T_1,\ T_2,\ T_3]$. Similarly to OP2, for shared-memory parallelism we adopt the following approach: (i) we serialize the execution of the elements inside a partition by scheduling them to a single thread; (ii) we color the partitions such that partitions assigned the same color can be executed in parallel by different threads. Two partitions can have the same color if they are not connected, because this ensures the absence of race conditions through indirect memory accesses. In the example we can use three colors: red (R), green (G), and blue (B). The two partitions at the top right and bottom left are not connected, so they are assigned the same color (green). In the following, with the notation $T_i^c$ we mean that the $i$-th tile has color $c$.

In order to populate the tiles with iterations from $L_1$ and $L_2$, we first have to establish a total ordering for the execution of partitions with different colors. Here, we assume the following order: green (G), blue (B), and red (R). This means, for instance, that \textit{all iterations} assigned to $T_1^B$ must be executed \textit{before all iterations} assigned to $T_2^R$. By ``all iterations'' we mean the iterations from $L_0$, as established by the seed partitioning, as well as the iterations that will later be assigned from $L_1$ and $L_2$. We assign integer positive numbers to colors to reflect their ordering, where a smaller number means higher execution priority. In this case, we can assign 0 to green, 1 to blue, and 2 to red.

To schedule the iterations of $L_1$ to $[T_0^G,\ T_1^B,\ T_2^R, T_3^G]$, we first need to compute a \textit{projection} of any write or local reduction performed within $L_0$.  A projection for $L_0$ is a function $\rho : V \rightarrow \mathbb{T}$ mapping a vertex (indirectly) incremented during the execution of $L_0$ to a tile.  Consider the vertex $v_0$ in Figure~\ref{fig:st-proj-1}. $v_0$ has 7 incident edges, 2 of which belong to $T_0^G$, while the remaining 5 to $T_1^B$. Since we established that $G \prec B$, $v_0$ can only be read after $T_1^B$ has finished executing the iterations from $L_0$ (i.e., the 5 incident blue edges). We express this condition by setting $\rho(v_0) = T_1^B$. Observe that we can compute $\rho$ by iterating over $V$ and, for each vertex, applying the maximum function ($MAX$) to the color of the adjacent edges. 

We now use $\rho$ to schedule $L_1$, a loop over cells, to the tiles. Consider again $v_0$ and the adjacent cells $[c_0,\ c_1,\ c_2]$. These three cells have in common the fact that they are adjacent to both green and blue vertices. For $c_1$, and similarly with the other cells, we compute $MAX(\rho(v_0),\ \rho(v_1),\ \rho(v_2)) = MAX(B, G, G) = B = 1$. This establishes that $c_1$ must be assigned to $T_1^B$, because otherwise ($c_1$ assigned instead to $T_0^G$) a read to $v_0$ would occur before the last increment from $T_1^B$ took place. We indeed reiterate once more that the execution order is ``all iterations from $[L_0, L_1, L_2]$ in the green tiles before all iterations from $[L_0, L_1, L_2]$ in the blue tiles''.

In order to schedule $L_2$ to $[T_0^G,\ T_1^B,\ T_2^R,\ T_3^G]$ we employ a similar process. Vertices are again written by $L_1$, so a new projection over $V$ will be computed and will be used in place of $\rho$ to schedule the edges in $L_2$. Figure~\ref{fig:st-final} shows the result of this last phase. 

\subsubsection*{Conflicting Colors}
It is worth noting how $T_2^R$ ``consumed'' the frontier elements of all other tiles every time a new loop was scheduled. Tiling a loop chain consisting of $k$ loops had the effect of expanding the frontier of a tile of at most $k$ vertices. With this in mind, we re-inspect the loop chain of the running example, although this time with a different execution order: blue (B), red (R), and green (G). We recall that the total ordering of colors is arbitrary, so this situation is legal. By applying the same inspection procedure that we just described, $T_0^G$ and $T_3^G$ will eventually become connected (see Figure~\ref{fig:st-conflicts}), thus violating the precondition ``tiles/partitions with the same color can run in parallel''. Indeed, race conditions during the execution of iterations belonging to $L_2$ are now theoretically possible. We will explain how to handle this problem in Section~\ref{sec:tiling:algo}.

\subsection{Distributed-Memory Parallelism}
If parallelism is achieved through message-passing, the mesh and all datasets defined over it are distributed to different processes. Similarly to the example in Listing~\ref{code:tiling-struct}, MPI calls now separate the execution of two consecutive loops in the chain. This means that our inspector/executor scheme will have to take extra care of data dependencies along the mesh partition boundaries.

We logically split sets into three regions, \textit{core}, \textit{boundary}, and \textit{non-exec}. The boundary region includes all iterations that cannot be executed until the MPI communications have successfully terminated. We pick $L_0$ as seed loop and we initialize the tiles as follows:
\begin{enumerate}
\item We take the core region of $L_0$ and partition it into tiles. Unless we aim for a mixed distributed/shared-memory parallelization scheme, there is no need to reassign the same color to unconnected tiles since we now have a single process executing sequentially all tiles (as opposed to the case of shared memory parallelism discussed in Section~\ref{sec:tiling:ex-sm}). We can assign colors increasingly: $T_i$ is given color $i$, so we can uniquely identify a tile with its color. As long as tiles with contiguous ID are also physically contiguous in the mesh, and assuming that colors are executed in ascending order (i.e., $T_0,\ T_1, ...$, as in the shared memory case), this assignment retains spatial locality when ``jumping'' from executing $T_i$ to $T_{i+1}$.
\item We replicate over the boundary region of $L_0$ what we just did for core. Note that by neatly separating core and boundary, there cannot be tiles crossing the two regions. Further, all tiles within boundary have greater color than those in core, posing a constraint on the execution order: no boundary tiles can be executed until all core tiles have been scheduled.
\item We map the whole non-exec region of $L_0$ to a single special tile, $T_{ne}$. This tile has highest color and will actually never be executed. Its role will be explained next.
\end{enumerate}

In Figure~\ref{fig:st-mpi-init}, the same mesh of Figure~\ref{fig:st-initial-part-sm} is distributed to two processes and the output of the initialization phase is displayed. The inspection then proceeds as in the case of shared memory parallelism. The application of the $MAX$ function when scheduling iterations from $L_1$ and $L_2$ to tiles makes higher color tiles (i.e., those that will be scheduled later at execution time) ``grow over'' lower color ones. In particular, the whole boundary region ``expands'' over core, and so does $T_{ne}$ over boundary (see Figure~\ref{fig:st-mpi-growth}). 

The executor starts with triggering the all MPI communications required for the execution of the loop chain; it proceeds to scheduling all core tiles, thus overlapping communication with computation; it finishes, once all core tiles have been executed and the MPI communications have been accomplished, with scheduling the boundary tiles.

We make two key observations concerning the correctness and the efficiency of the strategy:
\begin{description}
\item[Correctness] This inspector/executor scheme relies on redundant computation along the mesh partition boundary. The depth of the boundary region grows proportionally with the length of the loop chain. Roughly speaking, if the loop chain consists of $n$ loops, then the boundary region needs $n-1$ extra layers of elements. In Figure~\ref{fig:st-mpi-init}, the elements $[...]$ belong to $R_1$, although they also need be executed by $R_0$ in order for $[...]$ to be correctly computed when $T_x$ executes iterations from $L_2$. This is a conceptually simple expedient, which however poses a big challenge on software engineering. 
\item[Efficiency] The underlying hypothesis is that the increase in data locality achieved through sparse tiling will outweigh the overhead introduced by the redundant computation. This is based on the consideration that, in real applications, the core region tends to be way larger than the boundary one. In addition, not all iterations along the boundary region need be redundantly executed at every loop. For example, if we consider Figure~\ref{fig:st-mpi-nonexec}, we see that the strip of elements $[...]$ is not relevant any more for the correct computation of $[...]$. The non-exec tile $T_{ne}$, which we recall is not scheduled by the executor, is deliberately given the highest color to grow over the boundary region: this will leave dirty values in some datasets until the next round of MPI communications, but will not hurt correctness. 
\end{description}



%The main difference with respect to frameworks such as OP2 (see Section~\ref{sec:bkg:op2}) is that here, for reasons that will be more evident next, two regions, \textit{owned} and \textit{exec}, have been merged into a single one, \textit{boundary}. In essence, 


\section{Formalization}
\label{sec:tiling:algo}

\subsection{Data Dependency Analysis for Loop Chains}

As with all loop optimizations that reschedule the iterations in a sequence of loops, any sparse tiling must satisfy the data dependencies. The loop chain abstraction, which we have described in Section~\ref{sec:tiling:lc}, provides enough information to construct an inspector which analyzes all of the dependencies in a computation and builds a legal sparse tiling. We recall that one of the main assumptions in a loop chain is that each loop is fully parallel, equivalently, that there are no loop carried dependencies.

The descriptors in the loop chain abstraction enable a general derivation of the storage-related dependencies between loops in a loop chain. The storage related dependencies between loops can be described as either flow (read after write), anti (write after read), or output (write after write) dependencies. In the following, assume that loop $L_x$, with iteration space $S_x$, always comes before loop $L_y$, with iteration space $S_y$, in the loop chain. With an abuse of notation, let us identify a descriptor of a loop $L$ with $m_{S_i \rightarrow S_j}^{mode}$: this simply indicates that the loop $L_i$ over the iteration space $S_i$ uses a map $m$ to write/read/increment elements (respectively, $mode \in \lbrace w, r, i\rbrace$) in the space $S_j$.

The flow dependencies can then be enumerated by considering pairs of points ($\vec{i}$ and $\vec{j}$) in the iteration spaces of the two loops $L_x$ and $L_y$:
\[
	\{ \vec{i} \rightarrow \vec{j} \; | \; \vec{i} \in S_x \wedge \vec{j} \in S_y \wedge 
	m_{S_x\rightarrow S_z}^{w}(\vec{i}) \cap m_{S_y \rightarrow S_z}^{r}(\vec{j}) \ne \emptyset \}.
\]
Anti and output dependencies are defined in a similar way. The anti dependencies for all pairs of loops $L_x$ and $L_y$ are:
\[
	\{ \vec{i} \rightarrow \vec{j} \; | \; \vec{i} \in S_x \wedge \vec{j} \in S_y \wedge 
	m_{S_x\rightarrow S_z}^{r}(\vec{i}) \cap m_{S_y \rightarrow S_z}^{w}(\vec{j}) \ne \emptyset \}.
\]
While the output dependencies between loops $L_x$ and $L_y$ are:
\[
	\{ \vec{i} \rightarrow \vec{j} \; | \; \vec{i} \in S_x \wedge \vec{j} \in S_y \wedge 
	m_{S_x\rightarrow S_z}^{w}(\vec{i}) \cap m_{S_y \rightarrow S_z}^{w}(\vec{j}) \ne \emptyset \}.
\]
In essence, there is a storage-related data dependence between two iterations from different loops (and therefore between the tiles they are placed in) when one of those iterations writes to a data element and the other iteration reads from or writes to the same data element.

There are local reductions, or ``reduction dependencies'' between two or more iterations of the same loop when those iterations ``increment'' the same location(s); that is, when they read, modify with a commutative and associative operator, and write to the same location(s). The reduction dependencies in loop $L_x$ are:
\[
	\{ \vec{i} \rightarrow \vec{j} \; | \; \vec{i} \in S_x \wedge \vec{j} \in S_x \wedge m_{S_x\rightarrow S_z}^{i}(\vec{i}) \cap m_{S_x \rightarrow S_z}^{i}(\vec{j}) \ne \emptyset \}.
\]
The reduction dependencies between two iterations within the same loop indicates that those two iterations must be executed atomically with respect to each other.

Dependencies between non-adjacent loops are also possible. Our algorithm takes care of them by inspecting \textit{projections}, which we have already illustrated in the examples of the previous section. We will see that when distributing the iterations of $L_i$ to tiles, the most up-to-date projections for each space $S$ accessed by $L_i$ must be available. $S$ may have been lastly written in $L_{i-1}$ or even in $L_0$; in any case, each time $S$ is modified a new projection is computed, and this is inspected when tiling $L_i$.


\subsection{The Generalized Sparse Tiling Algorithms}
Algorithm~\ref{algo:st-inspector} shows a procedure to derive an inspector for a loop chain. The inspector produces a schedule which supports mixed distributed/shared memory parallelism.

A major constraint of the inspector is that only the first loop of the chain, $L_0$, can be chosen as seed loop, or $L_{seed}$. Any loop in the chain could theoretically be picked as seed, but this would make supporting distributed memory parallelism cumbersome because of the coordination required along the boundary. If only shared memory parallelism were requested, the seed loop could be chosen arbitrarily: in~\cite{st-paper}, we propose two ``symmetric'' algorithms to populate tiles, \textit{forward tiling} and \textit{backward tiling}, where the latter uses the $MIN$ function in place of $MAX$. The fact that $L_{seed} = L_0$ means that tiles only grow ``in one direction'', with the scheduling of $L_i$ being based upon that of $[L_0,\ L_1,\ ...,\ L_{i-1}]$. We therefore limit ourselves to show the \textit{forward} variant of the sparse tiling inspector. 

\begin{Algo}[Sparse Tiling Inspector]
\label{algo:st-inspector}
\normalfont 
~
\begin{enumerate}
\item 
\end{enumerate}
\end{Algo}

\begin{Algo}[Sparse Tiling Executor]
\label{algo:st-executor}
\normalfont 
~
\begin{enumerate}
\item 
\end{enumerate}
\end{Algo}


\subsection{Correctness of Inspection}
...

\subsection{Computational Complexity of Inspection}
...

\section{Automation}
\label{sec:tiling:automation}
%- is automation possible ? -> DSLs !!! + lazy evaluation + inspector/executor + mpi-trick
% subsets ! seed loop cannot be subset

\subsection{SLOPE: a Library for Tiling Irregular Computations}
...

\subsection{PyOP2: a Runtime Library for Mesh Iteration}
...

\subsection{Firedrake/DMPlex: the S-depth mechanism for MPI}
...


\section{Performance Evaluation}
...

\subsection{Benchmarks}
\begin{itemize}
\item Sparse Jacobi
\item Airfoil
\item Wave Explicit
\end{itemize}

\subsection{Seigen: an Elastic Wave Equation Solver for Seismological Problems}
\label{sec:tiling:seigen}
...
