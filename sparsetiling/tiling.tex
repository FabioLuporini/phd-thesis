\chapter{Automated Tiling for Irregular Computations}
\label{ch:sparsetiling}

\section{Motivation}
%mention dichotomy tiling/fusion...

%- irregular codes from pde-land are often memory bound (this however depends on the discretization)
%- investigation of tiling in real-world irregular codes

%- fully parallel loops ! with local reductions ... (diff with global reductions)



Many numerical methods for partial differential equations (PDEs) are structured as sequences of parallel loops, often interleaved by sparse linear algebra operations. This exposes parallelism well, but often does not convert data reuse between loops into data locality, since very large datasets are usually accessed. In Section~\ref{sec:bkg:poly} we have explained that loop fusion and loop tiling may be used to retain a significant fraction of this potential data locality. However, as elaborated in Section~\ref{sec:tiling:limits}, the complexity inherent in real-world applications make it challenging to adopt such optimizations in practice. 

Our focus in on unstructured mesh PDE solvers, such as those based on the finite volume or the finite element methods. Here, the  loop-to-loop dependence structure is data-dependent due to
indirect references such as \texttt{A[map[i]]}; the \texttt{map} array stores connectivity information, for example from elements in the mesh to degrees of freedom. A similar pattern occurs in molecular dynamics simulations and graph processing, so both the theory and the tools that we will develop in this chapter are generalizable to these domains. 

% Compressed version of the History of sparse tiling subsection: we see potential
Because of the irregular memory access pattern, our approach to loop optimization will be based on dynamic analysis, particularly on \textit{inspector/executor schemes automatically generated at execution time}. Our hypothesis, backed by the studies reviewed in Section~\ref{sec:bkg:ie}, is that dynamic loop optimization has strong potentials for the applications in which we are interested. The three main issues that we attack in this chapter are as follows:

\begin{itemize}
\item Previous approaches to irregular loop tiling/fusion are all based upon ``ad-hoc'' inspector/executor strategies; that is, developed ``by hand'', per application. We seek for a generalized technique, applicable to arbitrary computations on unstructured meshes.
\item For this research to be successful, we believe we need a fully automated system capable of optimizing real-world applications. Automation is essential because computational scientists must abstract from low level optimization. We aim for a mixed compiler/library based approach, to be integrated with a framework for solving PDEs.
\item There are only very few studies tackling loop fusion and inter-node parallelization. We are aware of none in presence of irregular memory accesses. Our technique must support MPI parallelization, because scientific simulations almost always run on at least hundreds of nodes.
\end{itemize}




\section{Limitations in Real-World Applications}
\label{sec:tiling:limits}

\subsection{Mesh-based Numerical Methods for PDEs}
Loop tiling is widely studied in the literature. In spite of a notable research effort, however, it is not clear how widespread this optimization is in real-world applications. Most studies centre their experimentation on relatively simple benchmarks and single-node performance; this unfortunately does not expose the complexity and the limitations of most scientific codes. On the other hand, it has repetedly been shown that applying tiling to ``simple'' memory-bound loop nests can result in considerable speed-ups. The most striking examples are stencil codes arising in the finite difference method~\cite{stencil-tiling}, BLAS routines such as matrix multiplication~\cite{MKL}, and image processing kernels~\cite{Halide}. Since numerical methods for partial differential equations (PDEs) are often structured as sequences of parallelizable ``sweeps'' over the discretized equation domain, often implemented as memory-intensive loop nests, the following questions arise naturally: 

\begin{description}
\item[Applicability] Can we adopt loop tiling, whose application has traditionally been limited to simple loop nests, in real-life numerical methods for solving PDEs?
\item[Lack of evidence] Why, despite decades of research, is it so difficult to find successful examples of integration of loop tiling with production code? 
\item[Challenges] What are the theoretical and technical challenges that we have to overcome to automate this optimization, such that an entire community of computational scientists can benefit from it?
\end{description}

In this chapter, we will try answering these questions. Our study focuses on a particular class of applications:
\begin{description}
\item[Irregular codes] Unstructured meshes are often used to discretize the computational domain, since they allow an accurate representation of complex geometries. For this type of meshes, the connectivity is usually stored by means of adjanceny lists (or any equivalent structure), which results in indirect memory accesses (e.g., \texttt{A[B[i]]}) within loop nests. Indirections break static analysis, thus making any compiler-based approach (e.g., polyhedral optimization) unsuitable for our context. Indirections also make data dependence analysis more difficult, since this must now take place at runtime, introducing additional overhead.
\item[Realistic dataset] Most complex simulations operate on at least gigabytes of data, requiring multi-node execution. Any tiling-based optimization we will consider must therefore cope well with MPI execution.
\item[Automation, but no legacy code] We view tiling as an ``extreme optimization''; that is, as an optimization that should be tried at the end of the development process, once the experimental results have been validated, to improve the execution time. In our experience, however, it is extremely rare that computational scientists have the expertise to add any low level optimizations, such as loop tiling, to their codes. In addition, applying these optimizations at the source level makes the code impenetrable and, therefore, almost impossible to maintain and to extend. Our aim is a fully automated technique abstracted to the users through a simple ``switch'' (i.e., loop tiling on/off) and a tiny set of parameters to drive performance tuning (e.g., the tile size). Automation clearly comes at the price of implementation complexity; we will integrate our tiling system with a generic framework for mesh iteration. We are therefore not interested in supporting legacy code, in which key computational aspects (e.g., mesh iteration, MPI communication) are usually hidden, for modularity, underneath several layers of software.
\end{description}

To support this class of applications and in order to maximize the chances that our work can be used in a variety of simulations in the years to come, our approach builds on two pillars:
\begin{itemize}
\item a library for writing inspector/executor schemes (\cite{IEscheme}) for tiling arbitrary computations on unstructured meshes (or graphs);
\item a multilayer framework based on DSLs and runtime code generation that uses such a library to automate the applciation of loop tiling.
\end{itemize}

\subsection{Hypotheses}
\label{sec:tiling:struct}

\begin{algorithm}
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}

// Time-stepping loop (T = total number of iterations)\\
\KwSty{for} t = 0 \KwSty{to} T $\lbrace$\\
~~// 1st sweep over the $C$ cells of the mesh\\
~~\KwSty{for} i = 0 \KwSty{to} $C$ $\lbrace$\\
~~~~buffer$\_$0 = gather$\_$data ( A[f(map[i])], ... )\\
~~~~...\\
~~~~kernel$\_$1( buffer$\_$0, buffer$\_$1, ... );\\
~~~~scatter$\_$data ( buffer$\_$0, f(map[i]) )\\
~~$\rbrace$\\
~~Calc ( ... );\\
~~MPI$\_$Comm ( ...); \\
~~// 2nd sweep over the $N$ nodes of the mesh\\
~~\KwSty{for} i = 0 \KwSty{to} $N$ $\lbrace$\\
~~~~... // Similar to sweep 1 \\
~~$\rbrace$\\
~~// Boundary conditions: sweep over the $BE$ boundary edges\\
~~\KwSty{for} i = 0 \KwSty{to} $BE$ $\lbrace$\\
~~~~... // Similar to sweep 1 \\
~~$\rbrace$\\
$\rbrace$
\caption{....}
\label{code:tiling-struct}
\end{algorithm}

The ``bare'' structure of a scientific code solving a PDE by applying a numerical method on a discretized domain is shown in Listing~\ref{code:tiling-struct}. In the next section, we will use this example to motivate two fundamental hypotheses:

\begin{description}
\item[Hypothesis 1] A wide range of loop nests in mesh-based codes is, from a computational viewpoint, memory-bound, and increasing the data reuse across consecutive loop nests may relieve this issue.
\item[Hypothesis 2] Automating loop tiling in this kind of codes presents challenges that none of the state-of-the-art technologies can overcome. Our approach aims to solve this problem.
\end{description}



% Is skewing used for optimizing cross-time dependency, i.e., is the wave due to time?

\subsection{Automating Loop Tiling is More Difficult than Commonly Thought}
We identify three classes of problems that, we believe, are commonly neglected in the relevant literature. 

\begin{description}
\item[Theoretical questions] To the best of our knowledge, the following are research questions that have not found an answer yet. 
\begin{description}
\item[Memory-boundedness] vectorization ...
\item[Tiling vs Space Filling Curves]
\end{description}

\item[Practical issues] Loop tiling for structured meshes is widely covered in the literature. Several studies have tackled automation (e.g., polyhedral compilers), time-loop tiling (i.e., time skewing), exotic tile shapes for minimizing communication (e.g., diamond tiling), and so on. However, the following challenges tend to be neglected, or at least only marginally tackled.
\begin{description}
\item[Unstructured meshes] A technique to tile arbitrary computations on unstructured meshes was missing until this thesis\footnote{We reinforce once more that the generalized tiling algorithm is actually the result of a joint collaboration between .... See~\cite{st-paper}.}. Some inspector-executor strategies for tiling specific benchmarks, as discussed in Section~\ref{sec:tiling:relatedwork}, were available though. 
\item[Time tiling and MPI] We reiterate the fact that real computations almost always run on distributed memory machines. The multi-node parallelization is often carried out resorting to MPI. This is evident in Listing~\ref{code:tiling-struct} where MPI calls are placed between two consecutive mesh sweeps, since the first cells loop produces data needed by the subsequent nodes loop. Distributed memory parallelism poses a big challenge for time tiling, because now tiles at the partition boundary need special handling.
\item[Time tiling and extra code] The \texttt{Comp(...)} function in Listing~\ref{code:tiling-struct} denotes the possibility that additional computation is performed between consecutive sweeps. This could be, for instance, check-pointing or I/O. Also, conditional execution of loops (e.g., through \texttt{if-then-else}) breaks time tiling. 
\item[Legacy code is usually impenetrable] Our experience is that real-life code for scientific simulation tends to hide the tiling opportunities that a polyhedral compiler, for example, would search for. As explained in~\cite{strout-common-problems}, common problems are: 1) finding tilable loop nests, which are usually hidden for code modularity; 2) handling of boundary conditions; 3) dist
\end{description}

\item[Limitations inherent in the model] ...
\begin{description}
\item[Synchronization points] Speculation: communication between worlds, change numerical method for better efficiency

\end{description}

\end{description}


%
%So what are the issues here? There are several:
%1 - MAGIC BOX is a synchronisation point, like a global reduction or a call to a third party library (e.g. calling PETSC to solve a linear system). This prevents tiling at all (right?).
%2 - MAGIC BOX includes MPI halo exchanges. How tiles are going to behave along the boundary region if halo exchanges are required? That really is challenging ! (Do not hope that polyhedral tools can handle this)
%3 - legacy codes which have this scheme are, in practice, messy. Perhaps the loop structure is not explicit as here. Perhaps the loops are in different files. Perhaphs you don't even have those loops! It's all hidden behind several layers of abstraction (as in DOLFIN for those of you who know about it)
%4 - unstructured codes (finite volume, finite element) are more problematic than structured ones (finite difference). The indirections A[B[i]], storing the mesh connectivity, are available only at runtime. Plus, data dependencies are now irregular. How do we build tiles here? and in which phase of the program execution? 
%5 - code is not as memory bound as one might have thought. Hey, vectorisation is important! But not always easy to achieve. Without vectorisation, there are chances that some kernels are actually compute-bound. Also, what about "very" high order methods? plenty of flops per operator invocation...unless we do intra-kernel parallelisation (useful for reducing the working set size), we will probably end up being compute-bound. 
%6 - tiling is a complicated optimisation that requires a profound engineering effort, difficult to produce and mantain
%
%And the most important issue:
%- the combination of the above ones !
%
%I put all issues together, but probably some logical separation can be found
%




\subsection{Time-Tilable Codes and Possible Solutions}
%I've been trying to solve these issues. Especially points 4, 5 and 2 (thanks Michael for simplifying my life!)
%
%
%What's the solution we propose?
%DSL and automated code generation schemes. Goal is to generate code which is a *suitable input* for other tools capable of saving us "automagically" from the burden of optimising everything by ourself
%
%
%So, back to the original question: "Is non-trivial loop tiling really widely applied in real-world software ?" And I said "no" because of the aforementioned points. 
%
%Where are with REAL tiling? Who is actually using it in real codes? What sort of improvements did they get? How did they work around the MPI "problem"?
%
%And, finally:
%
%What do we want to say about that body of literature presenting increasingly sophisticated tiling schemes that *seem* to be rarely used in practice? What do we want to say and ask to people talking about tiling?
%
%I'd like to hear what you think about this.

\section{Related Work}
\label{sec:tiling:relatedwork}
The data dependence analysis that we will develop is based on an abstraction called \textit{loop chain}, which was presented in~\cite{KriegerHIPS2013}. This abstraction is sufficiently general to capture data dependency in programs structured as arbitrary sequences of loops. We will detail our loop chain abstraction in Section~\ref{sec:tiling:lc}.

The loop chain is the mechanism by means of which we generalize inspector/executor schemes for unstructured codes. Inspector/executor strategies were first formalized by~\cite{Saltz91} and used to fuse and tile loops for improving data locality and providing parallelism in~\cite{dimeEtna00,StroutLCPC2002,Demmel08,KriegerIAAA2012}. Communication avoiding approaches~\cite{commAvoidingSparse2009}, which optimize a series of loops over unstructured meshes, fall in the same category. We reiterate the fact that all these works were specific to individual benchmarks. 

The automated code generation technique presented in \cite{Ravishankar12} examines the data affinity among loops and partitions the execution with the goal of minimizing communication between processes, while maintaining load balancing. This technique supports unstructured mesh applications (being based on an inspector/executor strategy) and targets distributed memory systems, although it does not exploit the loop chain abstraction and abstracts from loop optimization.

Automated code generation technique, such as those based on polyhedral compilers (reviewed in Section~\ref{sec:bkg:poly}), have been applied to structured mesh benchmarks or proxy applications. Notable examples are~\cite{pluto,polly,loopy}. There has been very little effort in providing evidence that these tools can be effective in real-world applications. Time-loop diamond tiling was applied in~\ref{cohen-timetiling} to a proxy code, but experimentation is limited to a single-node.
%
%In structured codes, using multiple layers of halo, or ghost, cells is
%a common optimization~\cite{Bassetti98}, but
%when done by hand breaks the typical software modularity in
%large applications.
%Structured codes can also use
%overlapped tiling techniques that reduce communication
%at the expense of performing redundant
%computation~\cite{Zhou12}.
%These techniques work by dividing a loop
%nest's iteration space into a number of overlapping regions called
%tiles. These tiles are shaped such that each of them can execute in
%parallel without requiring communication.  
%
%Several researchers have examined using overlapped communication
%techniques within single loop nest (often stencil)
%computations~\cite{Meng09, Krishnamoorthy07, Chen02}.  
%We believe that
%an overlapped tiling optimizer 
%could use information from the loop chaining
%abstraction to do overlapped tiling across loops.
%




%%%%%%%%%%%%%%%%%%%%%%
%  \item Various code transformations reschedule computation and/or reorder data to turn
%  such data reuse into data locality as well as parallelizing the computations. [Summarize communication avoiding and sparse tiling work.  
%  Also OSU and Purdue work.  Summarize previous work that Carlo did.  Paper that describes the benefit Hydra could achieve.]
%Various code transformation have been developed to reschedule 
%computation and reorder data for loop-chain-like code patterns.
%Many of these techniques also generate parallel execution schedules for the loops. 
%The approach in \cite{OhioStateMPICodeGen} identifies \emph{partitionable loops}, 
%and schedules these loops for execution on a distributed memory machine. 
%Likewise, there are approaches that take parallel loops identified by OpenMP 
%pragmas and transform them for execution on distributed memory clusters~\cite{Basumallik2006}. 
%NEED TO SUMMARIZE HYDRA WORK HERE AS WELL.

%  \item Some of these approaches were general, but they did not schedule across the loops and
%  therefore are throwing some possible data locality away.
%  The techniques that schedule across loops are specific to individual loop kernels.
%The approach presented in this paper differs from these techniques in two key ways. 
%First, these approaches generate a schedule in which each partition or processing 
%element executes its assigned iterations of one loop, then communicates a subset 
%of its results to other partitions that are dependent on that data. 
%After executing its iterations of a loop, each processing element potentially 
%waits to receive data from other partitions. 
%The full sparse tiling approach described here does not require any 
%synchronization or communication during the execution of a tile due to the \emph{atomicity} 
%of the tile. Before a tile begins execution, it waits until all necessary data is available 
%and then executes from start to finish without further communication or synchronization. 
%This approach can better exploit the locality available across the sequence of loops.

% OK, what should we say here without harming the arguments about specializing the algorithm for OP2? Also, are we trying to contrast with Demmel's stuff or what? Does the communication avoiding work rely on the topology of the mesh? I seem to remember that it did and they had
% specific versions for 1D, 2D, 3D. Compared to the others, we are about the same
% in terms of loop chains vs partitionable loops vs OpenMP loops, etc-- Krieger

%Secondly, the approach presented in this paper is general and only requires that the provided sequence of loops be a valid loop chain. %It does not make any assumptions about the pattern of data dependencies or the number of loops.

\section{Abstraction}
\label{sec:tiling:lc}
...

\subsection{Programming Model}
...

\subsection{Execution Model}
...

\subsection{Loop Chains for Generality}

\section{Example}
...
% the most general case, i.e., for shared memory

\section{The Tiling Algorithm}
...

\subsection{Formalization}
...

\subsection{Correctness}
...


\section{Automation}
%- is automation possible ? -> DSLs !!! + lazy evaluation + inspector/executor + mpi-trick

\subsection{SLOPE: a Library for Tiling Irregular Computations}
...

\subsection{PyOP2: a Runtime Library for Mesh Iteration}
...

\subsection{Firedrake/DMPlex: the S-depth mechanism for MPI}
...


\section{Performance Evaluation}
...

\subsection{Benchmarks}
\begin{itemize}
\item Sparse Jacobi
\item Airfoil
\item Wave Explicit
\end{itemize}

\subsection{Seigen: an Elastic Wave Equation Solver for Seismological Problems}
\label{sec:tiling:seigen}
...
