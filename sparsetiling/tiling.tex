\chapter{Automated Tiling for Irregular Computations}
\label{ch:sparsetiling}

\section{Motivation}
\label{sec:tiling:motivation}
%mention dichotomy tiling/fusion...

%- irregular codes from pde-land are often memory bound (this however depends on the discretization)
%- investigation of tiling in real-world irregular codes

%- fully parallel loops ! with local reductions ... (diff with global reductions)



Many numerical methods for partial differential equations (PDEs) are structured as sequences of parallel loops, often interleaved by sparse linear algebra operations. This exposes parallelism well, but often does not convert data reuse between loops into data locality, since very large datasets are usually accessed. In Section~\ref{sec:bkg:poly} we have explained that loop fusion and loop tiling may be used to retain a significant fraction of this potential data locality. However, as elaborated in Section~\ref{sec:tiling:limits}, the complexity inherent in real-world applications makes it challenging to adopt such optimizations in practice. 

Our focus is on unstructured mesh PDE solvers, such as those based on the finite volume or the finite element methods. Here, the  loop-to-loop dependence structure is data-dependent due to
indirect references such as \texttt{A[map[i]]}; the \texttt{map} array stores connectivity information, for example from elements in the mesh to degrees of freedom. A similar pattern occurs in molecular dynamics simulations and graph processing, so both the theory and the tools that we will develop in this chapter are generalizable to these domains. 

Because of the irregular memory access pattern, our approach to loop optimization must be based on dynamic analysis, particularly on \textit{inspector/executor schemes}. Our hypothesis, backed by the studies reviewed in Section~\ref{sec:bkg:ie}, is that dynamic loop optimization through inspector/executor schemes can improve the performance of the applications in which we are interested. Among the various dynamic loop optimizations, we target \textit{sparse tiling}. In Section~\ref{sec:bkg:sparsetiling} we explained that sparse tiling aims to exploit data reuse across consecutive loops. We recall that this optimization can be seen as the composition of three transformations: loop fusion, loop tiling, and automatic parallelization. 

To summarize, the three main issues that we attack in this chapter are as follows:

\begin{itemize}
\item Previous approaches to sparse tiling are all based upon ``ad-hoc'' inspector/executor strategies; that is, developed ``by hand'', per application. We seek for a generalized technique, applicable to arbitrary computations on unstructured meshes.
\item For this research to be successful, we believe we need a fully automated system capable of optimizing real-world applications. Automation is essential because computational scientists must abstract from low level optimization. We aim for a mixed compiler/library based approach, to be integrated with a framework for solving PDEs.
\item There are only very few studies tackling loop fusion and inter-node parallelization. We are aware of none in presence of irregular memory accesses. We describe a technique that can co-exist with distributed-memory parallelism. This is essential because most scientific simulations run on at least hundreds of nodes.
\end{itemize}



\section{Tackling Real-World Applications}
\label{sec:tiling:limits}

\subsection{Mesh-based Numerical Methods for PDEs}
Loop fusion and loop tiling are widely studied in the literature. In spite of a notable research effort, however, it is not clear how widespread these optimizations are in real-world applications. Most studies centre their experimentation on relatively simple benchmarks and single-node performance; this unfortunately does not expose the complexity and the limitations of most scientific codes. On the other hand, it has repeatedly been shown that applying tiling to ``simple'' memory-bound loop nests can result in considerable speed-ups. The most striking examples are stencil codes arising in the finite difference method~\cite{stencil-tiling}, BLAS routines such as matrix multiplication~\cite{MKL}, and image processing kernels~\cite{Halide}. Since numerical methods for partial differential equations (PDEs) are often structured as sequences of parallelizable ``sweeps'' over the discretized equation domain, often implemented as memory-intensive loop nests, the following questions arise naturally: 

\begin{description}
\item[Applicability] Can we adopt sparse tiling (i.e., loop fusion composed with loop tiling), whose application has traditionally been limited to simple loop nests, in real-life numerical methods for solving PDEs?
\item[Lack of evidence] Why, despite decades of research, is it so difficult to find successful examples of integration of loop tiling with production code? 
\item[Challenges] What are the theoretical and technical challenges that we have to overcome to automate this optimization, such that an entire community of computational scientists can benefit from it?
\end{description}

In this chapter, we will try answering these questions. Our study focuses on a particular class of applications:
\begin{description}
\item[Irregular codes] Unstructured meshes are often used to discretize the computational domain, since they allow an accurate representation of complex geometries. For this type of meshes, the connectivity is usually stored by means of adjacency lists (or any equivalent structure), which results in indirect memory accesses (e.g., \texttt{A[B[i]]}) within loop nests. Indirections break static analysis, thus making any compiler-based approach (e.g., polyhedral optimization) unsuitable for our context. Indirections also make data dependence analysis more difficult, since this must now take place at runtime, introducing additional overhead.
\item[Realistic dataset] Most complex simulations operate on at least gigabytes of data, requiring multi-node execution. Any tiling-based optimization we will consider must therefore cope well with MPI execution.
\item[Automation, but no legacy code] We view tiling as an ``extreme optimization''; that is, as an optimization that should be tried at the end of the development process, once the experimental results have been validated, to improve the execution time. In our experience, however, it is extremely rare that computational scientists have the expertise to add any low level optimizations, such as loop tiling, to their codes. In addition, applying these optimizations at the source level makes the code impenetrable and, therefore, almost impossible to maintain and to extend. Our aim is a fully automated technique abstracted to the users through a simple ``switch'' (i.e., loop tiling on/off) and a tiny set of parameters to drive performance tuning (e.g., the tile size). Automation clearly comes at the price of implementation complexity; we will integrate our tiling system with a generic framework for mesh iteration. We are therefore not interested in supporting legacy code, in which key computational aspects (e.g., mesh iteration, MPI communication) are usually hidden, for modularity, underneath several layers of software.
\end{description}

To support this class of applications and in order to maximize the chances that our work can be used in a variety of simulations in the years to come, our approach builds on two pillars:
\begin{itemize}
\item a library for writing inspector/executor schemes (\cite{IEscheme}) for tiling arbitrary computations on unstructured meshes (or graphs);
\item a multilayer framework based on DSLs and runtime code generation that uses such a library to automate the applciation of loop tiling.
\end{itemize}

\subsection{Hypotheses}
\label{sec:tiling:struct}

\begin{algorithm}
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}

// Time-stepping loop (T = total number of iterations)\\
\KwSty{for} t = 0 \KwSty{to} T $\lbrace$\\
~~// 1st sweep over the $C$ cells of the mesh\\
~~\KwSty{for} i = 0 \KwSty{to} $C$ $\lbrace$\\
~~~~buffer$\_$0 = gather$\_$data ( A[f(map[i])], ... )\\
~~~~...\\
~~~~kernel$\_$1( buffer$\_$0, buffer$\_$1, ... );\\
~~~~scatter$\_$data ( buffer$\_$0, f(map[i]) )\\
~~$\rbrace$\\
~~Calc ( ... );\\
~~MPI$\_$Comm ( ...); \\
~~// 2nd sweep over the $N$ nodes of the mesh\\
~~\KwSty{for} i = 0 \KwSty{to} $N$ $\lbrace$\\
~~~~... // Similar to sweep 1 \\
~~$\rbrace$\\
~~// Boundary conditions: sweep over the $BE$ boundary edges\\
~~\KwSty{for} i = 0 \KwSty{to} $BE$ $\lbrace$\\
~~~~... // Similar to sweep 1 \\
~~$\rbrace$\\
$\rbrace$
\caption{....}
\label{code:tiling-struct}
\end{algorithm}

The ``bare'' structure of a scientific code solving a PDE by applying a numerical method on a discretized domain is shown in Listing~\ref{code:tiling-struct}. In the next section, we will use this example to motivate two fundamental hypotheses:

\begin{description}
\item[Hypothesis 1] A wide range of loop nests in mesh-based codes is, from a computational viewpoint, memory-bound, and increasing the data reuse across consecutive loop nests may relieve this issue.
\item[Hypothesis 2] Automating loop tiling in this kind of codes presents challenges that none of the state-of-the-art technologies can overcome. Our approach aims to solve this problem.
\end{description}



% Is skewing used for optimizing cross-time dependency, i.e., is the wave due to time?

\subsection{Automating Loop Tiling is More Difficult than Commonly Thought}
We identify three classes of problems that are either neglected or treated disjointly in the relevant literature. 

\begin{description}
\item[Theoretical questions] We first of all wonder whether loop tiling really is the optimization we are looking for.
\begin{description}
\item[Computational Boundedness] Since most computational methods are structured as sequences of loop nests and each loop nest is characterized by its own operational intensity, the computational boundedness is a concept that does not refer to the whole application. Rather, some loop nests may be memory-bound, whereas others CPU-bound, due to the complexity of the equations being solved or the employment of high order function spaces. Clearly, if all loop nests in an application are CPU-bound, the benefits of tiling on data locality will tend to be marginal. Before applying an aggressive optimization such as loop tilling, it is therefore fundamental to study the computational behaviour of an application for: (i) determining what fraction of the execution time is due to memory-bound loop nests; (ii) understanding if CPU-boundedness can be eliminated by applying other optimizations (e.g., vectorization), or if it is instead a natural consequence of the kernels and the architecture.
\item[Tiling vs Space Filling Curves] It is our impression that different communities have disjointedly researched solutions to the problem of relieving the memory pressure of mesh-based computations for years. We view both tiling and space filling curves (SFCs) -- and likewise sequential execution -- as instances of possible scheduling functions for a given loop nest (see Section~\ref{sec:bkg:poly}). Nevertheless, we could not find studies comparing the performance of the two approaches, denoting a lack of communication between people in two distinct yet potentially very close domains: compiler optimization and computational science.

\end{description}

\item[Practical issues] Loop tiling for structured meshes is widely covered in the literature. Several studies have tackled automation (e.g., polyhedral compilers), time-loop tiling (i.e., time skewing), exotic tile shapes for minimizing communication (e.g., diamond tiling), and so on. However, the following challenges tend to be neglected, or at least only marginally tackled.
\begin{description}
\item[Unstructured meshes] A technique to tile arbitrary computations on unstructured meshes was missing until this thesis\footnote{We reinforce once more that the generalized tiling algorithm is actually the result of a joint collaboration between .... See~\cite{st-paper}.}. Some inspector-executor strategies for tiling specific benchmarks, as discussed in Section~\ref{sec:tiling:relatedwork}, were available though. 
\item[Time tiling and MPI] We reiterate the fact that real computations almost always run on distributed memory machines. The multi-node parallelization is often carried out resorting to MPI. This is evident in Listing~\ref{code:tiling-struct} where MPI calls are placed between two consecutive mesh sweeps, since the first cells loop produces data needed by the subsequent nodes loop. Distributed memory parallelism poses a big challenge for time tiling, because now tiles at the partition boundary need special handling.
\item[Time tiling and extra code] The \texttt{Comp(...)} function in Listing~\ref{code:tiling-struct} denotes the possibility that additional computation is performed between consecutive sweeps. This could be, for instance, check-pointing or I/O. Also, conditional execution of loops (e.g., through \texttt{if-then-else}) breaks time tiling. 
\item[Legacy code is usually impenetrable] Our experience is that real-life code for scientific simulation tends to hide the tiling opportunities that a polyhedral compiler, for example, would search for. As explained in~\cite{strout-common-problems}, common problems are: 1) finding tilable loop nests, which are usually hidden for code modularity; 2) handling of boundary conditions; 3) dist
\end{description}

\item[Limitations inherent in the numerical method] Two loops cannot be fused if they are separated by a global synchronization point. This is often a global reduction, either explicit (e.g., the first loop updates a global variable that will be needed by the second loop) or implicit (i.e., within an external function invoked in between the two loops, like in many iterative solvers for linear systems). By limiting the applicability of many loop optimizations, global synchronization points pose great challenges and research questions. If strong scaling is the primary goal and memory-boundedness is the key limiting factor, then fundamental questions are: (i) can the numerical method be reformulated to relieve the constraints on low level optimization (which requires a joint effort between people in different domains); (ii) can the tools be made more sophisticated to work around these problems; (iii) most importantly, will the effort be rewarded by significant performance improvements?
\end{description}

In this chapter, we will formulate answers to some of these problems.



%\subsection{Time-Tilable Codes and Possible Solutions}
%I've been trying to solve these issues. Especially points 4, 5 and 2 (thanks Michael for simplifying my life!)
%
%
%What's the solution we propose?
%DSL and automated code generation schemes. Goal is to generate code which is a *suitable input* for other tools capable of saving us "automagically" from the burden of optimising everything by ourself
%
%
%So, back to the original question: "Is non-trivial loop tiling really widely applied in real-world software ?" And I said "no" because of the aforementioned points. 
%
%Where are with REAL tiling? Who is actually using it in real codes? What sort of improvements did they get? How did they work around the MPI "problem"?
%
%And, finally:
%
%What do we want to say about that body of literature presenting increasingly sophisticated tiling schemes that *seem* to be rarely used in practice? What do we want to say and ask to people talking about tiling?
%
%I'd like to hear what you think about this.

\section{Related Work}
\label{sec:tiling:relatedwork}

\subsection*{Loop Chain}
% Loop chain
The data dependence analysis that we will develop is based on an abstraction called \textit{loop chain}, which was originally presented in~\cite{KriegerHIPS2013}. This abstraction is sufficiently general to capture data dependency in programs structured as arbitrary sequences of loops. We will detail our loop chain abstraction in Section~\ref{sec:tiling:lc}.

\subsection*{Inspector/Executor and Sparse Tiling}
% Inspector/Executor
The loop chain is the mechanism by means of which we will generalize inspector/executor schemes for unstructured codes. Inspector/executor strategies were first formalized by~\cite{Saltz91} and used to fuse and tile loops for improving data locality and providing parallelism in~\cite{dimeEtna00,StroutLCPC2002,Demmel08,KriegerIAAA2012}. 

Sparse tiling, which we introduced in Section~\ref{sec:bkg:ie}, is the most notable technique based upon inspection/execution. The term was coined by Strout et al.~\cite{StroutLCPC2002,StroutIJHPCA} in the context of the Gauss-Seidel algorithm and in~\cite{StroutPLDI03} in the context of the moldyn benchmark. However, the technique was initially proposed by Douglas et al.~\cite{dimeEtna00} to parallelize computations over unstructured meshes, taking the name of \textit{unstructured cache blocking}. The mesh was initially partitioned; the partitioning represented the tiling in the first sweep over the mesh. Tiles would then shrink by one layer of vertices for each iteration of the loop. This shrinking represented what parts of the mesh could be update in later iterations of the outer loop without communicating with the processes executing other tiles. The unstructured cache blocking technique also needed to execute a serial clean-up tile at the end of the computation. Mark Adams~\cite{Adams99c} also developed an algorithm very similar to sparse tiling, to parallelize Gauss-Seidel computations. The main difference between~\cite{StroutLCPC2002,StroutIJHPCA} and~\cite{dimeEtna00} was that in the former work the tiles fully covered the iteration space, so a sequential clean-up phase at the end could be avoided. 

We reiterate the fact that all these approaches were either specific to individual benchmarks or general but not scheduling across loops (i.e., loop fusion). Filling this gap is one of the contributions of this chapter.

\subsection*{Automated Code Generation and Optimization for Mesh-Based Computations}
% Automated code generation and unstructured grids
The automated code generation technique presented in \cite{Ravishankar12} examines the data affinity among loops and partitions the execution with the goal of minimizing communication between processes, while maintaining load balancing. This technique supports unstructured mesh applications (being based on an inspector/executor strategy) and targets distributed memory systems, although it does not exploit the loop chain abstraction and abstracts from loop optimization.

% Automated code generation and structured grids 
Automated code generation techniques, such as those based on polyhedral compilers (reviewed in Section~\ref{sec:bkg:poly}), have been applied to structured mesh benchmarks or proxy applications. Notable examples are~\cite{pluto,polly,loopy}. There has been very little effort in providing evidence that these tools can be effective in real-world applications. Time-loop diamond tiling was applied in~\ref{cohen-timetiling} to a proxy code, but experimentation is limited to a single-node.
%TODO other examples?

\subsection*{Overlapped Tiling}
% Overlapped tiling
In structured codes, Using multiple layers of halo, or ``ghost'', elements is a common optimization in structured codes~\cite{Bassetti98}. Overlapped tilling (see Section~\ref{sec:bkg:tiling}) exploits the same principle to reduce communication at the expense of performing redundant computation along the boundary~\cite{Zhou12}. Several studies centered on overlapped tiling within single regular loop nests (mostly stencil-based computations)~\cite{Meng09,Krishnamoorthy07,Chen02}. Techniques known as ``communication avoiding''~\cite{Demmel08,commAvoidingSparse2009} also fall in this broader category. To the best of our knowledge, overlapped tiling for unstructured grids by automated code generation has been studied only analytically in~\cite{gihan-overlapped}.



\section{Generalized Inspector/Executor Schemes}
\label{sec:tiling:lc}
In Section~\ref{sec:tiling:motivation} we introduced that our approach to sparse tiling is based upon an \textit{inspector/executor scheme automatically generated at execution time}. In this section, we explain what is required to to construct such an inspector/executor scheme. The automation of this process will be tackled in Section~\ref{sec:tiling:automation}.


\subsection{The Loop Chain Abstraction}
The mechanism we build is based upon the \textit{loop chain}, an abstraction originally introduced in~\cite{KriegerHIPS2013}. Informally, a loop chain is a sequence of loops with no global synchronization points, with attached some extra information to enable run-time data dependence analysis. 

We recall from Section~\ref{sec:tiling:limits} that the complexity -- particularly the presence of indirect memory accesses -- and the modularity of real-world unstructured mesh applications prevent most common static optimizations for data locality. The data dependence information carried by a loop chain is supposed to be used to replace compile-time with run-time optimization. The basic idea is that the user provides the loop chain; then the compiler modifies the source code adding two extra pieces of code, the inspector and the executor; finally, at run-time, the inspector will exploit the data dependence information to build a ``scheduling'', which will eventually be used by the executor. 

Before diving into the description of the loop chain abstraction, we observe two notable facts:
\begin{itemize}
\item The run-time execution of the inspection may introduce a significant overhead. In many scientific computations, however, the data dependence information is static; or, in more informal words, ``the mesh does not change over time''. Usually, the loop chain will be located within the time loop of the application, so the inspection cost is amortized. If the mesh changes over time (e.g., in case of adaptive mesh refinement), the inspection needs be re-executed. We have devoted a significant effort in optimizing the inspection algorithm,  with the hope that its cost tends to be negligible even in the extreme case of frequent changes in the data dependence pattern. Further details will be provided in the later sections. 
\item The loop chain is expected to be provided by the user. We reinforce the idea that, in our approach, we will have two possibilities for constructing loop chains: either explicitly, with users required to change their program by adding calls to an external library to build the inspector/executor scheme, or implicitly, with the loop chain built automatically ``behind the scenes'' through a higher level framework.
\end{itemize}

In~\cite{ST-KriegerHIPS2013,KriegerThesis}, a loop chain $\mathbb{L}$ is defined as follows:
\begin{itemize}
\item $\mathbb{L}$ consists of $n$ loops, $L_0, L_1, ..., L_{n-1}$. There are no global synchronization points between the loops. The execution order of a loop's iterations does not influence the result. This means that given $L_i$, we can choose an arbitrary scheduling of iterations because there will not be loop-carried dependencies. 
\item $\mathbb{D}$ is a set of disjoint $m$ data spaces, $D_0, D_1, ..., D_{m-1}$. Each loop accesses (reads from, writes to) a certain subset of these data spaces. An access can be either direct (e.g., \texttt{A[i]}) or indirect (e.g., \texttt{A[map(i)]}).
\item $R_{L_l\rightarrow D_d}(\vec{i})$ and $W_{L_l\rightarrow D_d}(\vec{i})$, where the $R$ and $W$ access relations are defined over for each data space $D_d \in D$ and indicate which data locations in data space $D_d$ an iteration $i \in L_l$ reads from and writes to respectively (e.g., if we have \texttt{A[map(i)] = ...} in loop $L_j$, the access relation $map_{L_j\rightarrow A}(\vec{i})$ will be available). 
\end{itemize}

\subsection{Loop Chains for Unstructured Mesh Computations}
\label{sec:tiling:lc-unstruct}
Because of our focus on unstructured mesh computations, and inspired by the programming and execution models of OP2 (see Section~\ref{sec:bkg:op2}), the definition of a loop chain $\mathbb{L}$ is refined as follows:
\begin{itemize}
\item $\mathbb{L}$ consists of $n$ loops, $L_0, L_1, ..., L_{n-1}$. There are no global synchronization points between the loops. The execution order of a loop's iterations does not influence the result. This means that given $L_i$, we can choose an arbitrary scheduling of iterations because there will not be loop-carried dependencies. 

\item $\mathbb{S}$ is a set of disjoint $m$ sets, $S_0, S_1, ..., S_{m-1}$. Possible sets are the elements in the mesh or the degrees of freedom associated to a certain function. A loop iterates over a set, or iteration space. A dataset is logically associated with a set (i.e., each value in the dataset is associated with a set element). 

A set $S$ is logically split into three contiguous regions: core ($S^{c}$), boundary ($S^{b}$), and non-exec ($S^{ne}$). Given a process $P$ and a set $S$, we have that:
\begin{description}
 \item[$S^{c}$] The iterations of $S$ that exclusively belong to $P$.
 \item[$S^{b}$] The boundary region can be seen as the union of two sub-regions, owned ($S^{owned}$) and exec ($S^{exec}$). $S^{owned}$ are iterations that belong to $P$ which will be redundantly executed by some other processes; $S^{exec}$ are iterations from other processes which are redundantly executed by $P$. We will see that redundant execution is exploited to preserve atomic execution; that is, the fact that all tiles can execute their iterations without the need for synchronization.
  \item[$S^{ne}$] These are iterations of other processes that are communicated to $P$ because they need be indirectly read to correctly compute $S^{b}$.
 \end{description} 
 
A set is uniquely identified by a name and the sizes of its three regions. For example, the notation $S = (\operatorname{vertices}, C, B, N)$ defines the $\operatorname{vertices}$ set, which comprises $C$ elements in the core region (iterations $0$ to $C-1$), $B$ elements in the boundary region (iterations $C$ to $C+B-1$), and $N$ elements in the non-exec region (iterations $C+B$ to $C+B+N-1$).
%We therefore have $S = S^{c} \frown S^{b} \frown S^{ne}$. 

\item The {\em depth} is an integer indicating the extent of the boundary region of a set. This constant is the same for all sets. 

\item $\mathbb{M}$ is a set of $k$ maps, $M_0, M_1, ..., M_{k-1}$. A map of arity $a$ is a vector-valued function $M : S_i \rightarrow S_j^0 \times S_j^1 \times ... \times S_j^{a-1}$. It connects each element of $S_i$ to one or more elements in $S_j$. For example, if a triangular cell $c$ is connected to three vertices $v_0, v_1, v_2$, we have $M(c) = [v_0, v_1, v_2]$. 

\item A loop $L_i$ over the iteration space $S$ is associated with $d$ descriptors, $D_0, D_1, ..., D_{d-1}$. Each descriptor $D_j$ is a 2-tuple $D_j = {<}M,\ \operatorname{mode}{>}$; $M$ is either a map from $S_j$ to some other sets or the special placeholder $\perp$, which indicates that the access is direct to some data associated with $S_j$, whereas $\operatorname{mode}$ is one of $[r,\ w,\ i]$, respectively read, write and increment.
\end{itemize}

With respect to the original definition, one of the most important differences is the lack of data spaces. In unstructured mesh applications, loops tend to access multiple data spaces associated with the same set, so the idea is to rather rely on the latter to perform data dependency analysis. This can significantly improve the inspection cost, because typically $|\mathbb{S}| << |\mathbb{D}|$. 

The second fundamental difference is the characterization of sets into the three regions core, boundary and non-exec. This separation is essential for distributed memory parallelism, whereas we have  $S^{b} = S^{ne} = \emptyset$ in the case of pure shared memory execution. The extent of the boundary regions is captured by the {\em depth} of the loop chain, whose meaning will become more clear in Section~\ref{algo:st-executor}. 

\begin{algorithm}
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}

\KwSty{for} t = 0 \KwSty{to} T $\lbrace$\\
~~// Loop over edges\\
~~\KwSty{for} e = 0 \KwSty{to} E $\lbrace$\\
~~~~x = X[e];\\
~~~~tmp$\_$0 = tmp + edges2vertices[e + 0];\\
~~~~tmp$\_$1 = tmp + edges2vertices[e + 1]; \\
~~~~kernel1 (x, tmp$\_0$, tmp$\_$1);\\
~~$\rbrace$\\
~\\
~~// Loop over cells\\
~~\KwSty{for} c = 0 \KwSty{to} C $\lbrace$\\
~~~~res = R[C];\\
~~~~tmp$\_$0 = tmp + cells2vertices[c + 0];\\
~~~~tmp$\_$1 = tmp + cells2vertices[c + 1];\\
~~~~tmp$\_$2 = tmp + cells2vertices[c + 2];\\
~~~~kernel2 (res, tmp$\_0$, tmp$\_$1, tmp$\_$2);\\
~~$\rbrace$\\
~\\
~~// Loop over edges\\
~~\KwSty{for} e = 0 \KwSty{to} E $\lbrace$\\
~~~~tmp$\_$0 = tmp + edges2vertices[e + 0];\\
~~~~tmp$\_$1 = tmp + edges2vertices[e + 1]; \\
~~~~kernel3 (tmp$\_0$, tmp$\_$1);\\
~~$\rbrace$\\
$\rbrace$\\

\caption{Section of a toy program that is used as a running example to illustrate the loop chain abstraction and show how the tiling algorithm works.}
\label{code:tiling-loopchain}
\end{algorithm}


\begin{algorithm}
\scriptsize\ttfamily
\SetAlgorithmName{LISTING}{}
inspector = init$\_$inspector (...);\\
~\\
// Three sets, edges, cells, and vertices\\
E = set (inspector, core$\_$edges, boundary$\_$edges, nonexec$\_$edges, ...);\\
C = set (inspector, core$\_$cells, boundary$\_$cells, nonexec$\_$cells, ...);\\
V = set (inspector, core$\_$vertices, boundary$\_$vertices, nonexec$\_$vertices, ...);\\ 
~\\
// Two maps: from edges to vertices and from cells to vertices\\
e2vMap = map (inspector, E, V, edges2vertices, ...);\\
c2vMap = map (inspector, C, V, cells2vertices, ...);\\
~\\
// The loop chain comprises three loops; each loop has a set of descriptors\\
loop (inspector, E, $\lbrace \perp$, READ$\rbrace$, $\lbrace$e2vMap, INC$\rbrace$);\\
loop (inspector, C, $\lbrace \perp$, READ$\rbrace$, $\lbrace$c2vMap, INC$\rbrace$);\\
loop (inspector, E, $\lbrace$e2vMap, INC$\rbrace$); \\
~\\
// Now can run the inspector\\
inspection = run$\_$inspection (inspector, tile$\_$size, ...)\\
~\\
\caption{Building the loop chain for inspection.}
\label{code:tiling-inspector}
\end{algorithm}


Listing~\ref{code:tiling-loopchain} shows a plain C implementation of the unstructured mesh OP2 program illustrated in Listing~\ref{code:op2program}. A loop chain for this program is constructed in Listing~\ref{code:tiling-inspector}. The inspection output is stored in the data structure \texttt{inspection}. This is used (Listing~\ref{code:tiling-executor}) to execute a ``tiled'' version of the original program. In the next section, we show through examples how the inspector exploits the information captured by the loop chain to construct tiles.


\section{Sparse Tiling Examples}
Using the example in Listing~\ref{code:tiling-loopchain}, we describe the actions performed by a sparse tiling inspector. We show two variants of inspection: for shared-memory and distributed-memory parallelism. 

\subsection{Overview of the Algorithm}
The inspector starts with partitioning the iteration space of a \textit{seed loop}, for example $L_0$. Partitions are used to initialize tiles: the edges in partition $P_i$ -- or, equivalently, the iterations of $L_0$ falling in $P_i$ -- are assigned to tile $T_i$. In the later stages of the inspection,  as detailed in the upcoming sections, $T_i$ will be populated with iterations from $L_1$ and $L_2$. The executor will assign a single thread/process to each $T_i$. $T_i$ is executed ``atomically''; that is, without the need for communication with other tiles. When executing $T_i$, first all iterations from $L_0$ are executed, then all iterations from $L_1$ and finally those from $L_2$. The challenge in scheduling iterations from $L_j$ to $T_i$, therefore, is to guarantee that all data dependencies -- read after write, write after read, write after write -- are preserved. 

\subsection{Shared-Memory Parallelism}
\label{sec:tiling:ex-sm}
Figure~\ref{fig:st-initial-part-sm} displays the situation after the initial partitioning of $L_0$. 

There are four partitions, two of which are not connected through any edge or cell. These four partitions correspond to four tiles, $[T_0,\ T_1,\ T_2,\ T_3]$. Similarly to OP2, for shared-memory parallelism we adopt the following approach: (i) we serialize the execution of the elements inside a partition by scheduling them to a single thread; (ii) we color the partitions such that partitions assigned the same color can be executed in parallel by different threads. Two partitions can have the same color if they are not connected, because this ensures the absence of race conditions through indirect memory accesses. In the example we can use three colors: red (R), green (G), and blue (B). The two partitions at the top right and bottom left are not connected, so they are assigned the same color (green). In the following, with the notation $T_i^c$ we mean that the $i$-th tile has color $c$.

In order to populate the tiles with iterations from $L_1$ and $L_2$, we first have to establish a total ordering for the execution of partitions with different colors. Here, we assume the following order: green (G), blue (B), and red (R). This means, for instance, that \textit{all iterations} assigned to $T_1^B$ must be executed \textit{before all iterations} assigned to $T_2^R$. By ``all iterations'' we mean the iterations from $L_0$, as established by the seed partitioning, as well as the iterations that will later be assigned from $L_1$ and $L_2$. We assign integer positive numbers to colors to reflect their ordering, where a smaller number means higher execution priority. In this case, we can assign 0 to green, 1 to blue, and 2 to red.

To schedule the iterations of $L_1$ to $[T_0^G,\ T_1^B,\ T_2^R,\ T_3^G]$, we first need to compute a \textit{projection} of any write or local reduction performed within $L_0$.  A projection for $L_0$ is a function $\phi : V \rightarrow \mathbb{T}$ mapping a vertex (indirectly) incremented during the execution of $L_0$ to a tile.  Consider the vertex $v_0$ in Figure~\ref{fig:st-proj-1}. $v_0$ has 7 incident edges, 2 of which belong to $T_0^G$, while the remaining 5 to $T_1^B$. Since we established that $G \prec B$, $v_0$ can only be read after $T_1^B$ has finished executing the iterations from $L_0$ (i.e., the 5 incident blue edges). We express this condition by setting $\phi(v_0) = T_1^B$. Observe that we can compute $\phi$ by iterating over $V$ and, for each vertex, applying the maximum function ($MAX$) to the color of the adjacent edges. 

We now use $\phi$ to schedule $L_1$, a loop over cells, to the tiles. Consider again $v_0$ and the adjacent cells $[c_0,\ c_1,\ c_2]$. These three cells have in common the fact that they are adjacent to both green and blue vertices. For $c_1$, and similarly with the other cells, we compute $MAX(\phi(v_0),\ \phi(v_1),\ \phi(v_2)) = MAX(B, G, G) = B = 1$. This establishes that $c_1$ must be assigned to $T_1^B$, because otherwise ($c_1$ assigned instead to $T_0^G$) a read to $v_0$ would occur before the last increment from $T_1^B$ took place. We indeed reiterate once more that the execution order is ``all iterations from $[L_0, L_1, L_2]$ in the green tiles before all iterations from $[L_0, L_1, L_2]$ in the blue tiles''.

In order to schedule $L_2$ to $[T_0^G,\ T_1^B,\ T_2^R,\ T_3^G]$ we employ a similar process. Vertices are again written by $L_1$, so a new projection over $V$ will be computed and will be used in place of $\phi$ to schedule the edges in $L_2$. Figure~\ref{fig:st-final} shows the result of this last phase. 

\subsubsection*{Conflicting Colors}
It is worth noting how $T_2^R$ ``consumed'' the frontier elements of all other tiles every time a new loop was scheduled. Tiling a loop chain consisting of $k$ loops had the effect of expanding the frontier of a tile of at most $k$ vertices. With this in mind, we re-inspect the loop chain of the running example, although this time with a different execution order: blue (B), red (R), and green (G). We recall that the total ordering of colors is arbitrary, so this situation is legal. By applying the same inspection procedure that we just described, $T_0^G$ and $T_3^G$ will eventually become connected (see Figure~\ref{fig:st-conflicts}), thus violating the precondition ``tiles/partitions with the same color can run in parallel''. Indeed, race conditions during the execution of iterations belonging to $L_2$ are now theoretically possible. We will explain how to handle this problem in Section~\ref{sec:tiling:algo}.

\subsection{Distributed-Memory Parallelism}
\label{sec:tiling:ex-dist}
If parallelism is achieved through message-passing, the mesh and all datasets defined over it are distributed to different processes. Similarly to the example in Listing~\ref{code:tiling-struct}, MPI calls now separate the execution of two consecutive loops in the chain. This means that our inspector/executor scheme will have to take extra care of data dependencies along the mesh partition boundaries.

We logically split the sets into three regions, \textit{core}, \textit{boundary}, and \textit{non-exec}. The boundary region includes all iterations that cannot be executed until the MPI communications have successfully terminated. We pick $L_0$ as seed loop and we initialize the tiles as follows:
\begin{enumerate}
\item We take the core region of $L_0$ and partition it into tiles. Unless we aim for a mixed distributed/shared-memory parallelization scheme, there is no need to reassign the same color to unconnected tiles since we now have a single process executing sequentially all tiles (as opposed to the case of shared memory parallelism discussed in Section~\ref{sec:tiling:ex-sm}). We can assign colors increasingly: $T_i$ is given color $i$, so we can uniquely identify a tile with its color. As long as tiles with contiguous ID are also physically contiguous in the mesh, and assuming that colors are executed in ascending order (i.e., $T_0,\ T_1, ...$, as in the shared memory case), this assignment retains spatial locality when ``jumping'' from executing $T_i$ to $T_{i+1}$.
\item We replicate over the boundary region of $L_0$ what we just did for core. Note that by neatly separating core and boundary, there cannot be tiles crossing the two regions. Further, all tiles within boundary have greater color than those in core, posing a constraint on the execution order: no boundary tiles can be executed until all core tiles have been scheduled.
\item We map the whole non-exec region of $L_0$ to a single special tile, $T_{ne}$. This tile has highest color and will actually never be executed. Its role will be explained next.
\end{enumerate}

In Figure~\ref{fig:st-mpi-init}, the same mesh of Figure~\ref{fig:st-initial-part-sm} is distributed to two processes and the output of the initialization phase is displayed. The inspection then proceeds as in the case of shared memory parallelism. The application of the $MAX$ function when scheduling iterations from $L_1$ and $L_2$ to tiles makes higher color tiles (i.e., those that will be scheduled later at execution time) ``grow over'' lower color ones. In particular, the whole boundary region ``expands'' over core, and so does $T_{ne}$ over boundary (see Figure~\ref{fig:st-mpi-growth}). 

The executor starts with triggering the all MPI communications required for the execution of the loop chain; it proceeds to scheduling all core tiles, thus overlapping communication with computation; it finishes, once all core tiles have been executed and the MPI communications have been accomplished, with scheduling the boundary tiles.

We finally observe that:
\begin{description}
\item[Correctness] This inspector/executor scheme relies on redundant computation along the mesh partition boundary. The depth of the boundary region grows proportionally with the length of the loop chain. Roughly speaking, if the loop chain consists of $n$ loops, then the boundary region needs $n-1$ extra layers of elements. In Figure~\ref{fig:st-mpi-init}, the elements $[...]$ belong to $R_1$, although they also need be executed by $R_0$ in order for $[...]$ to be correctly computed when $T_x$ executes iterations from $L_2$. This is a conceptually simple expedient, which however poses a big challenge on software engineering. 
\item[Efficiency] The underlying hypothesis is that the increase in data locality achieved through sparse tiling will outweigh the overhead introduced by the redundant computation. This is based on the consideration that, in real applications, the core region tends to be way larger than the boundary one. In addition, not all iterations along the boundary region need be redundantly executed at every loop. For example, if we consider Figure~\ref{fig:st-mpi-nonexec}, we see that the strip of elements $[...]$ is not relevant any more for the correct computation of $[...]$. The non-exec tile $T_{ne}$, which we recall is not scheduled by the executor, is deliberately given the highest color to grow over the boundary region: this will leave dirty values in some datasets until the next round of MPI communications, but will not hurt correctness. 
\end{description}



%The main difference with respect to frameworks such as OP2 (see Section~\ref{sec:bkg:op2}) is that here, for reasons that will be more evident next, two regions, \textit{owned} and \textit{exec}, have been merged into a single one, \textit{boundary}. In essence, 


\section{Formalization}
\label{sec:tiling:algo}

\subsection{Data Dependency Analysis for Loop Chains}

As with all loop optimizations that reschedule the iterations in a sequence of loops, any sparse tiling must satisfy the data dependencies. The loop chain abstraction, which we have described in Section~\ref{sec:tiling:lc}, provides enough information to construct an inspector which analyzes all of the dependencies in a computation and builds a legal sparse tiling. We recall that one of the main assumptions in a loop chain is that each loop is fully parallel or, equivalently, that there are no loop carried dependencies.

The descriptors in the loop chain abstraction enable a general derivation of the storage-related dependencies between loops in a loop chain. The storage related dependencies between loops can be described as either flow (read after write), anti (write after read), or output (write after write) dependencies. In the following, assume that loop $L_x$, having iteration space $S_x$, always comes before loop $L_y$, having iteration space $S_y$, in the loop chain. Let us identify a descriptor of a loop $L$ with $m_{S_i \rightarrow S_j}^{mode}$: this simply indicates that the loop $L_i$ has iteration space $S_i$ and uses a map $m$ to write/read/increment elements (respectively, $\operatorname{mode} \in \lbrace w, r, i\rbrace$) in the space $S_j$.

The flow dependencies can then be enumerated by considering pairs of points ($\vec{i}$ and $\vec{j}$) in the iteration spaces of the two loops $L_x$ and $L_y$:
\[
	\{ \vec{i} \rightarrow \vec{j} \; | \; \vec{i} \in S_x \wedge \vec{j} \in S_y \wedge 
	m_{S_x\rightarrow S_z}^{w}(\vec{i}) \cap m_{S_y \rightarrow S_z}^{r}(\vec{j}) \ne \emptyset \}.
\]
Anti and output dependencies are defined in a similar way. The anti dependencies for all pairs of loops $L_x$ and $L_y$ are:
\[
	\{ \vec{i} \rightarrow \vec{j} \; | \; \vec{i} \in S_x \wedge \vec{j} \in S_y \wedge 
	m_{S_x\rightarrow S_z}^{r}(\vec{i}) \cap m_{S_y \rightarrow S_z}^{w}(\vec{j}) \ne \emptyset \}.
\]
While the output dependencies between loops $L_x$ and $L_y$ are:
\[
	\{ \vec{i} \rightarrow \vec{j} \; | \; \vec{i} \in S_x \wedge \vec{j} \in S_y \wedge 
	m_{S_x\rightarrow S_z}^{w}(\vec{i}) \cap m_{S_y \rightarrow S_z}^{w}(\vec{j}) \ne \emptyset \}.
\]
In essence, there is a storage-related data dependence between two iterations from different loops (and therefore between the tiles they are placed in) when one of those iterations writes to a data element and the other iteration reads from or writes to the same data element.

There are local reductions, or ``reduction dependencies'' between two or more iterations of the same loop when those iterations ``increment'' the same location(s); that is, when they read, modify with a commutative and associative operator, and write to the same location(s). The reduction dependencies in $L_x$ are:
\[
	\{ \vec{i} \rightarrow \vec{j} \; | \; \vec{i} \in S_x \wedge \vec{j} \in S_x \wedge m_{S_x\rightarrow S_z}^{i}(\vec{i}) \cap m_{S_x \rightarrow S_z}^{i}(\vec{j}) \ne \emptyset \}.
\]
The reduction dependencies between two iterations within the same loop indicates that those two iterations must be executed atomically with respect to each other.

As seen in the example in Section~\ref{sec:tiling:ex-sm}, our inspector algorithm handles data dependencies, including those between non-adjacent loops, by tracking \textit{projections}. In the next section we explain how projections are constructed and used.

%In order to schedule $L$ to tiles, an up-to-date projection for each set $S$ accessed by $L$ must be available. For example, assume that $S$ is read by $L_k$ and is written by both $L_i$ and $L_j$. In the loop chain, the loops are in the order $L_i \prec ... \prec L_j \prec ... \prec L_k$. Let us denote by $\phi_S$ the projection for $S$. Each time that a loop accessing $S$ in write mode is inspected for scheduling, $\phi_S$ must be updated. When eventually scheduling $L_k$, therefore, an up-to-date snapshot of $\phi_S$ (i.e., the one produced right after tiling $L_j$) will be used to compute a legal tiling.


\subsection{The Generalized Sparse Tiling Inspector}
\label{sec:tiling:inspector}

The pseudo-code for the generalized sparse tiling inspector is showed in Algorithm~\ref{algo:st-inspector}. Given a loop chain and an average tile size, the algorithm produces a schedule suitable for mixed distributed/shared memory parallelism. In the following, we elaborate on the main steps of the algorithm. The notation used throughout the section is summarized in Table~\ref{table:st-summary-notation}.

\begin{table}
\centering
\begin{tabulary}{1.0\columnwidth}{C|C}
\hline
Symbol & Meaning \\
\hline
$\mathbb{L}$ & The loop chain \\
$L_i$ & The $i$-th loop in $\mathbb{L}$ \\ 
$S_i$ & The iteration space of $L_i$ \\
$S_i^{c}$, $S_i^{b}$, $S_i^{n}$ & the core, boundary, and non-exec regions of $S_i$ \\ 
$S$ & A generic set in $\mathbb{L}$ \\
$S_{in}$, $S_{out}$ & The input and output sets of a map \\
$D$ & A descriptor of a loop \\
$r$, $w$, $i$ & Possible values for $D$.mode \\
$\mathbb{T}$ & The set of all tiles \\
$\mathbb{T}[i]$ & Accessing the $i$-th tile \\
$T_i^{c}$, $T_i^{b}$ & the $i$-th tile over the core and boundary regions \\
$\phi_S$ & A projection $\phi_S : S \rightarrow \mathbb{T}$ \\
$\Phi$ & The set of all available projections \\
$\sigma_i$ & The tiling function $\sigma_i : S_i \rightarrow \mathbb{T}$ for $L_i$ \\
\hline
\end{tabulary}
\caption{Summary of the notation used throughout the section.}
\label{table:st-summary-notation}
\end{table}

\setcounter{algocf}{0}% Modify counter of algorithm
\begin{algorithm}[t]
\kwInput{The loop chain $\mathbb{L} = [L_0,\ L_1,\ ...,\ L_{n-1}]$, a tile size $ts$}
\kwOutput{A set of tiles $\mathbb{T}$, populated with iterations from $L_i \in \mathbb{L}$}
\nonl ~\\
\Comment{Initialization}
$seed \gets 0$\;
$\Phi \gets \emptyset$\;
$C \gets \perp$\;
\nonl ~\\
\Comment{Creation of tiles}
$\sigma_{seed}$, $\mathbb{T} \gets$ partition($S_{seed}$, $ts$)\;
seed$\_$map $\gets$ find$\_$map($S_{seed},\ \mathbb{L}$)\;
conflicts $\gets$ false\;
\Do{\Not conflicts}{
  \Comment{Schedule loops to tiles}
  color($\mathbb{T}$, seed$\_$map)\;
   \nonl ~\\
  \For{$i=1$ \KwTo $n-1$}{
    project($L_{i-1}$, $\sigma_{i-1}$, $\Phi$, $C$)\;
    $\sigma_i \gets$ tile($L_i$, $\Phi$)\;
    assign($\sigma_i$, $\mathbb{T}$)\;
  }
  \nonl ~\\
  \If{found$\_$conflicts($C$)}{
    conflicts $\gets$ true\;
    add$\_$fake$\_$connection(seed$\_$map, $C$)\;
  }
}
\nonl ~\\
\Comment{Inspection successful, create local maps and return}
create$\_$local$\_$maps($\mathbb{T}$)\;
\Return{$\mathbb{T}$}
\caption{The inspection algorithm}
\label{algo:st-inspector}
\end{algorithm}




\paragraph{Choice of the seed loop}
The seed loop $L_{seed}$ is used to initialize the tiles. Theoretically, any loop in the chain can be chosen as seed. Supporting distributed memory parallelism, however, is cumbersome if $L_{seed} \neq L_0$. This is because more general partitioning and coloring schemes would be needed to ensure that no iterations in any $S_i^{b}$ are assigned to a core tile. A constraint of our inspector algorithm in the case of distributed memory parallelism, therefore, is that $L_{seed} = L_0$. 

In the special case in which there is no need to distinguish between core and boundary tiles (e.g., because a program is executed on a single shared memory system), the seed loop can be chosen arbitrarily. If we however pick $L_{seed}$ in the middle of the loop chain ($L_0 \prec ... \prec L_{seed} \prec ...$), a mechanism for constructing tiles in the reverse direction (``backwards''), from $L_{seed}$ towards $L_0$, is necessary. In~\cite{st-paper}, we propose two ``symmetric'' algorithms to solve this problem, \textit{forward tiling} and \textit{backward tiling}, with the latter using the $MIN$ function in place of $MAX$ when computing projections. For ease of exposition, and since in the fundamental case of distributed memory parallelism we impose $L_{seed} = L_0$, we here neglect this distinction\footnote{The algorithm that is implemented in the SLOPE library, however, is more general and supports backwards tiling for the case in which distributed memory parallelism is not required.}. 

%\textit{Note: in real applications loops over ``subsets'' are possible; for instance, a loop over the exterior facets of the mesh. If one such loop is picked as seed, then the inspection is aborted.}

\paragraph{Tiles initialization}
The algorithm starts with partitioning $S_{seed}^{c}$ into $m$ subsets $\lbrace P_0, P_1, ..., P_{m-1}\rbrace$ such that $P_i \cap P_j = \emptyset$ and $\cup_{i = 0}^{m-1} P_i = S_{seed}^{c}$. The partitioning is sequential: $S_{seed}$ is ``chunked'' every $ts$ iterations, with $P_{m-1}$ that may be smaller than all other partitions. Similarly, we partition $S_{seed}^{b}$ into $k$ subsets.

We then create $m+k+1$ tiles, $\mathbb{T} = \lbrace T_0^c, ..., T_{m-1}^c, T_m^{b}, ..., T_{m+k-1}^b, T_{m+k}^{n} \rbrace$, one for each partition and one extra tile for $S_{seed}^{n}$. 

A tile $T_i$ has four fields, as summarized in Table~\ref{table:st-tile-structure}. The region is used by the executor to schedule tiles in a certain order. The value of this field is determined right after the seed partitioning, since a tile is exclusively in one of $\lbrace S_{seed}^c, S_{seed}^b, S_{seed}^n \rbrace$. The iterations lists represent the iterations in $\mathbb{L}$ belonging to $T_i$. For each $L_j \in \mathbb{L}$, there is one iterations list $T_i^{L_j}$. At the beginning, for each $T_i \in \mathbb{T}$ we have $T_i^{L_{seed}} = T_i^{L_0} = P_i$, whereas all other lists are empty. Local maps are used by the executor in place of the global maps provided by the loop chain; we will see that this allows avoiding double indirections.


\begin{table}
\centering
\begin{tabulary}{1.0\columnwidth}{P{2.7cm} | P{8.5cm}}
\hline
Field & Possible values \\
\hline
region & core, boundary, non-exec \\
color & an integer representing the execution priority \\ 
iterations lists & one list of iterations (integers) $T_i^{L_j}$ for each $L_j \in \mathbb{L}$\\ 
local maps & one list of local maps for each $L_j \in \mathbb{L}$; one local map for each map used in $L_j$\\
\hline
\end{tabulary}
\caption{The tile data structure.}
\label{table:st-tile-structure}
\end{table}

Each tile needs be assigned a color, which gives it a scheduling priority. If shared memory parallelism is requested, adjacent tiles are given different colors (the adjacency relation is determined through the maps available in $\mathbb{L}$). Otherwise, the colors are assigned to the tiles in increasing order (i.e., $T_i$ is given color $i$). The boundary tiles are always given colors higher than that of core tiles; the non-exec tile has highest color. In essence, this will allow the executor to schedule all core tiles first and all boundary tiles afterwards. 

\paragraph{Construction of tiles by tracking data dependencies}
In order to schedule a loop to tiles we use projections. A projection is a function $\phi_S : S \rightarrow \mathbb{T}$. Projections are computed and/or updated after tiling a loop. Initially, the set $\Phi$ of all projections is empty. Starting with the seed tiling $\sigma_{seed} : S_{seed} \rightarrow \mathbb{T}$, we iteratively update $\Phi$ and build tiling functions for all other loops $[L_1, L_2, ..., L_{n-1}]$. 

\paragraph{Projecting tiled sets}

\begin{algorithm}[t]
\kwInput{A loop $L_{i}$, a tiling function $\sigma_{i}$, the projections set $\Phi$, the conflicts matrix $C$}
\KwResult{Update $\Phi$ and $C$}
\nonl ~\\
\ForEach{$D$ in $L_i$.descriptors}{
  \If{$D$.mode $==$ r}{
    skip\;
  }
  \eIf{$D$.map $== \perp$}{
    $\Phi = \Phi \cup \sigma_{i}$\;
  }{
    inverse$\_$map $\gets$ map$\_$invert($D$.map)\;
    $S_{j}$, $S_{i}$, values, offsets $\gets$ inverse$\_$map\;
    $\phi_{S_{j}} \gets \perp$\; 
    \For{$e=0$ \KwTo $S_j$.size}{
      \For{$k=$ offsets[e] \KwTo offsets[e+1]}{
        adjacent$\_$tile = $\mathbb{T}$[values[k]]\;
        max$\_$color $\gets$ MAX($\phi_{S_{j}}[e]$.color, adjacent$\_$tile.color)\;
        \If{max$\_$color $\neq$ $\phi_{S_{j}}[e]$.color}{
          $\phi_{S_{j}}[e] \gets$ adjacent$\_$tile\;
        }
      }
    }
    update($C$, $\mathbb{T}$, $\phi_{S_{j}}$)\;
    $\Phi = \Phi \cup \phi_{S_{j}}$\;
  }
}
\caption{Projection of a tiled loop}
\label{algo:st-projection}
\end{algorithm}

Algorithm~\ref{algo:st-projection} takes as input $\sigma_{i-1}$ and (the descriptors of) $L_{i-1}$ to update $\Phi$. Further, the conflicts matrix $C \in \mathbb{N}^{m \times m}$, indicating whether two different tiles having the same color will become adjacent after tiling $L_{i-1}$, is updated. 

A projection tells what tile a set element logically belongs to at a given point of the tiling process. A new projection $\phi_{S}$ is needed if the elements of $S$ are written by $L_i$. Let us consider the non-trivial case in which writes or increments occur indirectly through a map $M : S_i \rightarrow S_j^0 \times S_j^1 \times ... \times S_j^{a-1}$. To compute $\phi_{S_{j}}$, we first determine the inverse map (an example is shown in Figure~\ref{fig:st-inverse-map}). Then, we iterate over all elements of $S_i$ that indirectly access elements in $S_j$. In particular, given $e \in S_j$, we determine the last tile that writes to $e$, say $T_{last}$, through the application of the $\operatorname{MAX}$ function to tile colors. We then simply set $\phi_{S_{j}}[e] = T_{last}$. 


% ($C[i,j] = 1$ implies that $T_i$ and $T_j$ have the same color and will become adjacent after tiling $L_{i-1}$).

\paragraph{Scheduling loops} 

\begin{algorithm}[t]
\kwInput{A loop $L_{i}$ (with iteration space $S_i$), the projections set $\Phi$}
\kwOutput{The tiling function $\sigma_{i}$}
\nonl ~\\
$\sigma_i \gets \perp$\;
\ForEach{$D$ in $L_i$.descriptors}{
  \eIf{$D$.map $== \perp$}{
    $\sigma_i \gets \Phi[S_i]$\;    
  }{
    arity $\gets D$.map.arity\;
    $\phi_{S} \gets \Phi[D$.map.$S_{j}$]\;
    \For{$e=0$ \KwTo $S_i$.size}{
       $\sigma_i[e] \gets T_{\perp}$ \;
      \For{$k=0$ \KwTo arity}{
        adjacent$\_$tile $\gets \phi_{S}$[$D$.map.values[e*arity + k]]\;
        max$\_$color $\gets$ MAX($\sigma_i[e]$.color, adjacent$\_$tile.color)\;
        \If{max$\_$color $\neq$ $\sigma_i[e]$.color}{
          $\sigma_i[e] \gets$ adjacent$\_$tile\;
        }
      }
    }
  }
}
\Return{$\sigma_i$}
\caption{Building a tiling function}
\label{algo:st-tiling}
\end{algorithm}

Using $\Phi$, we compute $\sigma_i$ as described in Algorithm~\ref{algo:st-tiling}. The algorithm is similar to the projection of a tiled loop, with the main difference being that now we use $\Phi$ to schedule iterations correctly. Finally, $\sigma_i$ is inverted and the iterations added to the corresponding iteration lists $T_j^{L_i}$ of all $T_j \in \mathbb{T}$. 


\paragraph{Detection of conflicts}
If $C$ indicates the presence of at least one conflict, say between $T_i$ and $T_j$, we add a ``fake connection'' between $T_i$ and $T_j$ and loop back to the coloring stage. $T_i$ and $T_j$ are now connected, so they will receive different colors. 
%Otherwise, a legal tiling has now been computed. 



\subsection{The Generalized Sparse Tiling Executor}

\begin{algorithm}[t]
\kwInput{A set of tiles $\mathbb{T}$}
\KwResult{Execute the loop chain, thus modifying the data sets written or incremented within $\mathbb{L}$}
\nonl ~\\
$\mathbb{T}^{c}$, $\mathbb{T}^{b}$ $\gets$ group$\_$tiles$\_$byregion($\mathbb{T}$)\;
\nonl ~\\
start$\_$MPI$\_$communications()\;
\nonl ~\\
\ForEach{available color $c$}{
  \ForEach{$T \in \mathbb{T}^{c}$ s.t. $T$.color $== c$}{
    execute$\_$tile($T$)\;
  }
}
\nonl ~\\
end$\_$MPI$\_$communications()\;
\nonl ~\\
\ForEach{available color $c$}{
  \ForEach{$T \in \mathbb{T}^{b}$ s.t. $T$.color $== c$}{
    execute$\_$tile($T$)\;
  }
}
\caption{The executor algorithm}
\label{algo:st-executor}
\end{algorithm}


The sparse tiling executor is illustrated in Algorithm~\ref{algo:st-executor}. It consists of four main phases: triggering of MPI communications; execution of core tiles (in overlap with communication); waiting for the termination of all MPI communications; execution of boundary tiles.

As seen in the example in Section~\ref{sec:tiling:ex-dist} and based on the explanation in Section~\ref{sec:tiling:inspector}, we know that the core tiles do not require any off-process information to execute as long as the boundary regions are (i) up-to-date and (ii) ``sufficiently deep''. If both conditions hold, the execution is semantically correct and it is safe to overlap computation of core tiles with the communication of boundary data. 


\paragraph{The depth of the boundary region}
In $\mathbb{L}$ we have $n$ loops. A tile ``crosses'' all of these loops and is executed atomically; that is, once it starts executing its iterations, it reaches the end without the need for synchronization with other processes. With this execution model, the boundary region must include a sufficient amount of off-process iterations for a correct computation of the tiles along the border with the core region. 

In the extreme case $n=1$, a single ``strip'' of iterations belonging to adjacent processes need be redundantly executed. As $n$ increases, more off-process data is required. If $n=3$, as in the example in Figure~\ref{fig:tiling:mpi-n3}, we need three ``strips'' of off-process iterations to be computed in order for the datasets over $X$ to be correctly updated when executing iterations belonging to $L_3$.

The {\em depth} is a constant provided through the loop chain abstraction that informs the inspector about the extent of the boundary region. For correctness, we cannot tile more than {\em depth} loops, so if $n >$ {\em depth} the loop chain must first be split into smaller sequences of loops, to be individually inspected and executed. 

%For ease of explanation, this aspect has been neglected in the inspector algorithms described in the previous section. 

%Our inspector/executor strategy, therefore, works correctly as long as the loop chain has been ``dimensioned'' properly, with sufficient information in all $S^{b}$ regions 

%\paragraph{The execution of a tile}


\subsection{Computational Complexity of Inspection}
Let $N$ be the maximum size of a set in $\mathbb{L} = [L_0, L_1, ..., L_{n-1}]$ and let $M$ be the maximum number of sets accessed in a loop. If $a$ is the maximum arity of a map, then $K = a N$ is the maximum cost for iterating over a map. $K$ is also the worst-case cost for inverting a map. Let $p < 1$ be the probability that a conflict arises during inspection in the case of shared memory parallelism; thus, the expected number of inspection rounds is $R = \frac{1}{1-p}$. Hence, the worst-case computational costs of the main inspection phases are as in Table~\ref{table:st-comp-cost}.

\begin{table}[h]
\centering
\begin{tabulary}{1.0\columnwidth}{P{2.7cm} | c | c}
\hline
Phase & Cost shared memory & Cost distributed memory \\
\hline
Partitioning & $N$ & $N$ \\
Coloring & $R K $ & $N$ \\ 
Projection & $R (n M N K^2) $ & $n M N K^2 $ \\ 
Tiling & $R (n M N K) $ & $n M N K $ \\
Local maps & $n M$ & $n M$\\
\hline
\end{tabulary}
\caption{Worst-case costs of inspection.}
\label{table:st-comp-cost}
\end{table}


%LOCAL MAPS: Finally, for each $T_i \in \mathbb{T}$, for each $L_j \in \mathbb{L}$, for each map used in $L_j$, create a local map. 

\section{Implementation}
\label{sec:tiling:automation}
%- is automation possible ? -> DSLs !!! + lazy evaluation + inspector/executor + mpi-trick
% subsets ! seed loop cannot be subset

\subsection{SLOPE: a Library for Tiling Irregular Computations}
...

\subsection{PyOP2: a Runtime Library for Mesh Iteration}
...

\subsection{Firedrake/DMPlex: the S-depth mechanism for MPI}
...


\section{Performance Evaluation}
...

\subsection{Benchmarks}
\begin{itemize}
\item Sparse Jacobi (don't forget to just use tables or words instead of plots...)
\item Airfoil
\item Wave Explicit
\end{itemize}

\subsection{Seigen: an Elastic Wave Equation Solver for Seismological Problems}
\label{sec:tiling:seigen}
...
