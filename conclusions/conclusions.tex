\chapter{Conclusions}
In this final chapter, the achievements and the shortcomings of this thesis are reviewed. Future research directions, including an estimate of their potential scientific impact, are also discussed. 

\section{Summary}
Novel techniques that improve the performance of numerical methods for solving partial differential equations have been introduced. Our work builds upon three cornerstones:

\begin{description} 
\item[Solid motivations] The performance optimization of a code must always start with an analysis of its bottlenecks and an assessment of its potentials. Furthermore, to avoid solving fictitious problems, real-world applications, kernels, and datasets must be used. This approach has systematically been adopted in this thesis.
\item[Automation through high-level compilers] Implementing and evaluating optimizations is challenging. Simplistic codes and benchmarks should be avoided for experimentation, since they often provide an incomplete picture of the computational domain. On the other hand, integrating optimizations with real codes is a long, tedious, error-prone task -- notably, a task that users should not be expected to carry out on their own. Our solution to this issue is inserting compilers and libraries into frameworks based upon domain-specific languages. In such environments, (i) domain specialists provide the real applications and (ii) performance optimization is automated ``behind the scenes'' -- that is, without the need for user intervention.
\item[Validation of the hypotheses] The hypotheses behind run-time improvements must always be validated. Optimizations, especially changes at the algorithmic level, may have unpredictable implications on the low-level performance. For instance, a transformation that reduces the operation count may harm the vectorizability of loops. All performance numbers reported in this thesis have been extensively analyzed through a variety of tools, including compiler reports and profilers. 
\end{description}

In this thesis, we have investigated three main problems in the field of numerical methods on unstructured meshes.

\paragraph{Sparse tiling of irregular loops (Chapter~\ref{ch:sparsetiling})} The biggest achievement is a technique for fusing arbitrary sequences of loops expressible by means of the loop chain abstraction. In fact, this technique is so general that any graph-based computation that adheres to the program model of the loop chain abstraction (or equivalent) is a potential optimization candidate. The first version of the generalized inspector/executor sparse tiling scheme was jointly devised with~\cite{st-paper}. The performance limitations of the inspector presented in this work and a careful study of the requirements of real-world applications (e.g., execution on distributed-memory architectures) motivated the design and the implementation of a second generalized inspector/executor scheme, which we presented in Section~\ref{sec:tiling:inspector}. Automation was achieved through integration of SLOPE with the Firedrake/PyOP2 tool-chain. The extensive performance investigation of the seismological code in Section~\ref{sec:tiling:seigen} clarifies the limitations and the potentials of sparse tiling. To the best of our knowledge, this is, to date, the first study that {\it simultaneously} attack (i) fusion/tiling of irregular loops, (ii) real-world applications, (iii) automation.

\paragraph{Minimizing flops in finite element kernels (Chapter~\ref{ch:optimality})} Automated code generation for finite element assembly is especially interesting when the kernels, as a consequence of the operators used in the problem specification, are characterized by the presence of complex mathematical expressions. Hand-writing these kernels requires a great deal of effort; even more complex is optimizing for the operation count. This thesis demonstrates how to leverage automated code generation and mathematical properties for reducing (in particular, reaching local or global optima) the operation count in finite element integration loop nests. The main challenge tackled in this chapter was determining how to coordinate different rewrite operators (e.g., common sub-expressions elimination, factorization, expansion) that are subjected to a non-trivial interplay. Our algorithm shows significant performance improvements over state-of-the-art code generation systems for finite element assembly. 

\paragraph{Low-level optimization of finite element kernels (Chapter~\ref{ch:lowlevelopt})}
A second question arising when studying finite element kernels concerns the efficiency of the generated code. We have addressed this problem for conventional multi-core architectures. The peculiar structure of the assembly kernels (e.g., small loops, small working set distributed over a large number of variables/arrays) makes it difficult to find a specific sequence of transformations that maximizes the performance of all problems, on all architectures. We have investigated a number of novel and traditional compiler transformations, aimed at creating or improving SIMD vectorization and data locality. Amongst all these, the most powerful one is padding and data alignment, which exploits the memory access pattern of assembly kernels to increase the effectiveness of SIMD vectorization, at the price of a few additional iterations. This transformation has been demonstrated to provide systematic improvements in execution time across a range of problems. Although the idea behind this optimization is simple, the implementation conceals several challenges, including the handling of non unit-stride memory accesses.
~\\ 
~\\
The techniques produced in this thesis have been released in publicly available software. The work on finite element kernels, in particular, is implemented in COFFEE, which we have described in Chapter~\ref{ch:coffee}. We recall that COFFEE is used in Firedrake, a framework for the resolution of partial differential equations through the finite element method, which comprises a user base in steady increase.

\section{Limitations}
Limitations of the proposed techniques have already been discussed in the respective chapters; here, we emphasize the most relevant.
 
\paragraph{Performance analysis and tuning of sparse tiling}
The system described in Chapter~\ref{ch:sparsetiling} automates sparse tiling in Firedrake/PyOP2 programs through the {\em loop$\_$chain} interface. This represented a major step towards the accessibility of the optimization. What is still missing is a cost model that facilitates, or ideally automates, the performance tuning. Most applications, such as Seigen (Section~\ref{sec:tiling:seigen}), are characterized by long sequences of heterogeneous loops. Multiple computational aspects need be considered: some loops may be memory-bound, while others compute-bound; the working set size may vary significantly amongst different subsets of loops; a loop may take much longer to execute than others; and so on. Altogether, these issues make it difficult thinking of a system that autonomously individuates loop chains and optimal tile sizes. Auto-tuning could help, but its implementation would be non-trivial. Moreover, an auto-tuning system could require (i) a significant amount of time to retrieve a configuration and (ii) re-execution as some problem parameters change (e.g., the domain discretization). It is however necessary to experiment with a wider class of programs and loops before addressing the automation of performance tuning.

%tiling : performance tuning is a super mess
%tiling: experimentation is LONG and need to talk to people to find more applications. there many hints there are more, but how easy? how easy the integration ? 

\paragraph{Combination of low-level transformations in finite element kernels}
All optimizations presented in Chapter~\ref{ch:lowlevelopt} can improve the performance of finite element kernels, but it is difficult to determine the optimal sequence of transformations for a given problem. If, on one hand, padding and data alignment shows consistent speed-ups across a variety of problems, challenging is understanding how to compose the other transformations. The cost model suggested in~\cite{Luporini-coffee} works decently if the expression rewriting stage (Section~\ref{sec:coffee:pipeline}) is limited to the sole generalized loop-invariant code motion. However, coordinating vector-register tiling and expression splitting, as well as the more general purpose transformations (especially vector-promotion; see Section~\ref{sec:coffee-genpurp-opts} for the full list), becomes unclear if the potential of the whole expression rewriting engine is exploited. This is a result of introducing more temporaries and complex sub-expressions in the outer loops, as well as creating more loops. These low-level transformations are available in COFFEE, but at the moment, apart from padding and data alignment, users are expected to execute performance tuning on their own. Preliminary work on compiler auto-tuning is available at~\citep{coffee-code}; this is one of the loose ends of this thesis.


\section{Future Research Directions}
We recapitulate the open questions that have arisen from the three main chapters of this thesis. 

\paragraph{Extensive performance evaluation of sparse tiling}
The theory and the tools behind generalized sparse tiling are now mature. The next big step is extensive experimentation with computations arising in a diverse range of domains, such as numerical methods on unstructured meshes (not necessarily finite element), molecular dynamics, and graph processing. The potentials of this optimization are still to be fully unveiled. 

\paragraph{Sparse tiling in general-purpose or research compilers}
It is unclear whether sparse tiling could (should) be automated in general-purpose or research (polyhedral-based) compilers. Although simple inspector/executor schemes are widespread (e.g., many compilers introduce alternative code paths for handling statically unknown loop bounds), the way towards supporting more advanced transformations is long and intricate. More importantly, sparse tiling can be seen, to some extent, as a domain-specific transformation; this makes us wonder whether integration with general-purpose compilers is even a rightful research direction. Unless in presence of very strong motivations (e.g., a suite of legacy codes characterized by memory-bound irregular loop nests), we find difficult to encourage investigating this path.

\paragraph{Sparse tiling and space-filling curves}
In Section~\ref{sec:tiling:difficult}, we have discussed the dichotomy between space-filling curves and loop tiling. Later, in Section~\ref{sec:tiling:seigen}, we have hypothesized that applying sparse tiling on top of a space-filling curve may significantly relieve the performance penalties that we are subjected to at the moment (e.g., load balancing, TLB misses). There is an on-going effort in the DMPlex community towards the introduction of space-filling curves in their system, so we one should be able to verify our hypothesis relatively soon. We speculate that space-filling curves can have a terrific impact on the performance of sparse tiling.

\paragraph{Minimizing operations and sum-factorization in assembly kernels}
The strategy for minimizing the operation count developed in Chapter~\ref{ch:optimality} targets a specific class of finite element integration loop nests. If the loop nest changes, a generalization of the strategy is necessary. This may happen if the chosen function spaces have some additional structure. For instance, in spectral element methods basis functions are constructed from the tensor product of two orthogonal spaces. This enables the application of sum-factorization, a well-known technique for reducing the operation count. In essence, sum-factorization is the mathematical name for something very close to Strategy~\ref{strategy:ii} in Section~\ref{sec:se-rln}. It is still to be investigated the relationship between this optimization and our transformation model. 

\paragraph{Minimizing operations and memory constraints in assembly kernels}
Two memory constraints have been introduced in Chapter~\ref{ch:optimality}. While Constraint~\ref{const:Le} was imposed to limit the transformation space, Constraint~\ref{const:TH} has a direct impact on the main optimization algorithm. Both constraints leave some interesting questions unanswered. First, the heuristic for choosing the memory threshold is a weakness of our methodology: we have proved (Section~\ref{sec:opt:perf-results}) that a drastic reduction in operation count should overrule any constraints on the working set size. However, we do not know how to identify the ``cut-off'' point, which is probably architecture-dependent. Second, Constraint~\ref{const:Le} becomes overly restrictive if the time dimension is included in our definition of finite element integration loop nest. Geometry-dependent sub-expressions are now generalized code motion candidates, which further expands the transformation space. Supporting this kind of hoisting could dramatically improve the execution time of an application; as explained in Section~\ref{sec:tiling:seigen}, Seigen would be extremely likely to benefit from it.

\paragraph{Extensions to many-core architectures}
In this thesis, we have focused on the platforms that are typically used for unstructured mesh computations, that is, conventional CPU architectures. Two of the key limiting factors to the execution on many-core platforms (e.g., GPUs) are the stringent memory requirements of these computations and the lack of support from third-party libraries (e.g., PETSc). It would not be surprising if this scenario evolved over the next years. The upcoming generation of many-core architectures is in this sense promising, with traditional ``accelerators'' slowly becoming first-class platforms (e.g., nodes that can run a full-fledged operating system, larger memory systems, tighter coupling with the CPU through a shared address space), like the Intel's Knights Landing. While sparse tiling has yet too many open questions that need be answered before even thinking about new platforms. the COFFEE-related work is sufficiently mature for a generalization. Automated generation of efficient code for finite element kernels on GPUs has never been treated, to date.

\paragraph{Compiler auto-tuning for assembly kernels}
As explained in the previous section, low-level optimization of assembly kernels could greatly benefit from compiler auto-tuning. Auto-tuning techniques have been proven successful in a variety of frameworks based upon domain-specific languages, such as Spiral and Halide. This would require some new technology in COFFEE. Our hypothesis is that a minimalistic auto-tuning system, performing a brute-force search out of a few tens of promising code syntheses, could result in significant speed-ups. This consideration is the fruit of preliminary work on the subject, with experimentation limited to relatively simple problems.

