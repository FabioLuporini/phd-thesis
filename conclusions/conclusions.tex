\chapter{Conclusions}
In this final chapter, the achievements and the shortcomings of this thesis are reviewed. Future research directions, including an estimate of their potential scientific impact, are also discussed. 

\section{Summary}
Novel techniques that improve the performance of numerical methods for solving partial differential equations have been introduced. Our work builds upon three cornerstones:

\begin{description} 
\item[Solid motivations] The performance optimization of a code must always start with an analysis of its bottlenecks and an assessment of its potentials. Furthermore, to avoid solving fictitious problems, real-world applications, kernels, and datasets must be used. This approach has systematically been adopted in this thesis.
\item[Automation through high-level compilers] Implementing and evaluating optimizations is challenging. Simplistic codes and benchmarks should be avoided for experimentation, since they often provide an incomplete picture of the computational domain. On the other hand, integrating optimizations with real codes is a long, tedious, error-prone task -- notably, a task that users should not be expected to carry out on their own. Our solution to this issue is to insert compilers and libraries into frameworks based upon domain-specific languages. In such environments, (i) domain specialists provide the real applications and (ii) performance optimization is automated ``behind the scenes'' -- that is, without the need for user intervention.
\item[Validation of the hypotheses] The hypotheses behind any run-time improvements must always be validated. Optimizations, especially radical changes at the algorithmic level, may have unpredictable implications on the low-level performance. For instance, a transformation that reduces the operation count may impair the vectorizability of loops. All performance numbers reported in this thesis have been extensively analyzed through a variety of tools, including compiler reports and profilers. 
\end{description}

In this thesis, we have investigated three main problems in the field of numerical methods on unstructured meshes.

\paragraph{Sparse tiling of irregular loops (Chapter~\ref{ch:sparsetiling})} The biggest achievement of this chapter is an inspector/executor scheme for fusing arbitrary sequences of loops expressible by means of the loop chain abstraction. In fact, this technique is so general that any graph-based computation that adheres to the program model of the loop chain abstraction (or equivalent) is a potential optimization candidate. The first version of the generalized inspector/executor sparse tiling scheme was jointly devised with~\cite{st-paper}. The performance limitations of this inspector and an in-depth study of the typical requirements of real-world applications (e.g., execution on distributed-memory architectures) motivated the design and the implementation of a second generalized inspector/executor scheme, presented in Section~\ref{sec:tiling:inspector}. Automation was achieved through integration of SLOPE with the Firedrake/PyOP2 tool-chain. The extensive performance investigation of the seismological code in Section~\ref{sec:tiling:seigen} clarifies the limitations and the potentials of sparse tiling. To the best of our knowledge, this is, to date, the first study that {\it simultaneously} attack (i) fusion/tiling of irregular loops, (ii) the optimization of real-world applications, (iii) automation.

\paragraph{Minimizing flops in finite element kernels (Chapter~\ref{ch:optimality})} Automated code generation for finite element assembly is especially interesting when the kernels, as a consequence of the operators used in the problem specification, are characterized by the presence of complex mathematical expressions. Hand-writing these kernels requires a great deal of effort; even more complex is optimizing for the operation count. This thesis demonstrates that automated code generation and mathematical properties  can be leveraged for reducing the operation count (in particular, for reaching a local optimum) in finite element assembly kernels. One of the main challenges tackled in this chapter is the coordination of different rewrite operators (e.g., common sub-expressions elimination, factorization, expansion), as subjected to a non-trivial interplay. Our algorithm shows significant performance improvements over state-of-the-art code generation systems for finite element assembly. 

\paragraph{Low-level optimization of finite element kernels (Chapter~\ref{ch:lowlevelopt})}
A second question arising when studying finite element kernels concerns the efficiency of the generated code. We have addressed this problem for conventional multi-core architectures. The peculiar structure of the assembly kernels (e.g., small loops, small working set distributed over a large number of variables/arrays) makes it difficult to find a specific sequence of transformations that maximizes the performance of all problems, on all architectures. We have investigated a number of novel and traditional compiler transformations, aimed at creating or improving SIMD vectorization and data locality. Amongst these, the most powerful is padding and data alignment, which exploits the memory access pattern of assembly kernels to increase the effectiveness of SIMD vectorization, at the price of a few additional scalar operations. This transformation has been demonstrated to provide systematic improvements in execution time across a range of problems. Although the idea behind the optimization is simple, the implementation conceals several challenges, including the handling of non unit-stride memory accesses.
~\\ 
~\\
The techniques produced in this thesis have been implemented in publicly available software. SLOPE is a C library to express inspector/executor schemes for sparse tiling. The work on finite element kernels is implemented in COFFEE. Both COFFEE and SLOPE are integrated with Firedrake.

\section{Limitations}
The limitations of this research have already been discussed in the previous chapters; here, we emphasize the most relevant.

\paragraph{Performance analysis and tuning of sparse tiling}
The system described in Chapter~\ref{ch:sparsetiling} automates sparse tiling in Firedrake programs through the {\em loop$\_$chain} interface. This was a major step, as it made a very complex optimization easily accessible. What is still missing is a cost model, or a system, that facilitates or even automates the performance tuning. Most applications, such as Seigen (Section~\ref{sec:tiling:seigen}), are characterized by long sequences of heterogeneous loops. Multiple computational aspects need be considered: some loops may be memory-bound, while others compute-bound; the working set size may vary significantly amongst different subsets of loops; a loop may take much longer to execute than others; and so on. Altogether, these issues make it difficult to think of a system capable of autonomously identifying which particular loops in a loop chain should be fused. Auto-tuning could help, but the implementation would be non-trivial. Moreover, an auto-tuning system could require (i) a significant amount of time to retrieve a configuration and (ii) re-execution as some problem parameters change (e.g., the domain discretization). Experimenting with other programs and loops is however fundamental before addressing this problem.

%tiling : performance tuning is a super mess
%tiling: experimentation is LONG and need to talk to people to find more applications. there many hints there are more, but how easy? how easy the integration ? 

\paragraph{Combination of low-level transformations in finite element kernels}
All optimizations presented in Chapter~\ref{ch:lowlevelopt} can improve the performance of finite element kernels, but it is difficult to determine the optimal sequence of transformations for a given problem. While, on one hand, padding and data alignment shows consistent speed-ups across a variety of problems, it is much more challenging to understand how to compose the other transformations. The cost model suggested in~\cite{Luporini-coffee} works decently if the expression rewriting stage (Section~\ref{sec:coffee:pipeline}) is limited to the sole generalized loop-invariant code motion. However, coordinating vector-register tiling and expression splitting, as well as the general-purpose transformations (especially vector-promotion; see Section~\ref{sec:coffee-genpurp-opts} for the full list), becomes difficult if the potential of the whole expression rewriting engine is exploited. This is a result of introducing more temporaries and complex sub-expressions in the outer loops, as well as creating more loops. Many low-level transformations are available in COFFEE, but none of them is automatically applied apart from padding/data-alignment and loop fusion. Users, therefore, have some responsibility for low-level performance tuning. Preliminary work on compiler auto-tuning is available at~\citep{coffee-code}, but remains one of the loose ends of this thesis.


\section{Future Research Directions}
We recapitulate the open questions arisen from the three main chapters of the thesis and provide insights into possible research directions. 

\paragraph{Extensive performance evaluation of sparse tiling}
The theory and the tools behind generalized sparse tiling are now mature. The next big step is extensive experimentation with computations arising in multiple domains; for instance, in addition to numerical methods on unstructured meshes (not necessarily finite element), molecular dynamics and graph processing. The potentials of this optimization are still to be fully discovered. 

\paragraph{Sparse tiling in general-purpose or research compilers}
It is unclear whether sparse tiling could (or should) be automated in general-purpose or research (polyhedral-based) compilers. Although simple inspector/executor schemes are widespread (e.g., many compilers introduce alternative code paths for handling statically unknown loop bounds), the path towards supporting more advanced transformations is long and intricate. More importantly, sparse tiling can be seen, to some extent, as a domain-specific transformation; this makes us wonder whether integration with general-purpose compilers is even a rightful research direction. Unless in presence of very strong motivations (e.g., a suite of legacy codes characterized by memory-bound irregular loop nests), we find difficult to encourage the investigation of this path.

\paragraph{Sparse tiling and space-filling curves}
In Section~\ref{sec:tiling:difficult}, we have discussed the dichotomy between space-filling curves and loop tiling. Later, in Section~\ref{sec:tiling:seigen}, we have hypothesized that applying sparse tiling on top of a space-filling curve may significantly reduce the performance penalties that we are subjected to at the moment (e.g., load balancing, TLB misses). There is an on-going effort in the DMPlex community that will make space-filling curves available in Firedrake, so our hypotheses could be verifiable relatively soon. We speculate that space-filling curves can have a great impact on the performance of sparse tiling.

\paragraph{Minimizing operations and sum-factorization in assembly kernels}
The strategy for minimizing the operation count developed in Chapter~\ref{ch:optimality} targets a specific class of finite element integration loop nests. If the loop nest changes dramatically, a generalization of the strategy will be necessary. This may happen if the chosen function spaces have additional structure. For instance, in spectral element methods basis functions are constructed from the tensor product of two orthogonal spaces. This enables the application of sum-factorization, a well-known technique for reducing the operation count (see for example~\cite{spencer}). In essence, sum-factorization is the mathematical name for what we call generalized code motion in Chapter~\ref{ch:optimality}. The relationship between this optimization and our transformation model is yet to be investigated.

\paragraph{Minimizing operations and memory constraints in assembly kernels}
Two memory constraints have been introduced in Chapter~\ref{ch:optimality}. While Constraint~\ref{const:Le} has been imposed to limit the transformation space, Constraint~\ref{const:TH} has a direct impact on the main optimization algorithm. Both constraints leave some interesting questions unanswered. First, the heuristic for choosing the memory threshold is partly flawed: we have proved (Section~\ref{sec:opt:perf-results}) that a drastic reduction in operation count should overrule any constraints on the working set size. It is unclear how to effectively approximate the ``cut-off'' point, which is architecture- and problem-dependent. Second, Constraint~\ref{const:Le} would be overly restrictive if the time dimension were included in the model of a finite element integration loop nest. Geometry-dependent sub-expressions would become generalized code motion candidates; this would make the transformation space even bigger. Supporting this kind of hoisting could dramatically improve the execution time of an application; for example, as explained in Section~\ref{sec:tiling:seigen}, Seigen would benefit from it. Further investigation in this direction is likely to be very rewarding.

\paragraph{Extensions to many-core architectures}
In this thesis, we have focused on the platforms that are typically used for unstructured mesh computations, that is, conventional CPU architectures. Two of the key limiting factors to the execution on many-core platforms (e.g., GPUs) are the stringent memory requirements of these computations and the lack of support from third-party libraries (e.g., PETSc). It would not be surprising if this scenario evolved over the next years. The upcoming generation of many-core architectures is in this sense promising, with traditional ``accelerators'' slowly becoming first-class platforms (e.g., nodes that can run a full-fledged operating system, larger memory systems, tighter coupling with the CPU through a shared address space), like the Intel's Knights Landing. While sparse tiling has yet too many open questions that need be answered before even thinking about new platforms. the COFFEE-related work is sufficiently mature for a generalization. Automated generation of efficient code for finite element kernels on GPUs has not been addressed.

\paragraph{Compiler auto-tuning for assembly kernels}
As explained in the previous section, low-level optimization of assembly kernels could greatly benefit from compiler auto-tuning. Auto-tuning techniques have been proven successful in a variety of frameworks based upon domain-specific languages, such as Spiral and Halide. This would require some new technology in COFFEE. Our hypothesis is that a minimalistic auto-tuning system, performing a brute-force search out of a few tens of promising code syntheses, could result in significant speed-ups. This consideration is the fruit of preliminary work on the subject, with experimentation limited to relatively simple problems.

